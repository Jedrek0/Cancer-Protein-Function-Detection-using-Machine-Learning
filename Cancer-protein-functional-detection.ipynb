{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91485c18",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layers\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m'''Defining the swish -activation function'''\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    " \n",
    "'''Defining the swish -activation function'''\n",
    "\n",
    "from tensorflow.keras.backend import sigmoid\n",
    "from tensorflow.keras.utils import get_custom_objects\n",
    "from tensorflow.keras.layers import Activation\n",
    "\n",
    "\n",
    "class Swish(Activation):\n",
    "    \n",
    "    def __init__(self, activation, **kwargs):\n",
    "        super(Swish, self).__init__(activation, **kwargs)\n",
    "        self.__name__ = 'swish'\n",
    "\n",
    "\n",
    "def swish(x, beta = 1):\n",
    "  \n",
    "    return (x * sigmoid(beta * x))\n",
    "\n",
    "get_custom_objects().clear()\n",
    "get_custom_objects().update({'swish': Swish(swish)})    \n",
    "\n",
    "class model_brain_inception_only_vin_1:\n",
    "\n",
    "    def __init__(self, channels=17,tower_min_max_only=False,activation='relu'):\n",
    "        '''Initialization\n",
    "        \n",
    "        activation can be 'selu','swish','nishy_vin1'\n",
    "        '''\n",
    "        self.activation = activation\n",
    "        self.channels = channels\n",
    "        self.square_height=200\n",
    "        self.square_width=200\n",
    "        self.tower_min_max_only = tower_min_max_only\n",
    "\n",
    "    def layer_1_reducer(self,lay_1_all_output,d_lay_1_to_2,m_p=4):\n",
    "        \n",
    "        layer_1_pool = layers.MaxPooling2D(pool_size=(m_p, m_p))(lay_1_all_output)\n",
    "        #to reduce the depth representation\n",
    "        incept_1_to_3=layers.Conv2D(d_lay_1_to_2, (1,1),strides=(1,1),padding='same',activation=self.activation)(layer_1_pool)\n",
    "        incept_1_to_5=layers.Conv2D(d_lay_1_to_2, (1,1),strides=(1,1),padding='same',activation=self.activation)(layer_1_pool)\n",
    "        return incept_1_to_3,incept_1_to_5,layer_1_pool\n",
    "\n",
    "    def layer_2_reducer(self,lay_2_all_output,d_lay_1_to_2,m_p=3):\n",
    "        \n",
    "        layer_2_pool = layers.MaxPooling2D(pool_size=(m_p, m_p))(lay_2_all_output)       \n",
    "        #to reduce the depth representation\n",
    "        incept_1_to_3=layers.Conv2D(d_lay_1_to_2, (1,1),strides=(1,1),padding='same',activation=self.activation)(layer_2_pool)\n",
    "        incept_1_to_5=layers.Conv2D(d_lay_1_to_2, (1,1),strides=(1,1),padding='same',activation=self.activation)(layer_2_pool)\n",
    "        return incept_1_to_3,incept_1_to_5,layer_2_pool\n",
    "\n",
    "    def layer_3_reducer(self,lay_3_all_output,d_lay_3_to_4,m_p=2):\n",
    "        \n",
    "        layer_3_pool = layers.MaxPooling2D(pool_size=(m_p, m_p))(lay_3_all_output)\n",
    "        #to reduce the depth representation\n",
    "        incept_1_to_3=layers.Conv2D(d_lay_3_to_4, (1,1),strides=(1,1),padding='same',activation=self.activation)(layer_3_pool)\n",
    "        incept_1_to_5=layers.Conv2D(d_lay_3_to_4, (1,1),strides=(1,1),padding='same',activation=self.activation)(layer_3_pool)\n",
    "        return incept_1_to_3,incept_1_to_5,layer_3_pool\n",
    "\n",
    "    def layer_4_final(self,lay_4_all_output,d_lay_3_to_4,m_p=2):\n",
    "        layer_4_pool = layers.MaxPooling2D(pool_size=(m_p, m_p))(lay_4_all_output)\n",
    "        incept_1_to_final =layers.Conv2D(d_lay_3_to_4, (1,1),strides=(1,1),padding='same',activation=self.activation)(layer_4_pool)\n",
    "        return incept_1_to_final\n",
    "\n",
    "    def model_maker(self,f_inc=1,f_d=1):\n",
    "\n",
    "        d1=128*f_inc\n",
    "        d3=64*f_inc\n",
    "        d5=32*f_inc\n",
    "        d_max=32*f_inc  \n",
    "        d_lay_1_to_2 = 32*f_d\n",
    "        d_lay_3_to_4 = 64*f_d\n",
    "        \n",
    "        m_p_l1=4#maxpool layer 1 size\n",
    "        m_p_l2=3\n",
    "        m_p_l3=2\n",
    "        m_p_l4=2\n",
    "        \n",
    "        inputs = keras.Input(shape=(self.square_height, self.square_width, self.channels))\n",
    "        \n",
    "        inp1 = keras.Input(shape=(self.square_height, self.square_width,  self.channels))\n",
    "        inp2 = keras.Input(shape=(self.square_height, self.square_width,  self.channels))\n",
    "        inp3 = keras.Input(shape=(self.square_height, self.square_width,  self.channels))    \n",
    "         \n",
    "        lay_1_incept_1 = layers.Conv2D(d1, (1,1),strides=(1,1),padding='same',activation=self.activation)(inputs)\n",
    "        lay_1_incept_3 = layers.Conv2D(d3, (3,3),strides=(1,1),padding='same',activation=self.activation)(inputs)\n",
    "        lay_1_incept_5 = layers.Conv2D(d5, (5,5),strides=(1,1),padding='same',activation=self.activation)(inputs)\n",
    "        lay_1_incept_max_pool= layers.MaxPooling2D(pool_size=(3, 3),strides=(1,1),padding='same')(inputs)\n",
    "        lay_1_incept_max_pool_depth=layers.Conv2D(d_max, (1,1),strides=(1,1),padding='same',activation='relu')(lay_1_incept_max_pool)\n",
    "        \n",
    "        lay_1_all_output = layers.concatenate([lay_1_incept_1, lay_1_incept_3,lay_1_incept_5,lay_1_incept_max_pool_depth], axis=3)\n",
    "        '''layer 1 general network'''    \n",
    "        layer_1_INCEPT_Net = keras.models.Model(inputs, lay_1_all_output, name='layer_1_INCEPT')     \n",
    "        \n",
    "        '''Applying layer_1 in projections'''\n",
    "        #layer1 output of projection 1\n",
    "        inp_1_lay_1_all_output = layer_1_INCEPT_Net(inp1) \n",
    "        inp_2_lay_1_all_output = layer_1_INCEPT_Net(inp2) \n",
    "        inp_3_lay_1_all_output = layer_1_INCEPT_Net(inp3) \n",
    "\n",
    "        lay_1_inp1_incept_1_to_3,lay_1_inp1_incept_1_to_5,inp1_lay_1_pool = self.layer_1_reducer(inp_1_lay_1_all_output,d_lay_1_to_2,m_p=m_p_l1)\n",
    "        lay_1_inp2_incept_1_to_3,lay_1_inp2_incept_1_to_5,inp2_lay_1_pool = self.layer_1_reducer(inp_2_lay_1_all_output,d_lay_1_to_2,m_p=m_p_l1)\n",
    "        lay_1_inp3_incept_1_to_3,lay_1_inp3_incept_1_to_5,inp3_lay_1_pool = self.layer_1_reducer(inp_3_lay_1_all_output,d_lay_1_to_2,m_p=m_p_l1)\n",
    "        '''place the size of NN layer_2 inputs'''\n",
    "        #since the stride is 1 ans same padding these equation works\n",
    "        lay_1_height=int(self.square_height/m_p_l1)\n",
    "        lay_1_width = int(self.square_width/m_p_l1)\n",
    "        \n",
    "        \n",
    "        lay_1_inp_incept_1_to_3 = keras.Input(shape=(lay_1_height,lay_1_width, d_lay_1_to_2))\n",
    "        lay_1_inp_incept_1_to_5 = keras.Input(shape=(lay_1_height,lay_1_width, d_lay_1_to_2))\n",
    "        inp_lay_1_pool = keras.Input(shape=(lay_1_height,lay_1_width,d1+d3+d5+d_max))\n",
    "        \n",
    "        lay_2_incept_1 = layers.Conv2D(d1, (1,1),strides=(1,1),padding='same',activation=self.activation)(inp_lay_1_pool)\n",
    "        lay_2_incept_3 = layers.Conv2D(d3, (3,3),strides=(1,1),padding='same',activation=self.activation)(lay_1_inp_incept_1_to_3)\n",
    "        lay_2_incept_5 = layers.Conv2D(d5, (5,5),strides=(1,1),padding='same',activation=self.activation)(lay_1_inp_incept_1_to_5)\n",
    "        lay_2_incept_max_pool= layers.MaxPooling2D(pool_size=(3, 3),strides=(1,1),padding='same')(inp_lay_1_pool)\n",
    "        lay_2_incept_max_pool_depth=layers.Conv2D(d_max, (1,1),strides=(1,1),padding='same',activation='relu')(lay_2_incept_max_pool)\n",
    "        \n",
    "        lay_2_all_output = layers.concatenate([lay_2_incept_1, lay_2_incept_3,lay_2_incept_5,lay_2_incept_max_pool_depth], axis=3)\n",
    "        '''layer 2 general network'''    \n",
    "        layer_2_INCEPT_Net = keras.models.Model(inputs=[lay_1_inp_incept_1_to_3,lay_1_inp_incept_1_to_5,inp_lay_1_pool],outputs= lay_2_all_output, name='layer_2_INCEPT')     \n",
    "\n",
    "        '''Applying layer_2 in projections'''\n",
    "        #layer1 output of projection 1\n",
    "        inp_1_lay_2_all_output = layer_2_INCEPT_Net([lay_1_inp1_incept_1_to_3,lay_1_inp1_incept_1_to_5,inp1_lay_1_pool]) \n",
    "        inp_2_lay_2_all_output = layer_2_INCEPT_Net([lay_1_inp2_incept_1_to_3,lay_1_inp2_incept_1_to_5,inp2_lay_1_pool]) \n",
    "        inp_3_lay_2_all_output = layer_2_INCEPT_Net([lay_1_inp3_incept_1_to_3,lay_1_inp3_incept_1_to_5,inp3_lay_1_pool]) \n",
    "\n",
    "        #layer2 output of projection 1\n",
    "        lay_2_inp1_incept_1_to_3,lay_2_inp1_incept_1_to_5,inp1_lay_2_pool = self.layer_2_reducer(inp_1_lay_2_all_output,d_lay_1_to_2,m_p=m_p_l2)\n",
    "        lay_2_inp2_incept_1_to_3,lay_2_inp2_incept_1_to_5,inp2_lay_2_pool = self.layer_2_reducer(inp_2_lay_2_all_output,d_lay_1_to_2,m_p=m_p_l2)\n",
    "        lay_2_inp3_incept_1_to_3,lay_2_inp3_incept_1_to_5,inp3_lay_2_pool = self.layer_2_reducer(inp_3_lay_2_all_output,d_lay_1_to_2,m_p=m_p_l2)\n",
    "\n",
    "        '''place the size of NN layer_3 inputs'''\n",
    "        #since the stride is 1 ans same padding these equation works\n",
    "        lay_2_height=int(lay_1_height/m_p_l2)\n",
    "        lay_2_width = int(lay_1_height/m_p_l2)\n",
    "        \n",
    "        lay_2_inp_incept_1_to_3 = keras.Input(shape=(lay_2_height,lay_1_width, d_lay_1_to_2))\n",
    "        lay_2_inp_incept_1_to_5 = keras.Input(shape=(lay_2_width,lay_1_width, d_lay_1_to_2))      \n",
    "        inp_lay_2_pool = keras.Input(shape=(lay_2_height,lay_1_width,d1+d3+d5+d_max))\n",
    "\n",
    "        lay_3_incept_1 = layers.Conv2D(d1, (1,1),strides=(1,1),padding='same',activation=self.activation)(inp_lay_2_pool)\n",
    "        lay_3_incept_3 = layers.Conv2D(d3, (3,3),strides=(1,1),padding='same',activation=self.activation)(lay_2_inp_incept_1_to_3)\n",
    "        lay_3_incept_5 = layers.Conv2D(d5, (5,5),strides=(1,1),padding='same',activation=self.activation)(lay_2_inp_incept_1_to_5)\n",
    "        lay_3_incept_max_pool= layers.MaxPooling2D(pool_size=(3, 3),strides=(1,1),padding='same')(inp_lay_2_pool)\n",
    "        lay_3_incept_max_pool_depth=layers.Conv2D(d_max, (1,1),strides=(1,1),padding='same',activation='relu')(lay_3_incept_max_pool)\n",
    "        \n",
    "        lay_3_all_output = layers.concatenate([lay_3_incept_1, lay_3_incept_3,lay_3_incept_5,lay_3_incept_max_pool_depth], axis=3)\n",
    "        '''layer 3 general network'''    \n",
    "        layer_3_INCEPT_Net = keras.models.Model(inputs=[lay_2_inp_incept_1_to_3,lay_2_inp_incept_1_to_5,inp_lay_2_pool],outputs= lay_3_all_output, name='layer_3_INCEPT')     \n",
    "\n",
    "        '''Applying layer_3 in projections'''\n",
    "        #layer3 output of projection 1\n",
    "        inp_1_lay_3_all_output = layer_3_INCEPT_Net([lay_2_inp1_incept_1_to_3,lay_2_inp1_incept_1_to_5,inp1_lay_2_pool]) \n",
    "        inp_2_lay_3_all_output = layer_3_INCEPT_Net([lay_2_inp2_incept_1_to_3,lay_2_inp2_incept_1_to_5,inp2_lay_2_pool]) \n",
    "        inp_3_lay_3_all_output = layer_3_INCEPT_Net([lay_2_inp3_incept_1_to_3,lay_2_inp3_incept_1_to_5,inp3_lay_2_pool]) \n",
    "      \n",
    "        #layer3 output of projection 1\n",
    "        lay_3_inp1_incept_1_to_3,lay_3_inp1_incept_1_to_5,inp1_lay_3_pool = self.layer_3_reducer(inp_1_lay_3_all_output,d_lay_3_to_4,m_p=m_p_l3)\n",
    "        lay_3_inp2_incept_1_to_3,lay_3_inp2_incept_1_to_5,inp2_lay_3_pool = self.layer_3_reducer(inp_2_lay_3_all_output,d_lay_3_to_4,m_p=m_p_l3)\n",
    "        lay_3_inp3_incept_1_to_3,lay_3_inp3_incept_1_to_5,inp3_lay_3_pool = self.layer_3_reducer(inp_3_lay_3_all_output,d_lay_3_to_4,m_p=m_p_l3)\n",
    "\n",
    "        '''place the size of NN layer_4 inputs'''\n",
    "        #since the stride is 1 ans same padding these equation works\n",
    "        lay_3_height=int(lay_2_height/m_p_l3)\n",
    "        lay_3_width = int(lay_2_height/m_p_l3)\n",
    "        \n",
    "        lay_3_inp_incept_1_to_3 = keras.Input(shape=(lay_3_height, lay_3_width,d_lay_3_to_4))\n",
    "        lay_3_inp_incept_1_to_5 = keras.Input(shape=(lay_3_height, lay_3_width,d_lay_3_to_4))\n",
    "        inp_lay_3_pool = keras.Input(shape=(lay_3_height, lay_3_width,d1+d3+d5+d_max))       \n",
    "        \n",
    "        lay_4_incept_1 = layers.Conv2D(d1, (1,1),strides=(1,1),padding='same',activation=self.activation)(inp_lay_3_pool)\n",
    "        lay_4_incept_3 = layers.Conv2D(d3, (3,3),strides=(1,1),padding='same',activation=self.activation)(lay_3_inp_incept_1_to_3)\n",
    "        lay_4_incept_5 = layers.Conv2D(d5, (5,5),strides=(1,1),padding='same',activation=self.activation)(lay_3_inp_incept_1_to_5)\n",
    "        lay_4_incept_max_pool= layers.MaxPooling2D(pool_size=(3, 3),strides=(1,1),padding='same')(inp_lay_3_pool)\n",
    "        lay_4_incept_max_pool_depth=layers.Conv2D(d_max, (1,1),strides=(1,1),padding='same',activation='relu')(lay_4_incept_max_pool)\n",
    "        \n",
    "        lay_4_all_output = layers.concatenate([lay_4_incept_1, lay_4_incept_3,lay_4_incept_5,lay_4_incept_max_pool_depth], axis=3)\n",
    "    \n",
    "        '''layer 3 general network'''    \n",
    "        layer_4_INCEPT_Net = keras.models.Model(inputs=[lay_3_inp_incept_1_to_3,lay_3_inp_incept_1_to_5,inp_lay_3_pool], outputs=lay_4_all_output, name='layer_4_INCEPT')     \n",
    "        '''Applying layer_4 in projections'''\n",
    "        inp_1_lay_4_all_output = layer_4_INCEPT_Net([lay_3_inp1_incept_1_to_3,lay_3_inp1_incept_1_to_5,inp1_lay_3_pool]) \n",
    "        inp_2_lay_4_all_output = layer_4_INCEPT_Net([lay_3_inp2_incept_1_to_3,lay_3_inp2_incept_1_to_5,inp2_lay_3_pool]) \n",
    "        inp_3_lay_4_all_output = layer_4_INCEPT_Net([lay_3_inp3_incept_1_to_3,lay_3_inp3_incept_1_to_5,inp3_lay_3_pool]) \n",
    "\n",
    "        \n",
    "        #parallel = keras.models.Model(inputs,all_output, name='parallel') \n",
    "        tower_1 = self.layer_4_final(inp_1_lay_4_all_output,d_lay_3_to_4,m_p=m_p_l4)\n",
    "        tower_2 = self.layer_4_final(inp_2_lay_4_all_output,d_lay_3_to_4,m_p=m_p_l4)\n",
    "        tower_3 = self.layer_4_final(inp_3_lay_4_all_output,d_lay_3_to_4,m_p=m_p_l4)\n",
    "\n",
    "        \n",
    "        if self.tower_min_max_only:\n",
    "            tower_max = layers.maximum([tower_1,tower_2,tower_3])\n",
    "            tower_min=layers.minimum([tower_max, tower_2,tower_3])\n",
    "            #tower_average=layers.average([tower_1, tower_2,tower_3])\n",
    "            #tower_mul= layers.Multiply([tower_1, tower_2,tower_3])#since multiplication is expensive\n",
    "            merged = layers.concatenate([tower_max, tower_min], axis=1)\n",
    "            model_name = ''.join(['MIN_MAX_model_brain_inception_only_mentor_vin_1_act_',self.activation,'_f_inc_',str(f_inc),'_f_d_',str(f_d),'.h5'])\n",
    "        \n",
    "        else:\n",
    "            merged = layers.concatenate([tower_1, tower_2, tower_3], axis=1)\n",
    "            model_name = ''.join(['model_brain_inception_only_mentor_vin_1_act_',self.activation,'_f_inc_',str(f_inc),'_f_d_',str(f_d),'.h5'])\n",
    "        \n",
    "        merged = layers.Flatten()(merged)\n",
    "        all_co = layers.Dense(100, activation=self.activation)(merged)\n",
    "        all_co = layers.Dropout(0.5)(all_co)\n",
    "        all_co = layers.Dense(50, activation=self.activation)(all_co)\n",
    "        all_co= layers.Dropout(0.5)(all_co)\n",
    "        outputs = layers.Dense(3, activation='softmax')(all_co)\n",
    "        \n",
    "        model = keras.models.Model(inputs=[inp1, inp2,inp3], outputs=outputs, name=model_name)\n",
    "        \n",
    "        return model,model_name    \n",
    "\n",
    "\n",
    "class model_brain_incept_RESIDUAL_only_vin_1(model_brain_inception_only_vin_1):\n",
    "\n",
    "    def __init__(self, channels=17,tower_min_max_only=False,activation='relu'):\n",
    "        '''Initialization\n",
    "        \n",
    "        activation can be 'selu','swish','nishy_vin1'\n",
    "        '''\n",
    "        self.activation = activation\n",
    "        self.channels = channels\n",
    "        self.square_height=200\n",
    "        self.square_width=200\n",
    "        self.tower_min_max_only = tower_min_max_only\n",
    "\n",
    "\n",
    "    def model_maker(self,f_inc=1,f_d=1):\n",
    "\n",
    "        d1=128*f_inc\n",
    "        d3=64*f_inc\n",
    "        d5=32*f_inc\n",
    "        d_max=32*f_inc  \n",
    "        d_lay_1_to_2 = 32*f_d\n",
    "        d_lay_3_to_4 = 64*f_d\n",
    "        \n",
    "        m_p_l1=4#maxpool layer 1 size\n",
    "        m_p_l2=3\n",
    "        m_p_l3=2\n",
    "        m_p_l4=2\n",
    "        \n",
    "        inputs = keras.Input(shape=(self.square_height, self.square_width, self.channels))\n",
    "        \n",
    "        inp1 = keras.Input(shape=(self.square_height, self.square_width,  self.channels))\n",
    "        inp2 = keras.Input(shape=(self.square_height, self.square_width,  self.channels))\n",
    "        inp3 = keras.Input(shape=(self.square_height, self.square_width,  self.channels))    \n",
    "         \n",
    "        lay_1_incept_1 = layers.Conv2D(d1, (1,1),strides=(1,1),padding='same',activation=self.activation)(inputs)\n",
    "        lay_1_incept_3 = layers.Conv2D(d3, (3,3),strides=(1,1),padding='same',activation=self.activation)(inputs)\n",
    "        lay_1_incept_5 = layers.Conv2D(d5, (5,5),strides=(1,1),padding='same',activation=self.activation)(inputs)\n",
    "        lay_1_incept_max_pool= layers.MaxPooling2D(pool_size=(3, 3),strides=(1,1),padding='same')(inputs)\n",
    "        lay_1_incept_max_pool_depth=layers.Conv2D(d_max, (1,1),strides=(1,1),padding='same',activation='relu')(lay_1_incept_max_pool)\n",
    "        \n",
    "        lay_1_all_output = layers.concatenate([lay_1_incept_1, lay_1_incept_3,lay_1_incept_5,lay_1_incept_max_pool_depth], axis=3)\n",
    "        '''layer 1 general network'''    \n",
    "        layer_1_INCEPT_Net = keras.models.Model(inputs, lay_1_all_output, name='layer_1_INCEPT')     \n",
    "        \n",
    "        '''Applying layer_1 in projections'''\n",
    "        #layer1 output of projection 1\n",
    "        inp_1_lay_1_all_output = layer_1_INCEPT_Net(inp1) \n",
    "        inp_2_lay_1_all_output = layer_1_INCEPT_Net(inp2) \n",
    "        inp_3_lay_1_all_output = layer_1_INCEPT_Net(inp3) \n",
    "\n",
    "        lay_1_inp1_incept_1_to_3,lay_1_inp1_incept_1_to_5,inp1_lay_1_pool = self.layer_1_reducer(inp_1_lay_1_all_output,d_lay_1_to_2,m_p=m_p_l1)\n",
    "        lay_1_inp2_incept_1_to_3,lay_1_inp2_incept_1_to_5,inp2_lay_1_pool = self.layer_1_reducer(inp_2_lay_1_all_output,d_lay_1_to_2,m_p=m_p_l1)\n",
    "        lay_1_inp3_incept_1_to_3,lay_1_inp3_incept_1_to_5,inp3_lay_1_pool = self.layer_1_reducer(inp_3_lay_1_all_output,d_lay_1_to_2,m_p=m_p_l1)\n",
    "        '''place the size of NN layer_2 inputs'''\n",
    "        #since the stride is 1 ans same padding these equation works\n",
    "        lay_1_height=int(self.square_height/m_p_l1)\n",
    "        lay_1_width = int(self.square_width/m_p_l1)\n",
    "        \n",
    "        \n",
    "        lay_1_inp_incept_1_to_3 = keras.Input(shape=(lay_1_height,lay_1_width, d_lay_1_to_2))\n",
    "        lay_1_inp_incept_1_to_5 = keras.Input(shape=(lay_1_height,lay_1_width, d_lay_1_to_2))\n",
    "        inp_lay_1_pool = keras.Input(shape=(lay_1_height,lay_1_width,d1+d3+d5+d_max))\n",
    "        \n",
    "        lay_2_incept_1 = layers.Conv2D(d1, (1,1),strides=(1,1),padding='same',activation=self.activation)(inp_lay_1_pool)\n",
    "        lay_2_incept_3 = layers.Conv2D(d3, (3,3),strides=(1,1),padding='same',activation=self.activation)(lay_1_inp_incept_1_to_3)\n",
    "        lay_2_incept_5 = layers.Conv2D(d5, (5,5),strides=(1,1),padding='same',activation=self.activation)(lay_1_inp_incept_1_to_5)\n",
    "        lay_2_incept_max_pool= layers.MaxPooling2D(pool_size=(3, 3),strides=(1,1),padding='same')(inp_lay_1_pool)\n",
    "        lay_2_incept_max_pool_depth=layers.Conv2D(d_max, (1,1),strides=(1,1),padding='same',activation='relu')(lay_2_incept_max_pool)\n",
    "        \n",
    "\n",
    "        lay_2_all_output = layers.concatenate([lay_2_incept_1, lay_2_incept_3,lay_2_incept_5,lay_2_incept_max_pool_depth], axis=3)\n",
    "        '''layer 2 general network'''    \n",
    "        layer_2_INCEPT_Net = keras.models.Model(inputs=[lay_1_inp_incept_1_to_3,lay_1_inp_incept_1_to_5,inp_lay_1_pool],outputs= lay_2_all_output, name='layer_2_INCEPT')     \n",
    "\n",
    "        '''Applying layer_2 in projections'''\n",
    "        #layer1 output of projection 1\n",
    "        inp_1_lay_2_incept_all_output = layer_2_INCEPT_Net([lay_1_inp1_incept_1_to_3,lay_1_inp1_incept_1_to_5,inp1_lay_1_pool]) \n",
    "        inp_2_lay_2_incept_all_output = layer_2_INCEPT_Net([lay_1_inp2_incept_1_to_3,lay_1_inp2_incept_1_to_5,inp2_lay_1_pool]) \n",
    "        inp_3_lay_2_incept_all_output = layer_2_INCEPT_Net([lay_1_inp3_incept_1_to_3,lay_1_inp3_incept_1_to_5,inp3_lay_1_pool]) \n",
    "        #make residUAL NETWORK\n",
    "        inp_1_lay_2_all_output= layers.add([inp1_lay_1_pool, inp_1_lay_2_incept_all_output])\n",
    "        inp_2_lay_2_all_output= layers.add([inp2_lay_1_pool, inp_2_lay_2_incept_all_output])\n",
    "        inp_3_lay_2_all_output= layers.add([inp3_lay_1_pool, inp_3_lay_2_incept_all_output])\n",
    "\n",
    "        #layer2 output of projection 1\n",
    "        lay_2_inp1_incept_1_to_3,lay_2_inp1_incept_1_to_5,inp1_lay_2_pool = self.layer_2_reducer(inp_1_lay_2_all_output,d_lay_1_to_2,m_p=m_p_l2)\n",
    "        lay_2_inp2_incept_1_to_3,lay_2_inp2_incept_1_to_5,inp2_lay_2_pool = self.layer_2_reducer(inp_2_lay_2_all_output,d_lay_1_to_2,m_p=m_p_l2)\n",
    "        lay_2_inp3_incept_1_to_3,lay_2_inp3_incept_1_to_5,inp3_lay_2_pool = self.layer_2_reducer(inp_3_lay_2_all_output,d_lay_1_to_2,m_p=m_p_l2)\n",
    "\n",
    "        '''place the size of NN layer_3 inputs'''\n",
    "        #since the stride is 1 ans same padding these equation works\n",
    "        lay_2_height=int(lay_1_height/m_p_l2)\n",
    "        lay_2_width = int(lay_1_height/m_p_l2)\n",
    "        \n",
    "        lay_2_inp_incept_1_to_3 = keras.Input(shape=(lay_2_height,lay_1_width, d_lay_1_to_2))\n",
    "        lay_2_inp_incept_1_to_5 = keras.Input(shape=(lay_2_width,lay_1_width, d_lay_1_to_2))      \n",
    "        inp_lay_2_pool = keras.Input(shape=(lay_2_height,lay_1_width,d1+d3+d5+d_max))\n",
    "\n",
    "        lay_3_incept_1 = layers.Conv2D(d1, (1,1),strides=(1,1),padding='same',activation=self.activation)(inp_lay_2_pool)\n",
    "        lay_3_incept_3 = layers.Conv2D(d3, (3,3),strides=(1,1),padding='same',activation=self.activation)(lay_2_inp_incept_1_to_3)\n",
    "        lay_3_incept_5 = layers.Conv2D(d5, (5,5),strides=(1,1),padding='same',activation=self.activation)(lay_2_inp_incept_1_to_5)\n",
    "        lay_3_incept_max_pool= layers.MaxPooling2D(pool_size=(3, 3),strides=(1,1),padding='same')(inp_lay_2_pool)\n",
    "        lay_3_incept_max_pool_depth=layers.Conv2D(d_max, (1,1),strides=(1,1),padding='same',activation='relu')(lay_3_incept_max_pool)\n",
    "        \n",
    "        lay_3_all_output = layers.concatenate([lay_3_incept_1, lay_3_incept_3,lay_3_incept_5,lay_3_incept_max_pool_depth], axis=3)\n",
    "        '''layer 3 general network'''    \n",
    "        layer_3_INCEPT_Net = keras.models.Model(inputs=[lay_2_inp_incept_1_to_3,lay_2_inp_incept_1_to_5,inp_lay_2_pool],outputs= lay_3_all_output, name='layer_3_INCEPT')     \n",
    "\n",
    "        '''Applying layer_3 in projections'''\n",
    "        #layer3 output of projection 1\n",
    "        inp_1_lay_3_incept_all_output = layer_3_INCEPT_Net([lay_2_inp1_incept_1_to_3,lay_2_inp1_incept_1_to_5,inp1_lay_2_pool]) \n",
    "        inp_2_lay_3_incept_all_output = layer_3_INCEPT_Net([lay_2_inp2_incept_1_to_3,lay_2_inp2_incept_1_to_5,inp2_lay_2_pool]) \n",
    "        inp_3_lay_3_incept_all_output = layer_3_INCEPT_Net([lay_2_inp3_incept_1_to_3,lay_2_inp3_incept_1_to_5,inp3_lay_2_pool]) \n",
    "      \n",
    "        #make residUAL NETWORK\n",
    "        inp_1_lay_3_all_output = layers.add([inp1_lay_2_pool, inp_1_lay_3_incept_all_output])\n",
    "        inp_2_lay_3_all_output = layers.add([inp2_lay_2_pool, inp_2_lay_3_incept_all_output])\n",
    "        inp_3_lay_3_all_output = layers.add([inp3_lay_2_pool, inp_3_lay_3_incept_all_output])\n",
    "\n",
    "        #layer3 output of projection 1\n",
    "        lay_3_inp1_incept_1_to_3,lay_3_inp1_incept_1_to_5,inp1_lay_3_pool = self.layer_3_reducer(inp_1_lay_3_all_output,d_lay_3_to_4,m_p=m_p_l3)\n",
    "        lay_3_inp2_incept_1_to_3,lay_3_inp2_incept_1_to_5,inp2_lay_3_pool = self.layer_3_reducer(inp_2_lay_3_all_output,d_lay_3_to_4,m_p=m_p_l3)\n",
    "        lay_3_inp3_incept_1_to_3,lay_3_inp3_incept_1_to_5,inp3_lay_3_pool = self.layer_3_reducer(inp_3_lay_3_all_output,d_lay_3_to_4,m_p=m_p_l3)\n",
    "\n",
    "        '''place the size of NN layer_4 inputs'''\n",
    "        #since the stride is 1 ans same padding these equation works\n",
    "        lay_3_height=int(lay_2_height/m_p_l3)\n",
    "        lay_3_width = int(lay_2_height/m_p_l3)\n",
    "        \n",
    "        lay_3_inp_incept_1_to_3 = keras.Input(shape=(lay_3_height, lay_3_width,d_lay_3_to_4))\n",
    "        lay_3_inp_incept_1_to_5 = keras.Input(shape=(lay_3_height, lay_3_width,d_lay_3_to_4))\n",
    "        inp_lay_3_pool = keras.Input(shape=(lay_3_height, lay_3_width,d1+d3+d5+d_max))       \n",
    "        \n",
    "        lay_4_incept_1 = layers.Conv2D(d1, (1,1),strides=(1,1),padding='same',activation=self.activation)(inp_lay_3_pool)\n",
    "        lay_4_incept_3 = layers.Conv2D(d3, (3,3),strides=(1,1),padding='same',activation=self.activation)(lay_3_inp_incept_1_to_3)\n",
    "        lay_4_incept_5 = layers.Conv2D(d5, (5,5),strides=(1,1),padding='same',activation=self.activation)(lay_3_inp_incept_1_to_5)\n",
    "        lay_4_incept_max_pool= layers.MaxPooling2D(pool_size=(3, 3),strides=(1,1),padding='same')(inp_lay_3_pool)\n",
    "        lay_4_incept_max_pool_depth=layers.Conv2D(d_max, (1,1),strides=(1,1),padding='same',activation='relu')(lay_4_incept_max_pool)\n",
    "        \n",
    "        lay_4_all_output = layers.concatenate([lay_4_incept_1, lay_4_incept_3,lay_4_incept_5,lay_4_incept_max_pool_depth], axis=3)\n",
    "    \n",
    "        '''layer 3 general network'''    \n",
    "        layer_4_INCEPT_Net = keras.models.Model(inputs=[lay_3_inp_incept_1_to_3,lay_3_inp_incept_1_to_5,inp_lay_3_pool], outputs=lay_4_all_output, name='layer_4_INCEPT')     \n",
    "        '''Applying layer_4 in projections'''\n",
    "        inp_1_lay_4_incept_all_output = layer_4_INCEPT_Net([lay_3_inp1_incept_1_to_3,lay_3_inp1_incept_1_to_5,inp1_lay_3_pool]) \n",
    "        inp_2_lay_4_incept_all_output = layer_4_INCEPT_Net([lay_3_inp2_incept_1_to_3,lay_3_inp2_incept_1_to_5,inp2_lay_3_pool]) \n",
    "        inp_3_lay_4_incept_all_output = layer_4_INCEPT_Net([lay_3_inp3_incept_1_to_3,lay_3_inp3_incept_1_to_5,inp3_lay_3_pool]) \n",
    "        #make residUAL NETWORK\n",
    "        inp_1_lay_4_all_output = layers.add([inp1_lay_3_pool, inp_1_lay_4_incept_all_output])\n",
    "        inp_2_lay_4_all_output = layers.add([inp2_lay_3_pool, inp_2_lay_4_incept_all_output])\n",
    "        inp_3_lay_4_all_output = layers.add([inp3_lay_3_pool, inp_3_lay_4_incept_all_output])\n",
    "\n",
    "        \n",
    "        #parallel = keras.models.Model(inputs,all_output, name='parallel') \n",
    "        tower_1 = self.layer_4_final(inp_1_lay_4_all_output,d_lay_3_to_4,m_p=m_p_l4)\n",
    "        tower_2 = self.layer_4_final(inp_2_lay_4_all_output,d_lay_3_to_4,m_p=m_p_l4)\n",
    "        tower_3 = self.layer_4_final(inp_3_lay_4_all_output,d_lay_3_to_4,m_p=m_p_l4)\n",
    "\n",
    "        \n",
    "        if self.tower_min_max_only:\n",
    "            tower_max = layers.maximum([tower_1,tower_2,tower_3])\n",
    "            tower_min=layers.minimum([tower_max, tower_2,tower_3])\n",
    "            #tower_average=layers.average([tower_1, tower_2,tower_3])\n",
    "            #tower_mul= layers.Multiply([tower_1, tower_2,tower_3])#since multiplication is expensive\n",
    "            merged = layers.concatenate([tower_max, tower_min], axis=1)\n",
    "            model_name = ''.join(['MIN_MAX_model_brain_incept_RESIDUAL_only_vin_1_act_',self.activation,'_f_inc_',str(f_inc),'_f_d_',str(f_d),'.h5'])\n",
    "        \n",
    "        else:\n",
    "            merged = layers.concatenate([tower_1, tower_2, tower_3], axis=1)\n",
    "            model_name = ''.join(['model_brain_incept_RESIDUAL_only_vin_1_act_',self.activation,'_f_inc_',str(f_inc),'_f_d_',str(f_d),'.h5'])\n",
    "        \n",
    "        merged = layers.Flatten()(merged)\n",
    "        all_co = layers.Dense(100, activation=self.activation)(merged)\n",
    "        all_co = layers.Dropout(0.5)(all_co)\n",
    "        all_co = layers.Dense(50, activation=self.activation)(all_co)\n",
    "        all_co= layers.Dropout(0.5)(all_co)\n",
    "        outputs = layers.Dense(3, activation='softmax')(all_co)\n",
    "        \n",
    "        model = keras.models.Model(inputs=[inp1, inp2,inp3], outputs=outputs, name=model_name)\n",
    "        \n",
    "        return model,model_name    \n",
    "    \n",
    "    \n",
    "class model_inception_only_mentor_vin_1:\n",
    "\n",
    "    def __init__(self, channels=17,tower_min_max_only=False,activation='relu'):\n",
    "        '''Initialization\n",
    "        \n",
    "        activation can be 'selu','swish','nishy_vin1'\n",
    "        '''\n",
    "        self.activation = activation\n",
    "        self.channels = channels\n",
    "        self.tower_min_max_only = tower_min_max_only\n",
    "\n",
    "    def parallel(self,inputs,f_inc,f_d):\n",
    "\n",
    "        d1=128*f_inc\n",
    "        d3=64*f_inc\n",
    "        d5=32*f_inc\n",
    "        d_max=32*f_inc  \n",
    "        d_lay_1_to_2 = 32*f_d\n",
    "        d_lay_3_to_4 = 64*f_d\n",
    "         \n",
    "        incept_1 = layers.Conv2D(d1, (1,1),strides=(1,1),padding='same',activation=self.activation)(inputs)\n",
    "        incept_3 = layers.Conv2D(d3, (3,3),strides=(1,1),padding='same',activation=self.activation)(inputs)\n",
    "        incept_5 = layers.Conv2D(d5, (5,5),strides=(1,1),padding='same',activation=self.activation)(inputs)\n",
    "        incept_max_pool= layers.MaxPooling2D(pool_size=(3, 3),strides=(1,1),padding='same')(inputs)\n",
    "        incept_max_pool_depth=layers.Conv2D(d_max, (1,1),strides=(1,1),padding='same',activation='relu')(incept_max_pool)\n",
    "        \n",
    "        all_output = layers.concatenate([incept_1, incept_3,incept_5,incept_max_pool_depth], axis=3)\n",
    "        layer_1_pool = layers.MaxPooling2D(pool_size=(4, 4))(all_output)\n",
    "        \n",
    "        #to reduce the depth representation\n",
    "        incept_1_to_3=layers.Conv2D(d_lay_1_to_2, (1,1),strides=(1,1),padding='same',activation=self.activation)(layer_1_pool)\n",
    "        incept_1_to_5=layers.Conv2D(d_lay_1_to_2, (1,1),strides=(1,1),padding='same',activation=self.activation)(layer_1_pool)\n",
    "        \n",
    "        incept_1 = layers.Conv2D(d1, (1,1),strides=(1,1),padding='same',activation=self.activation)(layer_1_pool)\n",
    "        incept_3 = layers.Conv2D(d3, (3,3),strides=(1,1),padding='same',activation=self.activation)(incept_1_to_3)\n",
    "        incept_5 = layers.Conv2D(d5, (5,5),strides=(1,1),padding='same',activation=self.activation)(incept_1_to_5)\n",
    "        incept_max_pool= layers.MaxPooling2D(pool_size=(3, 3),strides=(1,1),padding='same')(layer_1_pool)\n",
    "        incept_max_pool_depth=layers.Conv2D(d_max, (1,1),strides=(1,1),padding='same',activation='relu')(incept_max_pool)\n",
    "        \n",
    "        all_output = layers.concatenate([incept_1, incept_3,incept_5,incept_max_pool_depth], axis=3)\n",
    "        layer_2_pool = layers.MaxPooling2D(pool_size=(3, 3))(all_output)\n",
    "        \n",
    "        #to reduce the depth representation\n",
    "        incept_1_to_3=layers.Conv2D(d_lay_1_to_2, (1,1),strides=(1,1),padding='same',activation=self.activation)(layer_2_pool)\n",
    "        incept_1_to_5=layers.Conv2D(d_lay_1_to_2, (1,1),strides=(1,1),padding='same',activation=self.activation)(layer_2_pool)\n",
    "        \n",
    "        incept_1 = layers.Conv2D(d1, (1,1),strides=(1,1),padding='same',activation=self.activation)(layer_2_pool)\n",
    "        incept_3 = layers.Conv2D(d3, (3,3),strides=(1,1),padding='same',activation=self.activation)(incept_1_to_3)\n",
    "        incept_5 = layers.Conv2D(d5, (5,5),strides=(1,1),padding='same',activation=self.activation)(incept_1_to_5)\n",
    "        incept_max_pool= layers.MaxPooling2D(pool_size=(3, 3),strides=(1,1),padding='same')(layer_2_pool)\n",
    "        incept_max_pool_depth=layers.Conv2D(d_max, (1,1),strides=(1,1),padding='same',activation='relu')(incept_max_pool)\n",
    "        \n",
    "        all_output = layers.concatenate([incept_1, incept_3,incept_5,incept_max_pool_depth], axis=3)\n",
    "        layer_3_pool = layers.MaxPooling2D(pool_size=(2, 2))(all_output)\n",
    "        #to reduce the depth representation\n",
    "        incept_1_to_3=layers.Conv2D(d_lay_3_to_4, (1,1),strides=(1,1),padding='same',activation=self.activation)(layer_3_pool)\n",
    "        incept_1_to_5=layers.Conv2D(d_lay_3_to_4, (1,1),strides=(1,1),padding='same',activation=self.activation)(layer_3_pool)\n",
    "        \n",
    "        incept_1 = layers.Conv2D(d1, (1,1),strides=(1,1),padding='same',activation=self.activation)(layer_3_pool)\n",
    "        incept_3 = layers.Conv2D(d3, (3,3),strides=(1,1),padding='same',activation=self.activation)(incept_1_to_3)\n",
    "        incept_5 = layers.Conv2D(d5, (5,5),strides=(1,1),padding='same',activation=self.activation)(incept_1_to_5)\n",
    "        incept_max_pool= layers.MaxPooling2D(pool_size=(3, 3),strides=(1,1),padding='same')(layer_3_pool)\n",
    "        incept_max_pool_depth=layers.Conv2D(d_max, (1,1),strides=(1,1),padding='same',activation='relu')(incept_max_pool)\n",
    "        \n",
    "        all_output = layers.concatenate([incept_1, incept_3,incept_5,incept_max_pool_depth], axis=3)\n",
    "        layer_4_pool = layers.MaxPooling2D(pool_size=(2, 2))(all_output)\n",
    "        #\n",
    "        incept_1_to_final =layers.Conv2D(d_lay_3_to_4, (1,1),strides=(1,1),padding='same',activation=self.activation)(layer_4_pool)\n",
    "        return incept_1_to_final\n",
    "    \n",
    "    def model_maker(self,f_inc=1,f_d=1):\n",
    "\n",
    "        inp1 = keras.Input(shape=(200, 200, self.channels))\n",
    "        inp2 = keras.Input(shape=(200, 200, self.channels))\n",
    "        inp3 = keras.Input(shape=(200, 200, self.channels))\n",
    "        \n",
    "        tower_1 = self.parallel(inp1,f_inc,f_d)\n",
    "        tower_2 = self.parallel(inp2,f_inc,f_d)\n",
    "        tower_3 = self.parallel(inp3,f_inc,f_d)\n",
    "        \n",
    "        if self.tower_min_max_only:\n",
    "            tower_max = layers.maximum([tower_1,tower_2,tower_3])\n",
    "            tower_min=layers.minimum([tower_max, tower_2,tower_3])\n",
    "            #tower_average=layers.average([tower_1, tower_2,tower_3])\n",
    "            #tower_mul= layers.Multiply([tower_1, tower_2,tower_3])#since multiplication is expensive\n",
    "            merged = layers.concatenate([tower_max, tower_min], axis=1)\n",
    "            model_name = ''.join(['MIN_MAX_model_inception_only_mentor_vin_1_act_',self.activation,'_f_inc_',str(f_inc),'_f_d_',str(f_d),'.h5'])\n",
    "        \n",
    "        else:\n",
    "            merged = layers.concatenate([tower_1, tower_2, tower_3], axis=1)\n",
    "            model_name = ''.join(['model_inception_only_mentor_vin_1_act_',self.activation,'_f_inc_',str(f_inc),'_f_d_',str(f_d),'.h5'])\n",
    "        \n",
    "        merged = layers.Flatten()(merged)\n",
    "        all_co = layers.Dense(100, activation=self.activation)(merged)\n",
    "        all_co = layers.Dropout(0.5)(all_co)\n",
    "        all_co = layers.Dense(50, activation=self.activation)(all_co)\n",
    "        all_co= layers.Dropout(0.5)(all_co)\n",
    "        outputs = layers.Dense(3, activation='softmax')(all_co)\n",
    "        \n",
    "        model = keras.models.Model(inputs=[inp1, inp2,inp3], outputs=outputs, name=model_name)\n",
    "        \n",
    "        return model,model_name    \n",
    "\n",
    "class model_inception_residual_mentor_vin_1:\n",
    "\n",
    "    def __init__(self, channels=17,tower_min_max_only=False,activation='relu'):\n",
    "        '''Initialization\n",
    "        \n",
    "        activation can be 'selu','swish','nishy_vin1'\n",
    "        '''\n",
    "        self.activation = activation\n",
    "        self.channels = channels\n",
    "        self.tower_min_max_only = tower_min_max_only\n",
    "\n",
    "    def parallel(self,inputs,f_inc,f_d):\n",
    "\n",
    "        d1=128*f_inc\n",
    "        d3=64*f_inc\n",
    "        d5=32*f_inc\n",
    "        d_max=32*f_inc  \n",
    "        d_lay_1_to_2 = 32*f_d\n",
    "        d_lay_3_to_4 = 64*f_d\n",
    "         \n",
    "        incept_1 = layers.Conv2D(d1, (1,1),strides=(1,1),padding='same',activation=self.activation)(inputs)\n",
    "        incept_3 = layers.Conv2D(d3, (3,3),strides=(1,1),padding='same',activation=self.activation)(inputs)\n",
    "        incept_5 = layers.Conv2D(d5, (5,5),strides=(1,1),padding='same',activation=self.activation)(inputs)\n",
    "        incept_max_pool= layers.MaxPooling2D(pool_size=(3, 3),strides=(1,1),padding='same')(inputs)\n",
    "        incept_max_pool_depth=layers.Conv2D(d_max, (1,1),strides=(1,1),padding='same',activation='relu')(incept_max_pool)\n",
    "        \n",
    "        all_output = layers.concatenate([incept_1, incept_3,incept_5,incept_max_pool_depth], axis=3)\n",
    "        layer_1_pool = layers.MaxPooling2D(pool_size=(4, 4))(all_output)\n",
    "        \n",
    "        #to reduce the depth representation\n",
    "        incept_1_to_3=layers.Conv2D(d_lay_1_to_2, (1,1),strides=(1,1),padding='same',activation=self.activation)(layer_1_pool)\n",
    "        incept_1_to_5=layers.Conv2D(d_lay_1_to_2, (1,1),strides=(1,1),padding='same',activation=self.activation)(layer_1_pool)\n",
    "        \n",
    "        incept_1 = layers.Conv2D(d1, (1,1),strides=(1,1),padding='same',activation=self.activation)(layer_1_pool)\n",
    "        incept_3 = layers.Conv2D(d3, (3,3),strides=(1,1),padding='same',activation=self.activation)(incept_1_to_3)\n",
    "        incept_5 = layers.Conv2D(d5, (5,5),strides=(1,1),padding='same',activation=self.activation)(incept_1_to_5)\n",
    "        incept_max_pool= layers.MaxPooling2D(pool_size=(3, 3),strides=(1,1),padding='same')(layer_1_pool)\n",
    "        incept_max_pool_depth=layers.Conv2D(d_max, (1,1),strides=(1,1),padding='same',activation='relu')(incept_max_pool)\n",
    "        \n",
    "        all_output = layers.concatenate([incept_1, incept_3,incept_5,incept_max_pool_depth], axis=3)\n",
    "        \n",
    "        layer_1_2_pool=layers.Conv2D(d1+d3+d5+d_max, (1,1),strides=(1,1),padding='same',activation=self.activation)(layer_1_pool)\n",
    "        layer_21_residual = layers.add([layer_1_2_pool, all_output])\n",
    "        \n",
    "        layer_2_pool = layers.MaxPooling2D(pool_size=(3, 3))(layer_21_residual)\n",
    "        #to reduce the depth representation\n",
    "        incept_1_to_3=layers.Conv2D(d_lay_1_to_2, (1,1),strides=(1,1),padding='same',activation=self.activation)(layer_2_pool)\n",
    "        incept_1_to_5=layers.Conv2D(d_lay_1_to_2, (1,1),strides=(1,1),padding='same',activation=self.activation)(layer_2_pool)\n",
    "        \n",
    "        incept_1 = layers.Conv2D(d1, (1,1),strides=(1,1),padding='same',activation=self.activation)(layer_2_pool)\n",
    "        incept_3 = layers.Conv2D(d3, (3,3),strides=(1,1),padding='same',activation=self.activation)(incept_1_to_3)\n",
    "        incept_5 = layers.Conv2D(d5, (5,5),strides=(1,1),padding='same',activation=self.activation)(incept_1_to_5)\n",
    "        incept_max_pool= layers.MaxPooling2D(pool_size=(3, 3),strides=(1,1),padding='same')(layer_2_pool)\n",
    "        incept_max_pool_depth=layers.Conv2D(d_max, (1,1),strides=(1,1),padding='same',activation='relu')(incept_max_pool)\n",
    "        \n",
    "        all_output = layers.concatenate([incept_1, incept_3,incept_5,incept_max_pool_depth], axis=3)\n",
    "        layer_2_3_pool=layers.Conv2D(d1+d3+d5+d_max, (1,1),strides=(1,1),padding='same',activation=self.activation)(layer_2_pool)\n",
    "        layer_32_residual = layers.add([layer_2_3_pool, all_output])\n",
    "        \n",
    "        layer_3_pool = layers.MaxPooling2D(pool_size=(3, 3))(layer_32_residual)\n",
    "        #to reduce the depth representation\n",
    "        incept_1_to_3=layers.Conv2D(d_lay_3_to_4, (1,1),strides=(1,1),padding='same',activation=self.activation)(layer_3_pool)\n",
    "        incept_1_to_5=layers.Conv2D(d_lay_3_to_4, (1,1),strides=(1,1),padding='same',activation=self.activation)(layer_3_pool)\n",
    "        \n",
    "        incept_1 = layers.Conv2D(d1, (1,1),strides=(1,1),padding='same',activation=self.activation)(layer_3_pool)\n",
    "        incept_3 = layers.Conv2D(d3, (3,3),strides=(1,1),padding='same',activation=self.activation)(incept_1_to_3)\n",
    "        incept_5 = layers.Conv2D(d5, (5,5),strides=(1,1),padding='same',activation=self.activation)(incept_1_to_5)\n",
    "        incept_max_pool= layers.MaxPooling2D(pool_size=(3, 3),strides=(1,1),padding='same')(layer_3_pool)\n",
    "        incept_max_pool_depth=layers.Conv2D(d_max, (1,1),strides=(1,1),padding='same',activation='relu')(incept_max_pool)\n",
    "        \n",
    "        all_output = layers.concatenate([incept_1, incept_3,incept_5,incept_max_pool_depth], axis=3)\n",
    "        layer_3_4_pool=layers.Conv2D(d1+d3+d5+d_max, (1,1),strides=(1,1),padding='same',activation=self.activation)(layer_3_pool)\n",
    "        layer_34_residual = layers.add([layer_3_4_pool, all_output])\n",
    "        \n",
    "        layer_4_pool = layers.MaxPooling2D(pool_size=(3, 3))(layer_34_residual)\n",
    "        #\n",
    "        incept_1_to_final =layers.Conv2D(d_lay_3_to_4, (1,1),strides=(1,1),padding='same',activation=self.activation)(layer_4_pool)\n",
    "        return incept_1_to_final\n",
    "    \n",
    "    def model_maker(self,f_inc=1,f_d=1):\n",
    "\n",
    "        inp1 = keras.Input(shape=(200, 200, self.channels))\n",
    "        inp2 = keras.Input(shape=(200, 200, self.channels))\n",
    "        inp3 = keras.Input(shape=(200, 200, self.channels))\n",
    "        \n",
    "        tower_1 = self.parallel(inp1,f_inc,f_d)\n",
    "        tower_2 = self.parallel(inp2,f_inc,f_d)\n",
    "        tower_3 = self.parallel(inp3,f_inc,f_d)\n",
    "        \n",
    "        if self.tower_min_max_only:\n",
    "            tower_max = layers.maximum([tower_1,tower_2,tower_3])\n",
    "            tower_min=layers.minimum([tower_max, tower_2,tower_3])\n",
    "            #tower_average=layers.average([tower_1, tower_2,tower_3])\n",
    "            #tower_mul= layers.Multiply([tower_1, tower_2,tower_3])#since multiplication is expensive\n",
    "            merged = layers.concatenate([tower_max, tower_min], axis=1)\n",
    "            model_name = ''.join(['MIN_MAX_model_inception_residual_mentor_vin_1_act_',self.activation,'_f_inc_',str(f_inc),'_f_d_',str(f_d),'.h5'])\n",
    "        \n",
    "        else:\n",
    "            merged = layers.concatenate([tower_1, tower_2, tower_3], axis=1)\n",
    "            model_name = ''.join(['model_inception_residual_mentor_vin_1_act_',self.activation,'_f_inc_',str(f_inc),'_f_d_',str(f_d),'.h5'])\n",
    "        \n",
    "        merged = layers.Flatten()(merged)\n",
    "        all_co = layers.Dense(100, activation=self.activation)(merged)\n",
    "        all_co = layers.Dropout(0.5)(all_co)\n",
    "        all_co = layers.Dense(50, activation=self.activation)(all_co)\n",
    "        all_co= layers.Dropout(0.5)(all_co)\n",
    "        outputs = layers.Dense(3, activation='softmax')(all_co)\n",
    "        \n",
    "        model = keras.models.Model(inputs=[inp1, inp2,inp3], outputs=outputs, name=model_name)\n",
    "        \n",
    "        return model,model_name    \n",
    "    \n",
    "'''parallel inception models'''\n",
    "class model_par_inception_only_mentor_vin_1:\n",
    "\n",
    "    def __init__(self, channels=17,tower_min_max_only=False,activation='relu'):\n",
    "        '''Initialization\n",
    "        \n",
    "        activation can be 'selu','swish','nishy_vin1'\n",
    "        '''\n",
    "        self.activation = activation\n",
    "        self.channels = channels\n",
    "        self.tower_min_max_only = tower_min_max_only\n",
    "\n",
    "    def model_maker(self,f_inc=1,f_d=1):\n",
    "\n",
    "        d1=128*f_inc\n",
    "        d3=64*f_inc\n",
    "        d5=32*f_inc\n",
    "        d_max=32*f_inc  \n",
    "        d_lay_1_to_2 = 32*f_d\n",
    "        d_lay_3_to_4 = 64*f_d\n",
    "\n",
    "        inputs = keras.Input(shape=(200, 200, self.channels))\n",
    "        \n",
    "        inp1 = keras.Input(shape=(200, 200, self.channels))\n",
    "        inp2 = keras.Input(shape=(200, 200, self.channels))\n",
    "        inp3 = keras.Input(shape=(200, 200, self.channels))    \n",
    "         \n",
    "        incept_1 = layers.Conv2D(d1, (1,1),strides=(1,1),padding='same',activation=self.activation)(inputs)\n",
    "        incept_3 = layers.Conv2D(d3, (3,3),strides=(1,1),padding='same',activation=self.activation)(inputs)\n",
    "        incept_5 = layers.Conv2D(d5, (5,5),strides=(1,1),padding='same',activation=self.activation)(inputs)\n",
    "        incept_max_pool= layers.MaxPooling2D(pool_size=(3, 3),strides=(1,1),padding='same')(inputs)\n",
    "        incept_max_pool_depth=layers.Conv2D(d_max, (1,1),strides=(1,1),padding='same',activation='relu')(incept_max_pool)\n",
    "        \n",
    "        all_output = layers.concatenate([incept_1, incept_3,incept_5,incept_max_pool_depth], axis=3)\n",
    "        layer_1_pool = layers.MaxPooling2D(pool_size=(4, 4))(all_output)\n",
    "        \n",
    "        #to reduce the depth representation\n",
    "        incept_1_to_3=layers.Conv2D(d_lay_1_to_2, (1,1),strides=(1,1),padding='same',activation=self.activation)(layer_1_pool)\n",
    "        incept_1_to_5=layers.Conv2D(d_lay_1_to_2, (1,1),strides=(1,1),padding='same',activation=self.activation)(layer_1_pool)\n",
    "        \n",
    "        incept_1 = layers.Conv2D(d1, (1,1),strides=(1,1),padding='same',activation=self.activation)(layer_1_pool)\n",
    "        incept_3 = layers.Conv2D(d3, (3,3),strides=(1,1),padding='same',activation=self.activation)(incept_1_to_3)\n",
    "        incept_5 = layers.Conv2D(d5, (5,5),strides=(1,1),padding='same',activation=self.activation)(incept_1_to_5)\n",
    "        incept_max_pool= layers.MaxPooling2D(pool_size=(3, 3),strides=(1,1),padding='same')(layer_1_pool)\n",
    "        incept_max_pool_depth=layers.Conv2D(d_max, (1,1),strides=(1,1),padding='same',activation='relu')(incept_max_pool)\n",
    "        \n",
    "        all_output = layers.concatenate([incept_1, incept_3,incept_5,incept_max_pool_depth], axis=3)\n",
    "        layer_2_pool = layers.MaxPooling2D(pool_size=(3, 3))(all_output)\n",
    "        \n",
    "        #to reduce the depth representation\n",
    "        incept_1_to_3=layers.Conv2D(d_lay_1_to_2, (1,1),strides=(1,1),padding='same',activation=self.activation)(layer_2_pool)\n",
    "        incept_1_to_5=layers.Conv2D(d_lay_1_to_2, (1,1),strides=(1,1),padding='same',activation=self.activation)(layer_2_pool)\n",
    "        \n",
    "        incept_1 = layers.Conv2D(d1, (1,1),strides=(1,1),padding='same',activation=self.activation)(layer_2_pool)\n",
    "        incept_3 = layers.Conv2D(d3, (3,3),strides=(1,1),padding='same',activation=self.activation)(incept_1_to_3)\n",
    "        incept_5 = layers.Conv2D(d5, (5,5),strides=(1,1),padding='same',activation=self.activation)(incept_1_to_5)\n",
    "        incept_max_pool= layers.MaxPooling2D(pool_size=(3, 3),strides=(1,1),padding='same')(layer_2_pool)\n",
    "        incept_max_pool_depth=layers.Conv2D(d_max, (1,1),strides=(1,1),padding='same',activation='relu')(incept_max_pool)\n",
    "        \n",
    "        all_output = layers.concatenate([incept_1, incept_3,incept_5,incept_max_pool_depth], axis=3)\n",
    "        layer_3_pool = layers.MaxPooling2D(pool_size=(2, 2))(all_output)\n",
    "        #to reduce the depth representation\n",
    "        incept_1_to_3=layers.Conv2D(d_lay_3_to_4, (1,1),strides=(1,1),padding='same',activation=self.activation)(layer_3_pool)\n",
    "        incept_1_to_5=layers.Conv2D(d_lay_3_to_4, (1,1),strides=(1,1),padding='same',activation=self.activation)(layer_3_pool)\n",
    "        \n",
    "        incept_1 = layers.Conv2D(d1, (1,1),strides=(1,1),padding='same',activation=self.activation)(layer_3_pool)\n",
    "        incept_3 = layers.Conv2D(d3, (3,3),strides=(1,1),padding='same',activation=self.activation)(incept_1_to_3)\n",
    "        incept_5 = layers.Conv2D(d5, (5,5),strides=(1,1),padding='same',activation=self.activation)(incept_1_to_5)\n",
    "        incept_max_pool= layers.MaxPooling2D(pool_size=(3, 3),strides=(1,1),padding='same')(layer_3_pool)\n",
    "        incept_max_pool_depth=layers.Conv2D(d_max, (1,1),strides=(1,1),padding='same',activation='relu')(incept_max_pool)\n",
    "        \n",
    "        all_output = layers.concatenate([incept_1, incept_3,incept_5,incept_max_pool_depth], axis=3)\n",
    "        layer_4_pool = layers.MaxPooling2D(pool_size=(2, 2))(all_output)\n",
    "        #\n",
    "        incept_1_to_final =layers.Conv2D(d_lay_3_to_4, (1,1),strides=(1,1),padding='same',activation=self.activation)(layer_4_pool)\n",
    "        \n",
    "        parallel = keras.models.Model(inputs, incept_1_to_final, name='parallel')     \n",
    "        #parallel = keras.models.Model(inputs,all_output, name='parallel') \n",
    "        tower_1 = parallel(inp1)\n",
    "        tower_2 = parallel(inp2)\n",
    "        tower_3 = parallel(inp3)\n",
    "        \n",
    "        if self.tower_min_max_only:\n",
    "            tower_max = layers.maximum([tower_1,tower_2,tower_3])\n",
    "            tower_min=layers.minimum([tower_max, tower_2,tower_3])\n",
    "            #tower_average=layers.average([tower_1, tower_2,tower_3])\n",
    "            #tower_mul= layers.Multiply([tower_1, tower_2,tower_3])#since multiplication is expensive\n",
    "            merged = layers.concatenate([tower_max, tower_min], axis=1)\n",
    "            model_name = ''.join(['MIN_MAX_model_par_inception_only_mentor_vin_1_act_',self.activation,'_f_inc_',str(f_inc),'_f_d_',str(f_d),'.h5'])\n",
    "        \n",
    "        else:\n",
    "            merged = layers.concatenate([tower_1, tower_2, tower_3], axis=1)\n",
    "            model_name = ''.join(['model_par_inception_only_mentor_vin_1_act_',self.activation,'_f_inc_',str(f_inc),'_f_d_',str(f_d),'.h5'])\n",
    "        \n",
    "        merged = layers.Flatten()(merged)\n",
    "        all_co = layers.Dense(100, activation=self.activation)(merged)\n",
    "        all_co = layers.Dropout(0.5)(all_co)\n",
    "        all_co = layers.Dense(50, activation=self.activation)(all_co)\n",
    "        all_co= layers.Dropout(0.5)(all_co)\n",
    "        outputs = layers.Dense(3, activation='softmax')(all_co)\n",
    "        \n",
    "        model = keras.models.Model(inputs=[inp1, inp2,inp3], outputs=outputs, name=model_name)\n",
    "        \n",
    "        return model,model_name    \n",
    "\n",
    "class model_par_inception_residual_mentor_vin_1:\n",
    "\n",
    "    def __init__(self, channels=17,tower_min_max_only=False,activation='relu'):\n",
    "        '''Initialization\n",
    "        \n",
    "        activation can be 'selu','swish','nishy_vin1'\n",
    "        '''\n",
    "        self.activation = activation\n",
    "        self.channels = channels\n",
    "        self.tower_min_max_only = tower_min_max_only\n",
    "\n",
    "    def model_maker(self,f_inc=1,f_d=1):\n",
    "\n",
    "        d1=128*f_inc\n",
    "        d3=64*f_inc\n",
    "        d5=32*f_inc\n",
    "        d_max=32*f_inc  \n",
    "        d_lay_1_to_2 = 32*f_d\n",
    "        d_lay_3_to_4 = 64*f_d\n",
    "\n",
    "        inputs = keras.Input(shape=(200, 200, self.channels))\n",
    "        \n",
    "        inp1 = keras.Input(shape=(200, 200, self.channels))\n",
    "        inp2 = keras.Input(shape=(200, 200, self.channels))\n",
    "        inp3 = keras.Input(shape=(200, 200, self.channels))      \n",
    "         \n",
    "        incept_1 = layers.Conv2D(d1, (1,1),strides=(1,1),padding='same',activation=self.activation)(inputs)\n",
    "        incept_3 = layers.Conv2D(d3, (3,3),strides=(1,1),padding='same',activation=self.activation)(inputs)\n",
    "        incept_5 = layers.Conv2D(d5, (5,5),strides=(1,1),padding='same',activation=self.activation)(inputs)\n",
    "        incept_max_pool= layers.MaxPooling2D(pool_size=(3, 3),strides=(1,1),padding='same')(inputs)\n",
    "        incept_max_pool_depth=layers.Conv2D(d_max, (1,1),strides=(1,1),padding='same',activation='relu')(incept_max_pool)\n",
    "        \n",
    "        all_output = layers.concatenate([incept_1, incept_3,incept_5,incept_max_pool_depth], axis=3)\n",
    "        layer_1_pool = layers.MaxPooling2D(pool_size=(4, 4))(all_output)\n",
    "        \n",
    "        #to reduce the depth representation\n",
    "        incept_1_to_3=layers.Conv2D(d_lay_1_to_2, (1,1),strides=(1,1),padding='same',activation=self.activation)(layer_1_pool)\n",
    "        incept_1_to_5=layers.Conv2D(d_lay_1_to_2, (1,1),strides=(1,1),padding='same',activation=self.activation)(layer_1_pool)\n",
    "        \n",
    "        incept_1 = layers.Conv2D(d1, (1,1),strides=(1,1),padding='same',activation=self.activation)(layer_1_pool)\n",
    "        incept_3 = layers.Conv2D(d3, (3,3),strides=(1,1),padding='same',activation=self.activation)(incept_1_to_3)\n",
    "        incept_5 = layers.Conv2D(d5, (5,5),strides=(1,1),padding='same',activation=self.activation)(incept_1_to_5)\n",
    "        incept_max_pool= layers.MaxPooling2D(pool_size=(3, 3),strides=(1,1),padding='same')(layer_1_pool)\n",
    "        incept_max_pool_depth=layers.Conv2D(d_max, (1,1),strides=(1,1),padding='same',activation='relu')(incept_max_pool)\n",
    "        \n",
    "        all_output = layers.concatenate([incept_1, incept_3,incept_5,incept_max_pool_depth], axis=3)\n",
    "        \n",
    "        layer_1_2_pool=layers.Conv2D(d1+d3+d5+d_max, (1,1),strides=(1,1),padding='same',activation=self.activation)(layer_1_pool)\n",
    "        layer_21_residual = layers.add([layer_1_2_pool, all_output])\n",
    "        \n",
    "        layer_2_pool = layers.MaxPooling2D(pool_size=(3, 3))(layer_21_residual)\n",
    "        #to reduce the depth representation\n",
    "        incept_1_to_3=layers.Conv2D(d_lay_1_to_2, (1,1),strides=(1,1),padding='same',activation=self.activation)(layer_2_pool)\n",
    "        incept_1_to_5=layers.Conv2D(d_lay_1_to_2, (1,1),strides=(1,1),padding='same',activation=self.activation)(layer_2_pool)\n",
    "        \n",
    "        incept_1 = layers.Conv2D(d1, (1,1),strides=(1,1),padding='same',activation=self.activation)(layer_2_pool)\n",
    "        incept_3 = layers.Conv2D(d3, (3,3),strides=(1,1),padding='same',activation=self.activation)(incept_1_to_3)\n",
    "        incept_5 = layers.Conv2D(d5, (5,5),strides=(1,1),padding='same',activation=self.activation)(incept_1_to_5)\n",
    "        incept_max_pool= layers.MaxPooling2D(pool_size=(3, 3),strides=(1,1),padding='same')(layer_2_pool)\n",
    "        incept_max_pool_depth=layers.Conv2D(d_max, (1,1),strides=(1,1),padding='same',activation='relu')(incept_max_pool)\n",
    "        \n",
    "        all_output = layers.concatenate([incept_1, incept_3,incept_5,incept_max_pool_depth], axis=3)\n",
    "        layer_2_3_pool=layers.Conv2D(d1+d3+d5+d_max, (1,1),strides=(1,1),padding='same',activation=self.activation)(layer_2_pool)\n",
    "        layer_32_residual = layers.add([layer_2_3_pool, all_output])\n",
    "        \n",
    "        layer_3_pool = layers.MaxPooling2D(pool_size=(3, 3))(layer_32_residual)\n",
    "        #to reduce the depth representation\n",
    "        incept_1_to_3=layers.Conv2D(d_lay_3_to_4, (1,1),strides=(1,1),padding='same',activation=self.activation)(layer_3_pool)\n",
    "        incept_1_to_5=layers.Conv2D(d_lay_3_to_4, (1,1),strides=(1,1),padding='same',activation=self.activation)(layer_3_pool)\n",
    "        \n",
    "        incept_1 = layers.Conv2D(d1, (1,1),strides=(1,1),padding='same',activation=self.activation)(layer_3_pool)\n",
    "        incept_3 = layers.Conv2D(d3, (3,3),strides=(1,1),padding='same',activation=self.activation)(incept_1_to_3)\n",
    "        incept_5 = layers.Conv2D(d5, (5,5),strides=(1,1),padding='same',activation=self.activation)(incept_1_to_5)\n",
    "        incept_max_pool= layers.MaxPooling2D(pool_size=(3, 3),strides=(1,1),padding='same')(layer_3_pool)\n",
    "        incept_max_pool_depth=layers.Conv2D(d_max, (1,1),strides=(1,1),padding='same',activation='relu')(incept_max_pool)\n",
    "        \n",
    "        all_output = layers.concatenate([incept_1, incept_3,incept_5,incept_max_pool_depth], axis=3)\n",
    "        layer_3_4_pool=layers.Conv2D(d1+d3+d5+d_max, (1,1),strides=(1,1),padding='same',activation=self.activation)(layer_3_pool)\n",
    "        layer_34_residual = layers.add([layer_3_4_pool, all_output])\n",
    "        \n",
    "        layer_4_pool = layers.MaxPooling2D(pool_size=(3, 3))(layer_34_residual)\n",
    "        #\n",
    "        incept_1_to_final =layers.Conv2D(d_lay_3_to_4, (1,1),strides=(1,1),padding='same',activation=self.activation)(layer_4_pool)\n",
    "        parallel = keras.models.Model(inputs, incept_1_to_final, name='parallel')     \n",
    "        #parallel = keras.models.Model(inputs,all_output, name='parallel') \n",
    "        tower_1 = parallel(inp1)\n",
    "        tower_2 = parallel(inp2)\n",
    "        tower_3 = parallel(inp3)\n",
    "        \n",
    "        if self.tower_min_max_only:\n",
    "            tower_max = layers.maximum([tower_1,tower_2,tower_3])\n",
    "            tower_min=layers.minimum([tower_max, tower_2,tower_3])\n",
    "            #tower_average=layers.average([tower_1, tower_2,tower_3])\n",
    "            #tower_mul= layers.Multiply([tower_1, tower_2,tower_3])#since multiplication is expensive\n",
    "            merged = layers.concatenate([tower_max, tower_min], axis=1)\n",
    "            model_name = ''.join(['MIN_MAX_model_par_inception_residual_mentor_vin_1_act_',self.activation,'_f_inc_',str(f_inc),'_f_d_',str(f_d),'.h5'])\n",
    "        \n",
    "        else:\n",
    "            merged = layers.concatenate([tower_1, tower_2, tower_3], axis=1)\n",
    "            model_name = ''.join(['model_par_inception_residual_mentor_vin_1_act_',self.activation,'_f_inc_',str(f_inc),'_f_d_',str(f_d),'.h5'])\n",
    "        \n",
    "        merged = layers.Flatten()(merged)\n",
    "        all_co = layers.Dense(100, activation=self.activation)(merged)\n",
    "        all_co = layers.Dropout(0.5)(all_co)\n",
    "        all_co = layers.Dense(50, activation=self.activation)(all_co)\n",
    "        all_co= layers.Dropout(0.5)(all_co)\n",
    "        outputs = layers.Dense(3, activation='softmax')(all_co)\n",
    "        \n",
    "        model = keras.models.Model(inputs=[inp1, inp2,inp3], outputs=outputs, name=model_name)\n",
    "        \n",
    "        return model,model_name    \n",
    "\n",
    "class model_vin_4:\n",
    "    \n",
    "    def __init__(self,channels=17,tower_min_max_only=False):\n",
    "        'Initialization'\n",
    "        self.channels = channels\n",
    "        self.tower_min_max_only = tower_min_max_only\n",
    "    \n",
    "    def parallel(self,inputs,k1,k2,k3,k4):\n",
    "    \n",
    "        xy = layers.Conv2D(32, (k1,k1),strides=(1,1),padding='same',activation='relu')(inputs)\n",
    "        block_1_xy_output = layers.MaxPooling2D(pool_size=(4, 4))(xy)\n",
    "        \n",
    "        xy = layers.Conv2D(32, (k2,k2),strides=(1,1),padding='same',activation='relu')(block_1_xy_output)\n",
    "        block_2_output = layers.add([xy, block_1_xy_output])\n",
    "    \n",
    "        block_2_xy_output = layers.MaxPooling2D(pool_size=(2, 2))(block_2_output)\n",
    "    \n",
    "        xy = layers.Conv2D(64, (k3,k3), activation='relu', padding='same')(block_2_xy_output)\n",
    "        '''to be removed'''\n",
    "        xy = layers.MaxPooling2D(pool_size=(2, 2))(xy)\n",
    "        xy = layers.Conv2D(64, (k4,k4), activation='relu', padding='same')(xy)\n",
    "        block_3_xy_output = layers.MaxPooling2D(pool_size=(2, 2))(xy)\n",
    "        return block_3_xy_output\n",
    " \n",
    "#    def model_maker_same_kernal_projections(self,p1=7,p2=5,p3=3):\n",
    "    def model_maker(self,p1=3,p2=5):\n",
    "        \n",
    "        inp1 = keras.Input(shape=(200, 200, self.channels))\n",
    "        inp2 = keras.Input(shape=(200, 200, self.channels))\n",
    "        inp3 = keras.Input(shape=(200, 200, self.channels))\n",
    "        \n",
    "        tower_1 = self.parallel(inp1,p2,p2,p1,p1)\n",
    "        tower_2 = self.parallel(inp2,p2,p2,p1,p1)\n",
    "        tower_3 = self.parallel(inp3,p2,p2,p1,p1)\n",
    "        \n",
    "        if self.tower_min_max_only:\n",
    "            tower_max = layers.maximum([tower_1,tower_2,tower_3])\n",
    "            tower_min=layers.minimum([tower_max, tower_2,tower_3])\n",
    "            #tower_average=layers.average([tower_1, tower_2,tower_3])\n",
    "            #tower_mul= layers.Multiply([tower_1, tower_2,tower_3])#since multiplication is expensive\n",
    "            merged = layers.concatenate([tower_max, tower_min], axis=1)\n",
    "            model_name =''.join(['MIN_MAX_model_vin_4_p1_',str(p1),'_p2_',str(p2),'.h5'])\n",
    "\n",
    "        else:\n",
    "            merged = layers.concatenate([tower_1, tower_2, tower_3], axis=1)\n",
    "            model_name =''.join(['model_vin_4_p1_',str(p1),'_p2_',str(p2),'.h5'])\n",
    "      \n",
    "        \n",
    "        merged = layers.Flatten()(merged)\n",
    "        all_co = layers.Dense(100, activation='relu')(merged)\n",
    "        all_co = layers.Dropout(0.5)(all_co)\n",
    "        all_co = layers.Dense(50, activation='relu')(all_co)\n",
    "        all_co= layers.Dropout(0.5)(all_co)\n",
    "        outputs = layers.Dense(3, activation='softmax')(all_co)\n",
    "\n",
    "        model_name = 'model_vin_4_same.h5'\n",
    "        model = keras.Model(inputs=[inp1, inp2,inp3], outputs=outputs, name='model_vin_4_same')\n",
    "        \n",
    "        return model,model_name\n",
    "    \n",
    "#    def model_maker_same_kernal_combo_projections(self,p11=7,p12=3,p21=5,p22=5,p31=3,p32=7):\n",
    "#        \n",
    "#        inp1 = keras.Input(shape=(200, 200, self.channels))\n",
    "#        inp2 = keras.Input(shape=(200, 200, self.channels))\n",
    "#        inp3 = keras.Input(shape=(200, 200, self.channels))\n",
    "#        \n",
    "#        tower_1 = self.parallel(inp1,p11,p11,p12,p12)\n",
    "#        tower_2 = self.parallel(inp2,p21,p21,p22,p22)\n",
    "#        tower_3 = self.parallel(inp3,p31,p32,p31,p32)\n",
    "#        \n",
    "#        merged = layers.concatenate([tower_1, tower_2, tower_3], axis=1)\n",
    "#        merged = layers.Flatten()(merged)\n",
    "#        all_co = layers.Dense(100, activation='relu')(merged)\n",
    "#        all_co = layers.Dropout(0.5)(all_co)\n",
    "#        all_co = layers.Dense(50, activation='relu')(all_co)\n",
    "#        all_co= layers.Dropout(0.5)(all_co)\n",
    "#        outputs = layers.Dense(3, activation='softmax')(all_co)\n",
    "#\n",
    "#        model_name = 'model_vin_4_combo.h5'\n",
    "#        model = keras.Model(inputs=[inp1, inp2,inp3], outputs=outputs, name='model_vin_4_combo')\n",
    "#\n",
    "#        return model,model_name\n",
    "\n",
    "\n",
    "class model_accidental:\n",
    "    \n",
    "    def __init__(self,channels=17,tower_min_max_only=False):\n",
    "        'Initialization'\n",
    "        self.channels = channels\n",
    "        self.tower_min_max_only = tower_min_max_only\n",
    "    \n",
    "    def parallel(self,inputs,k1,k2,k3,k4,d1):\n",
    "    \n",
    "        xy = layers.Conv2D(d1, (k1,k1),strides=(1,1),padding='same',activation='relu')(inputs)\n",
    "        block_1_xy_output = layers.MaxPooling2D(pool_size=(4, 4))(xy)\n",
    "        \n",
    "        xy = layers.Conv2D(d1, (k2,k2),strides=(1,1),padding='same',activation='relu')(block_1_xy_output)\n",
    "        block_2_xy_output = layers.MaxPooling2D(pool_size=(2, 2))(xy)\n",
    "    \n",
    "        xy = layers.Conv2D(2*d1, (k3,k3), activation='relu', padding='same')(block_2_xy_output)\n",
    "        xy = layers.MaxPooling2D(pool_size=(2, 2))(xy)\n",
    "\n",
    "        xy = layers.Conv2D(2*d1, (k4,k4), activation='relu', padding='same')(xy)\n",
    "        block_3_xy_output = layers.MaxPooling2D(pool_size=(2, 2))(xy)\n",
    "        return block_3_xy_output\n",
    " \n",
    "    def model_maker(self,p1=3,p2=3,p3=3,d1=32):\n",
    "        \n",
    "        inp1 = keras.Input(shape=(200, 200, self.channels))\n",
    "        inp2 = keras.Input(shape=(200, 200, self.channels))\n",
    "        inp3 = keras.Input(shape=(200, 200, self.channels))\n",
    "        \n",
    "        tower_1 = self.parallel(inp1,p1,p2,p3,p3,d1)\n",
    "        tower_2 = self.parallel(inp2,p1,p2,p3,p3,d1)\n",
    "        tower_3 = self.parallel(inp3,p1,p2,p3,p3,d1)\n",
    "        \n",
    "        if self.tower_min_max_only:\n",
    "            tower_max = layers.maximum([tower_1,tower_2,tower_3])\n",
    "            tower_min=layers.minimum([tower_max, tower_2,tower_3])\n",
    "            #tower_average=layers.average([tower_1, tower_2,tower_3])\n",
    "            #tower_mul= layers.Multiply([tower_1, tower_2,tower_3])#since multiplication is expensive\n",
    "            merged = layers.concatenate([tower_max, tower_min], axis=1)\n",
    "            model_name =''.join(['MIN_MAX_model_accidental_p1_',str(p1),'_p2_',str(p2),'_p3_',str(p3),'_d1_',str(d1),'.h5'])\n",
    "\n",
    "        else:\n",
    "            merged = layers.concatenate([tower_1, tower_2, tower_3], axis=1)\n",
    "            model_name =''.join(['model_accidental_p1_',str(p1),'_p2_',str(p2),'_p3_',str(p3),'_d1_',str(d1),'.h5'])\n",
    "            \n",
    "        merged = layers.Flatten()(merged)\n",
    "        all_co = layers.Dense(100, activation='relu')(merged)\n",
    "        all_co = layers.Dropout(0.5)(all_co)\n",
    "        all_co = layers.Dense(50, activation='relu')(all_co)\n",
    "        all_co= layers.Dropout(0.5)(all_co)\n",
    "        outputs = layers.Dense(3, activation='softmax')(all_co)\n",
    "\n",
    "        model = keras.Model(inputs=[inp1, inp2,inp3], outputs=outputs, name=model_name)\n",
    "        \n",
    "        return model,model_name\n",
    "\n",
    "'''Models with inception'''\n",
    "class model_inception_layer_1:\n",
    "    \n",
    "    def __init__(self,channels=19,tower_min_max_only=False):\n",
    "        'Initialization'\n",
    "        self.channels = channels\n",
    "        self.tower_min_max_only = tower_min_max_only\n",
    "\n",
    "    def parallel(self,inputs,d1,k2,k3,k4,k_b1,k_b2,k_b3,k_b4):\n",
    "    \n",
    "        incept_3 = layers.Conv2D(d1, (k_b1,k_b1),strides=(1,1),padding='same',activation='relu')(inputs)\n",
    "        incept_5 = layers.Conv2D(d1, (k_b2,k_b2),strides=(1,1),padding='same',activation='relu')(inputs)\n",
    "        incept_7 = layers.Conv2D(d1, (k_b3,k_b3),strides=(1,1),padding='same',activation='relu')(inputs)\n",
    "        incept_9 = layers.Conv2D(d1, (k_b4,k_b4),strides=(1,1),padding='same',activation='relu')(inputs)\n",
    "#        incept_10 = layers.Conv2D(d1, (k_b5,k_b5),strides=(1,1),padding='same',activation='relu')(inputs)\n",
    "\n",
    "        data = layers.concatenate([incept_3, incept_5, incept_7,incept_9], axis=3)\n",
    "        data = layers.MaxPooling2D(pool_size=(4, 4))(data)\n",
    "        \n",
    "        data_2 = layers.Conv2D(d1*4, (k2,k2),strides=(1,1),padding='same',activation='relu')(data)\n",
    "        data = layers.add([data_2, data])\n",
    "    \n",
    "        data = layers.MaxPooling2D(pool_size=(2, 2))(data)\n",
    "    \n",
    "        data = layers.Conv2D(64, (k3,k3), activation='relu', padding='same')(data)\n",
    "        '''to be removed'''\n",
    "        data = layers.MaxPooling2D(pool_size=(2, 2))(data)\n",
    "        data = layers.Conv2D(64, (k4,k4), activation='relu', padding='same')(data)\n",
    "        data = layers.MaxPooling2D(pool_size=(2, 2))(data)\n",
    "        return data\n",
    "\n",
    "    def model_maker(self,d1=8,k2=3,k3=3,k4=3,k_b1=3,k_b2=3,k_b3=3,k_b4=5,k_b5=5):\n",
    "         \n",
    "        inp1 = keras.Input(shape=(200, 200, self.channels))\n",
    "        inp2 = keras.Input(shape=(200, 200, self.channels))\n",
    "        inp3 = keras.Input(shape=(200, 200, self.channels))\n",
    "        \n",
    "        tower_1 = self.parallel(inp1,d1,k2,k3,k4,k_b1,k_b2,k_b3,k_b4)\n",
    "        tower_2 = self.parallel(inp2,d1,k2,k3,k4,k_b1,k_b2,k_b3,k_b4)\n",
    "        tower_3 = self.parallel(inp3,d1,k2,k3,k4,k_b1,k_b2,k_b3,k_b4)\n",
    "\n",
    "        if self.tower_min_max_only:\n",
    "            tower_max = layers.maximum([tower_1,tower_2,tower_3])\n",
    "            tower_min=layers.minimum([tower_max, tower_2,tower_3])\n",
    "            #tower_average=layers.average([tower_1, tower_2,tower_3])\n",
    "            #tower_mul= layers.Multiply([tower_1, tower_2,tower_3])#since multiplication is expensive\n",
    "            merged = layers.concatenate([tower_max, tower_min], axis=1)\n",
    "            model_name =''.join(['MIN_MAX_model_inception_layer_1_d1_',str(d1),'_k2_',str(k2),'_k3_',str(k3),'_k4_',str(k4),'_kb1_',str(k_b1),'_kb2_',str(k_b2),'_kb3_',str(k_b3),'_kb4_',str(k_b4),'.h5'])\n",
    "\n",
    "        else:\n",
    "            merged = layers.concatenate([tower_1, tower_2, tower_3], axis=1)\n",
    "            model_name =''.join(['model_inception_layer_1_d1_',str(d1),'_k2_',str(k2),'_k3_',str(k3),'_k4_',str(k4),'_kb1_',str(k_b1),'_kb2_',str(k_b2),'_kb3_',str(k_b3),'_kb4_',str(k_b4),'.h5'])\n",
    "        \n",
    "\n",
    "        merged = layers.concatenate([tower_1, tower_2, tower_3], axis=1)\n",
    "        merged = layers.Flatten()(merged)\n",
    "        all_co = layers.Dense(100, activation='relu')(merged)\n",
    "        all_co = layers.Dropout(0.5)(all_co)\n",
    "        all_co = layers.Dense(50, activation='relu')(all_co)\n",
    "        all_co= layers.Dropout(0.5)(all_co)\n",
    "        outputs = layers.Dense(3, activation='softmax')(all_co)\n",
    "\n",
    "        model = keras.Model(inputs=[inp1, inp2,inp3], outputs=outputs, name=model_name)\n",
    "        \n",
    "        return model,model_name\n",
    "\n",
    "'''Models with inception'''\n",
    "class model_inception_layer_1_normal:\n",
    "    \n",
    "    def __init__(self,channels=19,tower_min_max_only=False):\n",
    "        'Initialization'\n",
    "        self.channels = channels\n",
    "        self.tower_min_max_only = tower_min_max_only\n",
    "\n",
    "    def parallel(self,inputs,d1,k2,k3,k4,k_b1,k_b2,k_b3,k_b4):\n",
    "    \n",
    "        incept_3 = layers.Conv2D(d1, (k_b1,k_b1),strides=(1,1),padding='same',activation='relu')(inputs)\n",
    "        incept_5 = layers.Conv2D(d1, (k_b2,k_b2),strides=(1,1),padding='same',activation='relu')(inputs)\n",
    "        incept_7 = layers.Conv2D(d1, (k_b3,k_b3),strides=(1,1),padding='same',activation='relu')(inputs)\n",
    "        incept_9 = layers.Conv2D(d1, (k_b4,k_b4),strides=(1,1),padding='same',activation='relu')(inputs)\n",
    "        \n",
    "        data = layers.concatenate([incept_3, incept_5, incept_7,incept_9], axis=3)\n",
    "        data = layers.MaxPooling2D(pool_size=(4, 4))(data)\n",
    "        \n",
    "        data = layers.Conv2D(d1*4, (k2,k2),strides=(1,1),padding='same',activation='relu')(data)\n",
    "\n",
    "        data = layers.MaxPooling2D(pool_size=(2, 2))(data)\n",
    "    \n",
    "        data = layers.Conv2D(64, (k3,k3), activation='relu', padding='same')(data)\n",
    "        '''to be removed'''\n",
    "        data = layers.MaxPooling2D(pool_size=(2, 2))(data)\n",
    "        data = layers.Conv2D(64, (k4,k4), activation='relu', padding='same')(data)\n",
    "        data = layers.MaxPooling2D(pool_size=(2, 2))(data)\n",
    "        return data\n",
    "\n",
    "    def model_maker(self,d1=8,k2=3,k3=3,k4=3,k_b1=3,k_b2=3,k_b3=3,k_b4=5):\n",
    "         \n",
    "        inp1 = keras.Input(shape=(200, 200, self.channels))\n",
    "        inp2 = keras.Input(shape=(200, 200, self.channels))\n",
    "        inp3 = keras.Input(shape=(200, 200, self.channels))\n",
    "        \n",
    "        tower_1 = self.parallel(inp1,d1,k2,k3,k4,k_b1,k_b2,k_b3,k_b4)\n",
    "        tower_2 = self.parallel(inp2,d1,k2,k3,k4,k_b1,k_b2,k_b3,k_b4)\n",
    "        tower_3 = self.parallel(inp3,d1,k2,k3,k4,k_b1,k_b2,k_b3,k_b4)\n",
    "\n",
    "        if self.tower_min_max_only:\n",
    "            tower_max = layers.maximum([tower_1,tower_2,tower_3])\n",
    "            tower_min=layers.minimum([tower_max, tower_2,tower_3])\n",
    "            #tower_average=layers.average([tower_1, tower_2,tower_3])\n",
    "            #tower_mul= layers.Multiply([tower_1, tower_2,tower_3])#since multiplication is expensive\n",
    "            merged = layers.concatenate([tower_max, tower_min], axis=1)\n",
    "            model_name =''.join(['MIN_MAX_model_inception_layer_1_d1_',str(d1),'_k2_',str(k2),'_k3_',str(k3),'_k4_',str(k4),'_kb1_',str(k_b1),'_kb2_',str(k_b2),'_kb3_',str(k_b3),'_kb4_',str(k_b4),'.h5'])\n",
    "\n",
    "        else:\n",
    "            merged = layers.concatenate([tower_1, tower_2, tower_3], axis=1)\n",
    "            model_name =''.join(['model_inception_layer_1_d1_',str(d1),'_k2_',str(k2),'_k3_',str(k3),'_k4_',str(k4),'_kb1_',str(k_b1),'_kb2_',str(k_b2),'_kb3_',str(k_b3),'_kb4_',str(k_b4),'.h5'])\n",
    "        \n",
    "\n",
    "        merged = layers.concatenate([tower_1, tower_2, tower_3], axis=1)\n",
    "        merged = layers.Flatten()(merged)\n",
    "        all_co = layers.Dense(100, activation='relu')(merged)\n",
    "        all_co = layers.Dropout(0.5)(all_co)\n",
    "        all_co = layers.Dense(50, activation='relu')(all_co)\n",
    "        all_co= layers.Dropout(0.5)(all_co)\n",
    "        outputs = layers.Dense(3, activation='softmax')(all_co)\n",
    "\n",
    "        model = keras.Model(inputs=[inp1, inp2,inp3], outputs=outputs, name=model_name)\n",
    "        \n",
    "        return model,model_name\n",
    "'''Models with inception_alias'''\n",
    "class model_inception_layer_1_alias_1:\n",
    "    \n",
    "    def __init__(self,channels=19,tower_min_max_only=False):\n",
    "        'Initialization'\n",
    "        self.channels = channels\n",
    "        self.tower_min_max_only = tower_min_max_only\n",
    "\n",
    "    def parallel(self,inputs,d1,k2,k3,k4,k_b1,k_b2,k_b3,k_b4,k_b5):\n",
    "    \n",
    "        incept_3 = layers.Conv2D(d1, (k_b1,k_b1),strides=(1,1),padding='same',activation='relu')(inputs)\n",
    "        incept_5 = layers.Conv2D(d1, (k_b2,k_b2),strides=(1,1),padding='same',activation='relu')(inputs)\n",
    "        incept_7 = layers.Conv2D(d1, (k_b3,k_b3),strides=(1,1),padding='same',activation='relu')(inputs)\n",
    "        incept_9 = layers.Conv2D(d1, (k_b4,k_b4),strides=(1,1),padding='same',activation='relu')(inputs)\n",
    "        incept_10 = layers.Conv2D(d1, (k_b5,k_b5),strides=(1,1),padding='same',activation='relu')(inputs)\n",
    "#        incept_10 = layers.Conv2D(d1, (k_b5,k_b5),strides=(1,1),padding='same',activation='relu')(inputs)\n",
    "\n",
    "        data = layers.concatenate([incept_3, incept_5, incept_7,incept_9,incept_10], axis=3)\n",
    "        data = layers.MaxPooling2D(pool_size=(4, 4))(data)\n",
    "        \n",
    "        data_2 = layers.Conv2D(d1*5, (k2,k2),strides=(1,1),padding='same',activation='relu')(data)\n",
    "        data = layers.add([data_2, data])\n",
    "    \n",
    "        data = layers.MaxPooling2D(pool_size=(2, 2))(data)\n",
    "    \n",
    "        data = layers.Conv2D(64, (k3,k3), activation='relu', padding='same')(data)\n",
    "        '''to be removed'''\n",
    "        data = layers.MaxPooling2D(pool_size=(2, 2))(data)\n",
    "        data = layers.Conv2D(64, (k4,k4), activation='relu', padding='same')(data)\n",
    "        data = layers.MaxPooling2D(pool_size=(2, 2))(data)\n",
    "        return data\n",
    "\n",
    "    def model_maker(self,d1=8,k2=3,k3=3,k4=3,k_b1=3,k_b2=3,k_b3=3,k_b4=5,k_b5=5):\n",
    "         \n",
    "        inp1 = keras.Input(shape=(200, 200, self.channels))\n",
    "        inp2 = keras.Input(shape=(200, 200, self.channels))\n",
    "        inp3 = keras.Input(shape=(200, 200, self.channels))\n",
    "        \n",
    "        tower_1 = self.parallel(inp1,d1,k2,k3,k4,k_b1,k_b2,k_b3,k_b4,k_b5)\n",
    "        tower_2 = self.parallel(inp2,d1,k2,k3,k4,k_b1,k_b2,k_b3,k_b4,k_b5)\n",
    "        tower_3 = self.parallel(inp3,d1,k2,k3,k4,k_b1,k_b2,k_b3,k_b4,k_b5)\n",
    "\n",
    "        if self.tower_min_max_only:\n",
    "            tower_max = layers.maximum([tower_1,tower_2,tower_3])\n",
    "            tower_min=layers.minimum([tower_max, tower_2,tower_3])\n",
    "            #tower_average=layers.average([tower_1, tower_2,tower_3])\n",
    "            #tower_mul= layers.Multiply([tower_1, tower_2,tower_3])#since multiplication is expensive\n",
    "            merged = layers.concatenate([tower_max, tower_min], axis=1)\n",
    "            model_name =''.join(['MIN_MAX_model_inception_layer_1_d1_',str(d1),'_k2_',str(k2),'_k3_',str(k3),'_k4_',str(k4),'_kb1_',str(k_b1),'_kb2_',str(k_b2),'_kb3_',str(k_b3),'_kb4_',str(k_b4),'.h5'])\n",
    "\n",
    "        else:\n",
    "            merged = layers.concatenate([tower_1, tower_2, tower_3], axis=1)\n",
    "            model_name =''.join(['model_inception_layer_1_d1_',str(d1),'_k2_',str(k2),'_k3_',str(k3),'_k4_',str(k4),'_kb1_',str(k_b1),'_kb2_',str(k_b2),'_kb3_',str(k_b3),'_kb4_',str(k_b4),'.h5'])\n",
    "        \n",
    "\n",
    "        merged = layers.concatenate([tower_1, tower_2, tower_3], axis=1)\n",
    "        merged = layers.Flatten()(merged)\n",
    "        all_co = layers.Dense(100, activation='relu')(merged)\n",
    "        all_co = layers.Dropout(0.5)(all_co)\n",
    "        all_co = layers.Dense(50, activation='relu')(all_co)\n",
    "        all_co= layers.Dropout(0.5)(all_co)\n",
    "        outputs = layers.Dense(3, activation='softmax')(all_co)\n",
    "\n",
    "        model = keras.Model(inputs=[inp1, inp2,inp3], outputs=outputs, name=model_name)\n",
    "        \n",
    "        return model,model_name\n",
    "'''Models with inception_alias'''\n",
    "class model_inception_layer_1_normal_alias_1:\n",
    "    \n",
    "    def __init__(self,channels=19,tower_min_max_only=False):\n",
    "        'Initialization'\n",
    "        self.channels = channels\n",
    "        self.tower_min_max_only = tower_min_max_only\n",
    "\n",
    "    def parallel(self,inputs,d1,k2,k3,k4,k_b1,k_b2,k_b3,k_b4,k_b5):\n",
    "    \n",
    "        incept_3 = layers.Conv2D(d1, (k_b1,k_b1),strides=(1,1),padding='same',activation='relu')(inputs)\n",
    "        incept_5 = layers.Conv2D(d1, (k_b2,k_b2),strides=(1,1),padding='same',activation='relu')(inputs)\n",
    "        incept_7 = layers.Conv2D(d1, (k_b3,k_b3),strides=(1,1),padding='same',activation='relu')(inputs)\n",
    "        incept_9 = layers.Conv2D(d1, (k_b4,k_b4),strides=(1,1),padding='same',activation='relu')(inputs)\n",
    "        incept_10 = layers.Conv2D(d1, (k_b5,k_b5),strides=(1,1),padding='same',activation='relu')(inputs)\n",
    "#        incept_10 = layers.Conv2D(d1, (k_b5,k_b5),strides=(1,1),padding='same',activation='relu')(inputs)\n",
    "\n",
    "        data = layers.concatenate([incept_3, incept_5, incept_7,incept_9,incept_10], axis=3)\n",
    "        data = layers.MaxPooling2D(pool_size=(4, 4))(data)\n",
    "        \n",
    "        data = layers.Conv2D(d1*5, (k2,k2),strides=(1,1),padding='same',activation='relu')(data)\n",
    "    \n",
    "        data = layers.MaxPooling2D(pool_size=(2, 2))(data)\n",
    "    \n",
    "        data = layers.Conv2D(64, (k3,k3), activation='relu', padding='same')(data)\n",
    "        '''to be removed'''\n",
    "        data = layers.MaxPooling2D(pool_size=(2, 2))(data)\n",
    "        data = layers.Conv2D(64, (k4,k4), activation='relu', padding='same')(data)\n",
    "        data = layers.MaxPooling2D(pool_size=(2, 2))(data)\n",
    "        return data\n",
    "\n",
    "    def model_maker(self,d1=8,k2=3,k3=3,k4=3,k_b1=3,k_b2=3,k_b3=3,k_b4=5,k_b5=5):\n",
    "         \n",
    "        inp1 = keras.Input(shape=(200, 200, self.channels))\n",
    "        inp2 = keras.Input(shape=(200, 200, self.channels))\n",
    "        inp3 = keras.Input(shape=(200, 200, self.channels))\n",
    "        \n",
    "        tower_1 = self.parallel(inp1,d1,k2,k3,k4,k_b1,k_b2,k_b3,k_b4,k_b5)\n",
    "        tower_2 = self.parallel(inp2,d1,k2,k3,k4,k_b1,k_b2,k_b3,k_b4,k_b5)\n",
    "        tower_3 = self.parallel(inp3,d1,k2,k3,k4,k_b1,k_b2,k_b3,k_b4,k_b5)\n",
    "\n",
    "        if self.tower_min_max_only:\n",
    "            tower_max = layers.maximum([tower_1,tower_2,tower_3])\n",
    "            tower_min=layers.minimum([tower_max, tower_2,tower_3])\n",
    "            #tower_average=layers.average([tower_1, tower_2,tower_3])\n",
    "            #tower_mul= layers.Multiply([tower_1, tower_2,tower_3])#since multiplication is expensive\n",
    "            merged = layers.concatenate([tower_max, tower_min], axis=1)\n",
    "            model_name =''.join(['MIN_MAX_model_inception_layer_1_d1_',str(d1),'_k2_',str(k2),'_k3_',str(k3),'_k4_',str(k4),'_kb1_',str(k_b1),'_kb2_',str(k_b2),'_kb3_',str(k_b3),'_kb4_',str(k_b4),'.h5'])\n",
    "\n",
    "        else:\n",
    "            merged = layers.concatenate([tower_1, tower_2, tower_3], axis=1)\n",
    "            model_name =''.join(['model_inception_layer_1_d1_',str(d1),'_k2_',str(k2),'_k3_',str(k3),'_k4_',str(k4),'_kb1_',str(k_b1),'_kb2_',str(k_b2),'_kb3_',str(k_b3),'_kb4_',str(k_b4),'.h5'])\n",
    "        \n",
    "\n",
    "        merged = layers.concatenate([tower_1, tower_2, tower_3], axis=1)\n",
    "        merged = layers.Flatten()(merged)\n",
    "        all_co = layers.Dense(100, activation='relu')(merged)\n",
    "        all_co = layers.Dropout(0.5)(all_co)\n",
    "        all_co = layers.Dense(50, activation='relu')(all_co)\n",
    "        all_co= layers.Dropout(0.5)(all_co)\n",
    "        outputs = layers.Dense(3, activation='softmax')(all_co)\n",
    "\n",
    "        model = keras.Model(inputs=[inp1, inp2,inp3], outputs=outputs, name=model_name)\n",
    "        \n",
    "        return model,model_name\n",
    "'''Models with inception_alias'''\n",
    "class model_inception_layer_1_normal_alias_2:\n",
    "    \n",
    "    def __init__(self,channels=19,tower_min_max_only=False):\n",
    "        'Initialization'\n",
    "        self.channels = channels\n",
    "        self.tower_min_max_only = tower_min_max_only\n",
    "\n",
    "    def parallel(self,inputs,d1,k2,k3,k4,k_b1,k_b2,k_b3,k_b4,k_b5,k_b6):\n",
    "    \n",
    "        incept_3 = layers.Conv2D(d1, (k_b1,k_b1),strides=(1,1),padding='same',activation='relu')(inputs)\n",
    "        incept_5 = layers.Conv2D(d1, (k_b2,k_b2),strides=(1,1),padding='same',activation='relu')(inputs)\n",
    "        incept_7 = layers.Conv2D(d1, (k_b3,k_b3),strides=(1,1),padding='same',activation='relu')(inputs)\n",
    "        incept_9 = layers.Conv2D(d1, (k_b4,k_b4),strides=(1,1),padding='same',activation='relu')(inputs)\n",
    "        incept_10 = layers.Conv2D(d1, (k_b5,k_b5),strides=(1,1),padding='same',activation='relu')(inputs)\n",
    "        incept_11 = layers.Conv2D(d1, (k_b6,k_b6),strides=(1,1),padding='same',activation='relu')(inputs)\n",
    "\n",
    "        data = layers.concatenate([incept_3, incept_5, incept_7,incept_9,incept_10,incept_11], axis=3)\n",
    "        data = layers.MaxPooling2D(pool_size=(4, 4))(data)\n",
    "        \n",
    "        data = layers.Conv2D(d1*6, (k2,k2),strides=(1,1),padding='same',activation='relu')(data)\n",
    "    \n",
    "        data = layers.MaxPooling2D(pool_size=(2, 2))(data)\n",
    "    \n",
    "        data = layers.Conv2D(64, (k3,k3), activation='relu', padding='same')(data)\n",
    "        '''to be removed'''\n",
    "        data = layers.MaxPooling2D(pool_size=(2, 2))(data)\n",
    "        data = layers.Conv2D(64, (k4,k4), activation='relu', padding='same')(data)\n",
    "        data = layers.MaxPooling2D(pool_size=(2, 2))(data)\n",
    "        return data\n",
    "\n",
    "    def model_maker(self,d1=8,k2=3,k3=3,k4=3,k_b1=3,k_b2=3,k_b3=3,k_b4=5,k_b5=5,k_b6=5):\n",
    "         \n",
    "        inp1 = keras.Input(shape=(200, 200, self.channels))\n",
    "        inp2 = keras.Input(shape=(200, 200, self.channels))\n",
    "        inp3 = keras.Input(shape=(200, 200, self.channels))\n",
    "        \n",
    "        tower_1 = self.parallel(inp1,d1,k2,k3,k4,k_b1,k_b2,k_b3,k_b4,k_b5,k_b6)\n",
    "        tower_2 = self.parallel(inp2,d1,k2,k3,k4,k_b1,k_b2,k_b3,k_b4,k_b5,k_b6)\n",
    "        tower_3 = self.parallel(inp3,d1,k2,k3,k4,k_b1,k_b2,k_b3,k_b4,k_b5,k_b6)\n",
    "\n",
    "        if self.tower_min_max_only:\n",
    "            tower_max = layers.maximum([tower_1,tower_2,tower_3])\n",
    "            tower_min=layers.minimum([tower_max, tower_2,tower_3])\n",
    "            #tower_average=layers.average([tower_1, tower_2,tower_3])\n",
    "            #tower_mul= layers.Multiply([tower_1, tower_2,tower_3])#since multiplication is expensive\n",
    "            merged = layers.concatenate([tower_max, tower_min], axis=1)\n",
    "            model_name =''.join(['MIN_MAX_model_inception_layer_1_d1_',str(d1),'_k2_',str(k2),'_k3_',str(k3),'_k4_',str(k4),'_kb1_',str(k_b1),'_kb2_',str(k_b2),'_kb3_',str(k_b3),'_kb4_',str(k_b4),'.h5'])\n",
    "\n",
    "        else:\n",
    "            merged = layers.concatenate([tower_1, tower_2, tower_3], axis=1)\n",
    "            model_name =''.join(['model_inception_layer_1_d1_',str(d1),'_k2_',str(k2),'_k3_',str(k3),'_k4_',str(k4),'_kb1_',str(k_b1),'_kb2_',str(k_b2),'_kb3_',str(k_b3),'_kb4_',str(k_b4),'.h5'])\n",
    "        \n",
    "\n",
    "        merged = layers.concatenate([tower_1, tower_2, tower_3], axis=1)\n",
    "        merged = layers.Flatten()(merged)\n",
    "        all_co = layers.Dense(100, activation='relu')(merged)\n",
    "        all_co = layers.Dropout(0.5)(all_co)\n",
    "        all_co = layers.Dense(50, activation='relu')(all_co)\n",
    "        all_co= layers.Dropout(0.5)(all_co)\n",
    "        outputs = layers.Dense(3, activation='softmax')(all_co)\n",
    "\n",
    "        model = keras.Model(inputs=[inp1, inp2,inp3], outputs=outputs, name=model_name)\n",
    "        \n",
    "        return model,model_name\n",
    "'''model-1'''\n",
    "class model_inception_vin_1:\n",
    "    \n",
    "    def __init__(self,channels=17):\n",
    "        'Initialization'\n",
    "        self.channels = channels\n",
    "        \n",
    "\n",
    "    def parallel(self,inputs,d1,k2,k3,k4):\n",
    "    \n",
    "        incept_3 = layers.Conv2D(d1, (3,3),strides=(1,1),padding='same',activation='relu')(inputs)\n",
    "        incept_5 = layers.Conv2D(d1, (5,5),strides=(1,1),padding='same',activation='relu')(inputs)\n",
    "        incept_7 = layers.Conv2D(d1, (7,7),strides=(1,1),padding='same',activation='relu')(inputs)\n",
    "        incept_9 = layers.Conv2D(d1, (9,9),strides=(1,1),padding='same',activation='relu')(inputs)\n",
    "        data = layers.concatenate([incept_3, incept_5, incept_7,incept_9], axis=3)\n",
    "        data = layers.MaxPooling2D(pool_size=(4, 4))(data)\n",
    "        \n",
    "        data_2 = layers.Conv2D(d1*4, (k2,k2),strides=(1,1),padding='same',activation='relu')(data)\n",
    "        data = layers.add([data_2, data])\n",
    "    \n",
    "        data = layers.MaxPooling2D(pool_size=(2, 2))(data)\n",
    "    \n",
    "        data = layers.Conv2D(64, (k3,k3), activation='relu', padding='same')(data)\n",
    "        '''to be removed'''\n",
    "        data = layers.MaxPooling2D(pool_size=(2, 2))(data)\n",
    "        data = layers.Conv2D(64, (k4,k4), activation='relu', padding='same')(data)\n",
    "        data = layers.MaxPooling2D(pool_size=(2, 2))(data)\n",
    "        return data\n",
    "\n",
    "    def model_maker(self,d1=8,k2=3,k3=3,k4=3):\n",
    "         \n",
    "        inp1 = keras.Input(shape=(200, 200, self.channels))\n",
    "        inp2 = keras.Input(shape=(200, 200, self.channels))\n",
    "        inp3 = keras.Input(shape=(200, 200, self.channels))\n",
    "        \n",
    "        tower_1 = self.parallel(inp1,d1,k2,k3,k4)\n",
    "        tower_2 = self.parallel(inp2,d1,k2,k3,k4)\n",
    "        tower_3 = self.parallel(inp3,d1,k2,k3,k4)\n",
    "\n",
    "        merged = layers.concatenate([tower_1, tower_2, tower_3], axis=1)\n",
    "        merged = layers.Flatten()(merged)\n",
    "        all_co = layers.Dense(100, activation='relu')(merged)\n",
    "        all_co = layers.Dropout(0.5)(all_co)\n",
    "        all_co = layers.Dense(50, activation='relu')(all_co)\n",
    "        all_co= layers.Dropout(0.5)(all_co)\n",
    "        outputs = layers.Dense(3, activation='softmax')(all_co)\n",
    "\n",
    "        model_name = 'model_inception_vin_1.h5'\n",
    "        model = keras.Model(inputs=[inp1, inp2,inp3], outputs=outputs, name='model_inception_vin_1')\n",
    "        \n",
    "        return model,model_name\n",
    "\n",
    "\n",
    "'''model-2'''\n",
    "class model_inception_all__depths__inception_vin_1:\n",
    "    \n",
    "    def __init__(self,channels=17):\n",
    "        'Initialization'\n",
    "        self.channels = channels\n",
    "        \n",
    "    def parallel(self,inputs,d1,d2,d3,d4):\n",
    "    \n",
    "        incept_3 = layers.Conv2D(d1, (3,3),strides=(1,1),padding='same',activation='relu')(inputs)\n",
    "        incept_5 = layers.Conv2D(d1, (5,5),strides=(1,1),padding='same',activation='relu')(inputs)\n",
    "        incept_7 = layers.Conv2D(d1, (7,7),strides=(1,1),padding='same',activation='relu')(inputs)\n",
    "        incept_9 = layers.Conv2D(d1, (9,9),strides=(1,1),padding='same',activation='relu')(inputs)\n",
    "        \n",
    "        incept_3_pool = layers.MaxPooling2D(pool_size=(4, 4))(incept_3)\n",
    "        incept_5_pool = layers.MaxPooling2D(pool_size=(4, 4))(incept_5)\n",
    "        incept_7_pool = layers.MaxPooling2D(pool_size=(4, 4))(incept_7)\n",
    "        incept_9_pool = layers.MaxPooling2D(pool_size=(4, 4))(incept_9)\n",
    "        \n",
    "        incept_3 = layers.Conv2D(d2, (3,3),strides=(1,1),padding='same',activation='relu')(incept_3_pool)\n",
    "        incept_5 = layers.Conv2D(d2, (5,5),strides=(1,1),padding='same',activation='relu')(incept_5_pool)\n",
    "        incept_7 = layers.Conv2D(d2, (7,7),strides=(1,1),padding='same',activation='relu')(incept_7_pool)\n",
    "        incept_9 = layers.Conv2D(d2, (9,9),strides=(1,1),padding='same',activation='relu')(incept_9_pool)\n",
    "        \n",
    "        incept_3_pool = layers.MaxPooling2D(pool_size=(2, 2))(incept_3)\n",
    "        incept_5_pool = layers.MaxPooling2D(pool_size=(2, 2))(incept_5)\n",
    "        incept_7_pool = layers.MaxPooling2D(pool_size=(2, 2))(incept_7)\n",
    "        incept_9_pool = layers.MaxPooling2D(pool_size=(2, 2))(incept_9)\n",
    "        \n",
    "        incept_3 = layers.Conv2D(d3, (3,3),strides=(1,1),padding='same',activation='relu')(incept_3_pool)\n",
    "        incept_5 = layers.Conv2D(d3, (5,5),strides=(1,1),padding='same',activation='relu')(incept_5_pool)\n",
    "        incept_7 = layers.Conv2D(d3, (7,7),strides=(1,1),padding='same',activation='relu')(incept_7_pool)\n",
    "        incept_9 = layers.Conv2D(d3, (9,9),strides=(1,1),padding='same',activation='relu')(incept_9_pool)\n",
    "        \n",
    "        incept_3_pool = layers.MaxPooling2D(pool_size=(2, 2))(incept_3)\n",
    "        incept_5_pool = layers.MaxPooling2D(pool_size=(2, 2))(incept_5)\n",
    "        incept_7_pool = layers.MaxPooling2D(pool_size=(2, 2))(incept_7)\n",
    "        incept_9_pool = layers.MaxPooling2D(pool_size=(2, 2))(incept_9)\n",
    "        \n",
    "        incept_3 = layers.Conv2D(d4, (3,3),strides=(1,1),padding='same',activation='relu')(incept_3_pool)\n",
    "        incept_5 = layers.Conv2D(d4, (5,5),strides=(1,1),padding='same',activation='relu')(incept_5_pool)\n",
    "        incept_7 = layers.Conv2D(d4, (7,7),strides=(1,1),padding='same',activation='relu')(incept_7_pool)\n",
    "        incept_9 = layers.Conv2D(d4, (9,9),strides=(1,1),padding='same',activation='relu')(incept_9_pool)\n",
    "        \n",
    "        incept_3_pool = layers.MaxPooling2D(pool_size=(2, 2))(incept_3)\n",
    "        incept_5_pool = layers.MaxPooling2D(pool_size=(2, 2))(incept_5)\n",
    "        incept_7_pool = layers.MaxPooling2D(pool_size=(2, 2))(incept_7)\n",
    "        incept_9_pool = layers.MaxPooling2D(pool_size=(2, 2))(incept_9)\n",
    "         \n",
    "        all_output = layers.concatenate([incept_3_pool, incept_5_pool, incept_7_pool,incept_9_pool], axis=3)\n",
    "    \n",
    "        return all_output\n",
    "\n",
    "    def model_maker(self,d1=8,d2=8,d3=16,d4=16):\n",
    "         \n",
    "        inp1 = keras.Input(shape=(200, 200, self.channels))\n",
    "        inp2 = keras.Input(shape=(200, 200, self.channels))\n",
    "        inp3 = keras.Input(shape=(200, 200, self.channels))\n",
    "        \n",
    "        tower_1 = self.parallel(inp1,d1,d2,d3,d4)\n",
    "        tower_2 = self.parallel(inp2,d1,d2,d3,d4)\n",
    "        tower_3 = self.parallel(inp3,d1,d2,d3,d4)\n",
    "        \n",
    "        merged = layers.concatenate([tower_1, tower_2, tower_3], axis=1)\n",
    "        merged = layers.Flatten()(merged)\n",
    "        all_co = layers.Dense(100, activation='relu')(merged)\n",
    "        all_co = layers.Dropout(0.5)(all_co)\n",
    "        all_co = layers.Dense(50, activation='relu')(all_co)\n",
    "        all_co= layers.Dropout(0.5)(all_co)\n",
    "        outputs = layers.Dense(3, activation='softmax')(all_co)\n",
    "    \n",
    "        model_name = 'model_inception_all__depths__inception_vin_1.h5'\n",
    "        model = keras.Model(inputs=[inp1, inp2,inp3], outputs=outputs, name='model_inception_all__depths__inception_vin_1')\n",
    "        return model,model_name\n",
    "    \n",
    "'''model type 3'''\n",
    "class model_inception_all__depths__inception_complic_vin_1:\n",
    "    \n",
    "    def __init__(self,channels=17):\n",
    "        'Initialization'\n",
    "        self.channels = channels\n",
    "        \n",
    "    def parallel(self,inputs,d1,d3):\n",
    "    \n",
    "        incept_3 = layers.Conv2D(d1, (3,3),strides=(1,1),padding='same',activation='relu')(inputs)\n",
    "        incept_5 = layers.Conv2D(d1, (5,5),strides=(1,1),padding='same',activation='relu')(inputs)\n",
    "        incept_7 = layers.Conv2D(d1, (7,7),strides=(1,1),padding='same',activation='relu')(inputs)\n",
    "        incept_9 = layers.Conv2D(d1, (9,9),strides=(1,1),padding='same',activation='relu')(inputs)\n",
    "        \n",
    "        incept_3_pool = layers.MaxPooling2D(pool_size=(4, 4))(incept_3)\n",
    "        incept_5_pool = layers.MaxPooling2D(pool_size=(4, 4))(incept_5)\n",
    "        incept_7_pool = layers.MaxPooling2D(pool_size=(4, 4))(incept_7)\n",
    "        incept_9_pool = layers.MaxPooling2D(pool_size=(4, 4))(incept_9)\n",
    "        \n",
    "        all_output = layers.concatenate([incept_3_pool, incept_5_pool, incept_7_pool,incept_9_pool], axis=3)\n",
    "    \n",
    "        incept_3 = layers.Conv2D(d1, (3,3),strides=(1,1),padding='same',activation='relu')(all_output)\n",
    "        incept_5 = layers.Conv2D(d1, (5,5),strides=(1,1),padding='same',activation='relu')(all_output)\n",
    "        incept_7 = layers.Conv2D(d1, (7,7),strides=(1,1),padding='same',activation='relu')(all_output)\n",
    "        incept_9 = layers.Conv2D(d1, (9,9),strides=(1,1),padding='same',activation='relu')(all_output)\n",
    "          \n",
    "        all_output_2 = layers.concatenate([incept_3, incept_5, incept_7,incept_9], axis=3)\n",
    "        all_output=  layers.add([all_output,all_output_2])\n",
    "        all_output = layers.MaxPooling2D(pool_size=(2,2))(all_output)\n",
    "     \n",
    "        incept_3 = layers.Conv2D(d3, (3,3),strides=(1,1),padding='same',activation='relu')(all_output)\n",
    "        incept_5 = layers.Conv2D(d3, (5,5),strides=(1,1),padding='same',activation='relu')(all_output)\n",
    "        incept_7 = layers.Conv2D(d3, (7,7),strides=(1,1),padding='same',activation='relu')(all_output)\n",
    "        incept_9 = layers.Conv2D(d3, (9,9),strides=(1,1),padding='same',activation='relu')(all_output)\n",
    "        \n",
    "        incept_3_pool = layers.MaxPooling2D(pool_size=(2, 2))(incept_3)\n",
    "        incept_5_pool = layers.MaxPooling2D(pool_size=(2, 2))(incept_5)\n",
    "        incept_7_pool = layers.MaxPooling2D(pool_size=(2, 2))(incept_7)\n",
    "        incept_9_pool = layers.MaxPooling2D(pool_size=(2, 2))(incept_9)\n",
    "        \n",
    "        all_output = layers.concatenate([incept_3_pool, incept_5_pool, incept_7_pool,incept_9_pool], axis=3)\n",
    "        \n",
    "        incept_3 = layers.Conv2D(d3, (3,3),strides=(1,1),padding='same',activation='relu')(all_output)\n",
    "        incept_5 = layers.Conv2D(d3, (5,5),strides=(1,1),padding='same',activation='relu')(all_output)\n",
    "        incept_7 = layers.Conv2D(d3, (7,7),strides=(1,1),padding='same',activation='relu')(all_output)\n",
    "        incept_9 = layers.Conv2D(d3, (9,9),strides=(1,1),padding='same',activation='relu')(all_output)\n",
    "        \n",
    "        all_output_4 = layers.concatenate([incept_3, incept_5, incept_7,incept_9], axis=3)\n",
    "        all_output=  layers.add([all_output,all_output_4])\n",
    "        all_output = layers.MaxPooling2D(pool_size=(2,2))(all_output)\n",
    "    \n",
    "        return all_output\n",
    "\n",
    "    def model_maker(self,d1=8,d3=16):\n",
    "         \n",
    "        inp1 = keras.Input(shape=(200, 200, self.channels))\n",
    "        inp2 = keras.Input(shape=(200, 200, self.channels))\n",
    "        inp3 = keras.Input(shape=(200, 200, self.channels))\n",
    "        \n",
    "        tower_1 = self.parallel(inp1,d1,d3)\n",
    "        tower_2 = self.parallel(inp2,d1,d3)\n",
    "        tower_3 = self.parallel(inp3,d1,d3)\n",
    "        \n",
    "        merged = layers.concatenate([tower_1, tower_2, tower_3], axis=1)\n",
    "        merged = layers.Flatten()(merged)\n",
    "        all_co = layers.Dense(100, activation='relu')(merged)\n",
    "        all_co = layers.Dropout(0.5)(all_co)\n",
    "        all_co = layers.Dense(50, activation='relu')(all_co)\n",
    "        all_co= layers.Dropout(0.5)(all_co)\n",
    "        outputs = layers.Dense(3, activation='softmax')(all_co)\n",
    "        model_name = 'model_inception_all__depths__inception_complic_vin_1.h5'\n",
    "        model = keras.Model(inputs=[inp1, inp2,inp3], outputs=outputs, name='model_inception_all__depths__inception_complic_vin_1')\n",
    "        return model,model_name\n",
    "\n",
    "'''Models have parallel towers'''\n",
    "\n",
    "'''Models with inception'''\n",
    "'''model parallel tower-1'''\n",
    "class model_par_inception_vin_1:\n",
    "    \n",
    "    def __init__(self,channels=17,tower_min_max_only=False):\n",
    "        'Initialization'\n",
    "        self.channels = channels\n",
    "        self.tower_min_max_only = tower_min_max_only\n",
    "        \n",
    "\n",
    "    def model_maker(self,d1=8,k2=3,k3=3,k4=3):\n",
    "        \n",
    "        inputs = keras.Input(shape=(200, 200, self.channels))\n",
    "\n",
    "        inp1 = keras.Input(shape=(200, 200, self.channels))\n",
    "        inp2 = keras.Input(shape=(200, 200, self.channels))\n",
    "        inp3 = keras.Input(shape=(200, 200, self.channels))\n",
    "        \n",
    "        incept_3 = layers.Conv2D(d1, (3,3),strides=(1,1),padding='same',activation='relu')(inputs)\n",
    "        incept_5 = layers.Conv2D(d1, (5,5),strides=(1,1),padding='same',activation='relu')(inputs)\n",
    "        incept_7 = layers.Conv2D(d1, (7,7),strides=(1,1),padding='same',activation='relu')(inputs)\n",
    "        incept_9 = layers.Conv2D(d1, (9,9),strides=(1,1),padding='same',activation='relu')(inputs)\n",
    "        data = layers.concatenate([incept_3, incept_5, incept_7,incept_9], axis=3)\n",
    "        data = layers.MaxPooling2D(pool_size=(4, 4))(data)\n",
    "        \n",
    "        data_2 = layers.Conv2D(d1*4, (k2,k2),strides=(1,1),padding='same',activation='relu')(data)\n",
    "        data = layers.add([data_2, data])\n",
    "    \n",
    "        data = layers.MaxPooling2D(pool_size=(2, 2))(data)\n",
    "    \n",
    "        data = layers.Conv2D(64, (k3,k3), activation='relu', padding='same')(data)\n",
    "        '''to be removed'''\n",
    "        data = layers.MaxPooling2D(pool_size=(2, 2))(data)\n",
    "        data = layers.Conv2D(64, (k4,k4), activation='relu', padding='same')(data)\n",
    "        data = layers.MaxPooling2D(pool_size=(2, 2))(data)\n",
    "        \n",
    "        parallel = keras.models.Model(inputs, data, name='parallel')     \n",
    "\n",
    "        tower_1 = parallel(inp1)\n",
    "        tower_2 = parallel(inp2)\n",
    "        tower_3 = parallel(inp3)\n",
    "       \n",
    "        if self.tower_min_max_only:\n",
    "            tower_max = layers.maximum([tower_1,tower_2,tower_3])\n",
    "            tower_min=layers.minimum([tower_max, tower_2,tower_3])\n",
    "            #tower_average=layers.average([tower_1, tower_2,tower_3])\n",
    "            #tower_mul= layers.Multiply([tower_1, tower_2,tower_3])#since multiplication is expensive\n",
    "            merged = layers.concatenate([tower_max, tower_min], axis=1)\n",
    "            model_name = ''.join(['MIN_MAX_model_inception_vin_1_d1',str(d1),'_k2_',str(k2),'_k3_',str(k3),'_k4_',str(k4),'.h5'])\n",
    "\n",
    "        else:\n",
    "            merged = layers.concatenate([tower_1, tower_2, tower_3], axis=1)\n",
    "            model_name = ''.join(['model_inception_vin_1_d1',str(d1),'_k2_',str(k2),'_k3_',str(k3),'_k4_',str(k4),'.h5'])\n",
    "\n",
    "        merged = layers.Flatten()(merged)\n",
    "        all_co = layers.Dense(100, activation='relu')(merged)\n",
    "        all_co = layers.Dropout(0.5)(all_co)\n",
    "        all_co = layers.Dense(50, activation='relu')(all_co)\n",
    "        all_co= layers.Dropout(0.5)(all_co)\n",
    "        outputs = layers.Dense(3, activation='softmax')(all_co)\n",
    "\n",
    "        model = keras.models.Model(inputs=[inp1, inp2,inp3], outputs=outputs, name=model_name)\n",
    "        \n",
    "        return model,model_name\n",
    "'''model parallel tower-2'''\n",
    "class model_par_inception_w_o_addition_vin_1:\n",
    "    \n",
    "    def __init__(self,channels=17,tower_min_max_only=False):\n",
    "        'Initialization'\n",
    "        self.channels = channels\n",
    "        self.tower_min_max_only = tower_min_max_only\n",
    "        \n",
    "\n",
    "    def model_maker(self,d1=8,k2=3,k3=3,k4=3):\n",
    "        \n",
    "        inputs = keras.Input(shape=(200, 200, self.channels))\n",
    "\n",
    "        inp1 = keras.Input(shape=(200, 200, self.channels))\n",
    "        inp2 = keras.Input(shape=(200, 200, self.channels))\n",
    "        inp3 = keras.Input(shape=(200, 200, self.channels))\n",
    "        \n",
    "        incept_3 = layers.Conv2D(d1, (3,3),strides=(1,1),padding='same',activation='relu')(inputs)\n",
    "        incept_5 = layers.Conv2D(d1, (5,5),strides=(1,1),padding='same',activation='relu')(inputs)\n",
    "        incept_7 = layers.Conv2D(d1, (7,7),strides=(1,1),padding='same',activation='relu')(inputs)\n",
    "        incept_9 = layers.Conv2D(d1, (9,9),strides=(1,1),padding='same',activation='relu')(inputs)\n",
    "        data = layers.concatenate([incept_3, incept_5, incept_7,incept_9], axis=3)\n",
    "        data = layers.MaxPooling2D(pool_size=(4, 4))(data)\n",
    "        \n",
    "        data = layers.Conv2D(d1*4, (k2,k2),strides=(1,1),padding='same',activation='relu')(data)   \n",
    "        data = layers.MaxPooling2D(pool_size=(2, 2))(data)\n",
    "    \n",
    "        data = layers.Conv2D(64, (k3,k3), activation='relu', padding='same')(data)\n",
    "        '''to be removed'''\n",
    "        data = layers.MaxPooling2D(pool_size=(2, 2))(data)\n",
    "        data = layers.Conv2D(64, (k4,k4), activation='relu', padding='same')(data)\n",
    "        data = layers.MaxPooling2D(pool_size=(2, 2))(data)\n",
    "        \n",
    "        parallel = keras.models.Model(inputs, data, name='parallel')     \n",
    "\n",
    "        tower_1 = parallel(inp1)\n",
    "        tower_2 = parallel(inp2)\n",
    "        tower_3 = parallel(inp3)\n",
    "        \n",
    "        if self.tower_min_max_only:\n",
    "            tower_max = layers.maximum([tower_1,tower_2,tower_3])\n",
    "            tower_min=layers.minimum([tower_max, tower_2,tower_3])\n",
    "            #tower_average=layers.average([tower_1, tower_2,tower_3])\n",
    "            #tower_mul= layers.Multiply([tower_1, tower_2,tower_3])#since multiplication is expensive\n",
    "            merged = layers.concatenate([tower_max, tower_min], axis=1)\n",
    "            model_name = ''.join(['MIN_MAX_model_inception_w_o_addition_vin_1_d1',str(d1),'_k2_',str(k2),'_k3_',str(k3),'_k4_',str(k4),'.h5'])\n",
    "\n",
    "        else:\n",
    "            merged = layers.concatenate([tower_1, tower_2, tower_3], axis=1)\n",
    "            model_name = ''.join(['model_inception_w_o_addition_vin_1_d1',str(d1),'_k2_',str(k2),'_k3_',str(k3),'_k4_',str(k4),'.h5'])\n",
    "\n",
    "        merged = layers.Flatten()(merged)\n",
    "        all_co = layers.Dense(100, activation='relu')(merged)\n",
    "        all_co = layers.Dropout(0.5)(all_co)\n",
    "        all_co = layers.Dense(50, activation='relu')(all_co)\n",
    "        all_co= layers.Dropout(0.5)(all_co)\n",
    "        outputs = layers.Dense(3, activation='softmax')(all_co)\n",
    "\n",
    "        model = keras.models.Model(inputs=[inp1, inp2,inp3], outputs=outputs, name=model_name)\n",
    "        \n",
    "        return model,model_name\n",
    "\n",
    "'''model parallel tower -3'''\n",
    "class model_par_inception_all__depths__inception_vin_1:\n",
    "    \n",
    "    def __init__(self,channels=17,tower_min_max_only=False):\n",
    "        'Initialization'\n",
    "        self.channels = channels\n",
    "        self.tower_min_max_only = tower_min_max_only\n",
    "        \n",
    "    def model_maker(self,d1=4,d2=4,d3=8,d4=8):\n",
    "        inputs = keras.Input(shape=(200, 200, self.channels))\n",
    "\n",
    "        inp1 = keras.Input(shape=(200, 200, self.channels))\n",
    "        inp2 = keras.Input(shape=(200, 200, self.channels))\n",
    "        inp3 = keras.Input(shape=(200, 200, self.channels))    \n",
    "       \n",
    "        incept_3 = layers.Conv2D(d1, (3,3),strides=(1,1),padding='same',activation='relu')(inputs)\n",
    "        incept_5 = layers.Conv2D(d1, (5,5),strides=(1,1),padding='same',activation='relu')(inputs)\n",
    "        incept_7 = layers.Conv2D(d1, (7,7),strides=(1,1),padding='same',activation='relu')(inputs)\n",
    "        incept_9 = layers.Conv2D(d1, (9,9),strides=(1,1),padding='same',activation='relu')(inputs)\n",
    "        \n",
    "        incept_3_pool = layers.MaxPooling2D(pool_size=(4, 4))(incept_3)\n",
    "        incept_5_pool = layers.MaxPooling2D(pool_size=(4, 4))(incept_5)\n",
    "        incept_7_pool = layers.MaxPooling2D(pool_size=(4, 4))(incept_7)\n",
    "        incept_9_pool = layers.MaxPooling2D(pool_size=(4, 4))(incept_9)\n",
    "        \n",
    "        incept_3 = layers.Conv2D(d2, (3,3),strides=(1,1),padding='same',activation='relu')(incept_3_pool)\n",
    "        incept_5 = layers.Conv2D(d2, (5,5),strides=(1,1),padding='same',activation='relu')(incept_5_pool)\n",
    "        incept_7 = layers.Conv2D(d2, (7,7),strides=(1,1),padding='same',activation='relu')(incept_7_pool)\n",
    "        incept_9 = layers.Conv2D(d2, (9,9),strides=(1,1),padding='same',activation='relu')(incept_9_pool)\n",
    "        \n",
    "        incept_3_pool = layers.MaxPooling2D(pool_size=(2, 2))(incept_3)\n",
    "        incept_5_pool = layers.MaxPooling2D(pool_size=(2, 2))(incept_5)\n",
    "        incept_7_pool = layers.MaxPooling2D(pool_size=(2, 2))(incept_7)\n",
    "        incept_9_pool = layers.MaxPooling2D(pool_size=(2, 2))(incept_9)\n",
    "        \n",
    "        incept_3 = layers.Conv2D(d3, (3,3),strides=(1,1),padding='same',activation='relu')(incept_3_pool)\n",
    "        incept_5 = layers.Conv2D(d3, (5,5),strides=(1,1),padding='same',activation='relu')(incept_5_pool)\n",
    "        incept_7 = layers.Conv2D(d3, (7,7),strides=(1,1),padding='same',activation='relu')(incept_7_pool)\n",
    "        incept_9 = layers.Conv2D(d3, (9,9),strides=(1,1),padding='same',activation='relu')(incept_9_pool)\n",
    "        \n",
    "        incept_3_pool = layers.MaxPooling2D(pool_size=(2, 2))(incept_3)\n",
    "        incept_5_pool = layers.MaxPooling2D(pool_size=(2, 2))(incept_5)\n",
    "        incept_7_pool = layers.MaxPooling2D(pool_size=(2, 2))(incept_7)\n",
    "        incept_9_pool = layers.MaxPooling2D(pool_size=(2, 2))(incept_9)\n",
    "        \n",
    "        incept_3 = layers.Conv2D(d4, (3,3),strides=(1,1),padding='same',activation='relu')(incept_3_pool)\n",
    "        incept_5 = layers.Conv2D(d4, (5,5),strides=(1,1),padding='same',activation='relu')(incept_5_pool)\n",
    "        incept_7 = layers.Conv2D(d4, (7,7),strides=(1,1),padding='same',activation='relu')(incept_7_pool)\n",
    "        incept_9 = layers.Conv2D(d4, (9,9),strides=(1,1),padding='same',activation='relu')(incept_9_pool)\n",
    "        \n",
    "        incept_3_pool = layers.MaxPooling2D(pool_size=(2, 2))(incept_3)\n",
    "        incept_5_pool = layers.MaxPooling2D(pool_size=(2, 2))(incept_5)\n",
    "        incept_7_pool = layers.MaxPooling2D(pool_size=(2, 2))(incept_7)\n",
    "        incept_9_pool = layers.MaxPooling2D(pool_size=(2, 2))(incept_9)\n",
    "         \n",
    "        data = layers.concatenate([incept_3_pool, incept_5_pool, incept_7_pool,incept_9_pool], axis=3)\n",
    "\n",
    "    \n",
    "        parallel = keras.models.Model(inputs, data, name='parallel')     \n",
    "\n",
    "        tower_1 = parallel(inp1)\n",
    "        tower_2 = parallel(inp2)\n",
    "        tower_3 = parallel(inp3)\n",
    "        \n",
    "        if self.tower_min_max_only:\n",
    "            tower_max = layers.maximum([tower_1,tower_2,tower_3])\n",
    "            tower_min=layers.minimum([tower_max, tower_2,tower_3])\n",
    "            #tower_average=layers.average([tower_1, tower_2,tower_3])\n",
    "            #tower_mul= layers.Multiply([tower_1, tower_2,tower_3])#since multiplication is expensive\n",
    "            merged = layers.concatenate([tower_max, tower_min], axis=1)\n",
    "            model_name = ''.join(['MIN_MAX_model_inception_all__depths__inception_vin_1_d1',str(d1),'_d2_',str(d2),'_d3_',str(d3),'_d4_',str(d4),'.h5'])\n",
    "\n",
    "        else:\n",
    "            merged = layers.concatenate([tower_1, tower_2, tower_3], axis=1)\n",
    "            model_name = ''.join(['model_inception_all__depths__inception_vin_1_d1',str(d1),'_d2_',str(d2),'_d3_',str(d3),'_d4_',str(d4),'.h5'])\n",
    "        \n",
    "        merged = layers.Flatten()(merged)\n",
    "        all_co = layers.Dense(100, activation='relu')(merged)\n",
    "        all_co = layers.Dropout(0.5)(all_co)\n",
    "        all_co = layers.Dense(50, activation='relu')(all_co)\n",
    "        all_co= layers.Dropout(0.5)(all_co)\n",
    "        outputs = layers.Dense(3, activation='softmax')(all_co)\n",
    "\n",
    "        model = keras.models.Model(inputs=[inp1, inp2,inp3], outputs=outputs, name=model_name)\n",
    "    \n",
    "        return model,model_name\n",
    "\n",
    "\n",
    "'''model parallel tower -4'''\n",
    "class model_par_parallel_inception_all__depths_min_max_inception_vin_1:\n",
    "    \n",
    "    def __init__(self,channels=17,tower_min_max_only=False):\n",
    "        'Initialization'\n",
    "        self.channels = channels\n",
    "        self.tower_min_max_only = tower_min_max_only\n",
    "        \n",
    "    def model_maker(self,d1=8,d2=8,d3=16,d4=16):\n",
    "        inputs = keras.Input(shape=(200, 200, self.channels))\n",
    "\n",
    "        inp1 = keras.Input(shape=(200, 200, self.channels))\n",
    "        inp2 = keras.Input(shape=(200, 200, self.channels))\n",
    "        inp3 = keras.Input(shape=(200, 200, self.channels))    \n",
    "       \n",
    "        incept_3 = layers.Conv2D(d1, (3,3),strides=(1,1),padding='same',activation='relu')(inputs)\n",
    "        incept_5 = layers.Conv2D(d1, (5,5),strides=(1,1),padding='same',activation='relu')(inputs)\n",
    "        incept_7 = layers.Conv2D(d1, (7,7),strides=(1,1),padding='same',activation='relu')(inputs)\n",
    "        incept_9 = layers.Conv2D(d1, (9,9),strides=(1,1),padding='same',activation='relu')(inputs)\n",
    "        \n",
    "        incept_3_pool = layers.MaxPooling2D(pool_size=(4, 4))(incept_3)\n",
    "        incept_5_pool = layers.MaxPooling2D(pool_size=(4, 4))(incept_5)\n",
    "        incept_7_pool = layers.MaxPooling2D(pool_size=(4, 4))(incept_7)\n",
    "        incept_9_pool = layers.MaxPooling2D(pool_size=(4, 4))(incept_9)\n",
    "        \n",
    "        all_output_max = layers.maximum([incept_3_pool, incept_5_pool, incept_7_pool,incept_9_pool])\n",
    "        all_output_min =layers.minimum([incept_3_pool, incept_5_pool, incept_7_pool,incept_9_pool])\n",
    "        all_output = layers.concatenate([all_output_max, all_output_min], axis=3)\n",
    "        \n",
    "        incept_3 = layers.Conv2D(d2, (3,3),strides=(1,1),padding='same',activation='relu')(all_output)\n",
    "        incept_5 = layers.Conv2D(d2, (5,5),strides=(1,1),padding='same',activation='relu')(all_output)\n",
    "        incept_7 = layers.Conv2D(d2, (7,7),strides=(1,1),padding='same',activation='relu')(all_output)\n",
    "        incept_9 = layers.Conv2D(d2, (9,9),strides=(1,1),padding='same',activation='relu')(all_output)\n",
    "        \n",
    "        incept_3_pool = layers.MaxPooling2D(pool_size=(2, 2))(incept_3)\n",
    "        incept_5_pool = layers.MaxPooling2D(pool_size=(2, 2))(incept_5)\n",
    "        incept_7_pool = layers.MaxPooling2D(pool_size=(2, 2))(incept_7)\n",
    "        incept_9_pool = layers.MaxPooling2D(pool_size=(2, 2))(incept_9)\n",
    "        \n",
    "        all_output_max = layers.maximum([incept_3_pool, incept_5_pool, incept_7_pool,incept_9_pool])\n",
    "        all_output_min =layers.minimum([incept_3_pool, incept_5_pool, incept_7_pool,incept_9_pool])\n",
    "        all_output = layers.concatenate([all_output_max, all_output_min], axis=3)\n",
    "        \n",
    "        incept_3 = layers.Conv2D(d3, (3,3),strides=(1,1),padding='same',activation='relu')(all_output)\n",
    "        incept_5 = layers.Conv2D(d3, (5,5),strides=(1,1),padding='same',activation='relu')(all_output)\n",
    "        incept_7 = layers.Conv2D(d3, (7,7),strides=(1,1),padding='same',activation='relu')(all_output)\n",
    "        incept_9 = layers.Conv2D(d3, (9,9),strides=(1,1),padding='same',activation='relu')(all_output)\n",
    "        \n",
    "        incept_3_pool = layers.MaxPooling2D(pool_size=(2, 2))(incept_3)\n",
    "        incept_5_pool = layers.MaxPooling2D(pool_size=(2, 2))(incept_5)\n",
    "        incept_7_pool = layers.MaxPooling2D(pool_size=(2, 2))(incept_7)\n",
    "        incept_9_pool = layers.MaxPooling2D(pool_size=(2, 2))(incept_9)\n",
    "        \n",
    "        all_output_max = layers.maximum([incept_3_pool, incept_5_pool, incept_7_pool,incept_9_pool])\n",
    "        all_output_min =layers.minimum([incept_3_pool, incept_5_pool, incept_7_pool,incept_9_pool])\n",
    "        all_output = layers.concatenate([all_output_max, all_output_min], axis=3)\n",
    "        \n",
    "        incept_3 = layers.Conv2D(d4, (3,3),strides=(1,1),padding='same',activation='relu')(all_output)\n",
    "        incept_5 = layers.Conv2D(d4, (5,5),strides=(1,1),padding='same',activation='relu')(all_output)\n",
    "        incept_7 = layers.Conv2D(d4, (7,7),strides=(1,1),padding='same',activation='relu')(all_output)\n",
    "        incept_9 = layers.Conv2D(d4, (9,9),strides=(1,1),padding='same',activation='relu')(all_output)\n",
    "        \n",
    "        incept_3_pool = layers.MaxPooling2D(pool_size=(2, 2))(incept_3)\n",
    "        incept_5_pool = layers.MaxPooling2D(pool_size=(2, 2))(incept_5)\n",
    "        incept_7_pool = layers.MaxPooling2D(pool_size=(2, 2))(incept_7)\n",
    "        incept_9_pool = layers.MaxPooling2D(pool_size=(2, 2))(incept_9)\n",
    "         \n",
    "        all_output_max = layers.maximum([incept_3_pool, incept_5_pool, incept_7_pool,incept_9_pool])\n",
    "        all_output_min =layers.minimum([incept_3_pool, incept_5_pool, incept_7_pool,incept_9_pool])\n",
    "        all_output = layers.concatenate([all_output_max, all_output_min], axis=3)\n",
    "    \n",
    "        parallel = keras.models.Model(inputs, all_output, name='parallel')     \n",
    "\n",
    "        tower_1 = parallel(inp1)\n",
    "        tower_2 = parallel(inp2)\n",
    "        tower_3 = parallel(inp3)\n",
    "        \n",
    "        if self.tower_min_max_only:\n",
    "            tower_max = layers.maximum([tower_1,tower_2,tower_3])\n",
    "            tower_min=layers.minimum([tower_max, tower_2,tower_3])\n",
    "            #tower_average=layers.average([tower_1, tower_2,tower_3])\n",
    "            #tower_mul= layers.Multiply([tower_1, tower_2,tower_3])#since multiplication is expensive\n",
    "            merged = layers.concatenate([tower_max, tower_min], axis=1)\n",
    "            model_name = ''.join(['MIN_MAX_model_inception_all__depths__inception_vin_1_d1',str(d1),'_d2_',str(d2),'_d3_',str(d3),'_d4_',str(d4),'.h5'])\n",
    "\n",
    "        else:\n",
    "            merged = layers.concatenate([tower_1, tower_2, tower_3], axis=1)\n",
    "            model_name = ''.join(['model_inception_all__depths__inception_vin_1_d1',str(d1),'_d2_',str(d2),'_d3_',str(d3),'_d4_',str(d4),'.h5'])\n",
    "        \n",
    "        merged = layers.Flatten()(merged)\n",
    "        all_co = layers.Dense(100, activation='relu')(merged)\n",
    "        all_co = layers.Dropout(0.5)(all_co)\n",
    "        all_co = layers.Dense(50, activation='relu')(all_co)\n",
    "        all_co= layers.Dropout(0.5)(all_co)\n",
    "        outputs = layers.Dense(3, activation='softmax')(all_co)\n",
    "\n",
    "        model = keras.models.Model(inputs=[inp1, inp2,inp3], outputs=outputs, name=model_name)\n",
    "    \n",
    "        return model,model_name\n",
    "\n",
    "    \n",
    "'''model parallel tower -5'''\n",
    "class model_par_inception_all__depths__inception_complic_vin_1:\n",
    "    \n",
    "    def __init__(self,channels=17,tower_min_max_only=False):\n",
    "        'Initialization'\n",
    "        self.channels = channels\n",
    "        self.tower_min_max_only = tower_min_max_only\n",
    "        \n",
    "\n",
    "    def model_maker(self,d1=8,d3=16):\n",
    "                \n",
    "        inputs = keras.Input(shape=(200, 200, self.channels))\n",
    "\n",
    "        inp1 = keras.Input(shape=(200, 200, self.channels))\n",
    "        inp2 = keras.Input(shape=(200, 200, self.channels))\n",
    "        inp3 = keras.Input(shape=(200, 200, self.channels))   \n",
    "        \n",
    "        incept_3 = layers.Conv2D(d1, (3,3),strides=(1,1),padding='same',activation='relu')(inputs)\n",
    "        incept_5 = layers.Conv2D(d1, (5,5),strides=(1,1),padding='same',activation='relu')(inputs)\n",
    "        incept_7 = layers.Conv2D(d1, (7,7),strides=(1,1),padding='same',activation='relu')(inputs)\n",
    "        incept_9 = layers.Conv2D(d1, (9,9),strides=(1,1),padding='same',activation='relu')(inputs)\n",
    "        \n",
    "        incept_3_pool = layers.MaxPooling2D(pool_size=(4, 4))(incept_3)\n",
    "        incept_5_pool = layers.MaxPooling2D(pool_size=(4, 4))(incept_5)\n",
    "        incept_7_pool = layers.MaxPooling2D(pool_size=(4, 4))(incept_7)\n",
    "        incept_9_pool = layers.MaxPooling2D(pool_size=(4, 4))(incept_9)\n",
    "        \n",
    "        all_output = layers.concatenate([incept_3_pool, incept_5_pool, incept_7_pool,incept_9_pool], axis=3)\n",
    "    \n",
    "        incept_3 = layers.Conv2D(d1, (3,3),strides=(1,1),padding='same',activation='relu')(all_output)\n",
    "        incept_5 = layers.Conv2D(d1, (5,5),strides=(1,1),padding='same',activation='relu')(all_output)\n",
    "        incept_7 = layers.Conv2D(d1, (7,7),strides=(1,1),padding='same',activation='relu')(all_output)\n",
    "        incept_9 = layers.Conv2D(d1, (9,9),strides=(1,1),padding='same',activation='relu')(all_output)\n",
    "          \n",
    "        all_output_2 = layers.concatenate([incept_3, incept_5, incept_7,incept_9], axis=3)\n",
    "        all_output=  layers.add([all_output,all_output_2])\n",
    "        all_output = layers.MaxPooling2D(pool_size=(2,2))(all_output)\n",
    "     \n",
    "        incept_3 = layers.Conv2D(d3, (3,3),strides=(1,1),padding='same',activation='relu')(all_output)\n",
    "        incept_5 = layers.Conv2D(d3, (5,5),strides=(1,1),padding='same',activation='relu')(all_output)\n",
    "        incept_7 = layers.Conv2D(d3, (7,7),strides=(1,1),padding='same',activation='relu')(all_output)\n",
    "        incept_9 = layers.Conv2D(d3, (9,9),strides=(1,1),padding='same',activation='relu')(all_output)\n",
    "        \n",
    "        incept_3_pool = layers.MaxPooling2D(pool_size=(2, 2))(incept_3)\n",
    "        incept_5_pool = layers.MaxPooling2D(pool_size=(2, 2))(incept_5)\n",
    "        incept_7_pool = layers.MaxPooling2D(pool_size=(2, 2))(incept_7)\n",
    "        incept_9_pool = layers.MaxPooling2D(pool_size=(2, 2))(incept_9)\n",
    "        \n",
    "        all_output = layers.concatenate([incept_3_pool, incept_5_pool, incept_7_pool,incept_9_pool], axis=3)\n",
    "        \n",
    "        incept_3 = layers.Conv2D(d3, (3,3),strides=(1,1),padding='same',activation='relu')(all_output)\n",
    "        incept_5 = layers.Conv2D(d3, (5,5),strides=(1,1),padding='same',activation='relu')(all_output)\n",
    "        incept_7 = layers.Conv2D(d3, (7,7),strides=(1,1),padding='same',activation='relu')(all_output)\n",
    "        incept_9 = layers.Conv2D(d3, (9,9),strides=(1,1),padding='same',activation='relu')(all_output)\n",
    "        \n",
    "        all_output_4 = layers.concatenate([incept_3, incept_5, incept_7,incept_9], axis=3)\n",
    "        all_output=  layers.add([all_output,all_output_4])\n",
    "        all_output = layers.MaxPooling2D(pool_size=(2,2))(all_output)\n",
    "\n",
    "        \n",
    "        parallel = keras.models.Model(inputs, all_output, name='parallel')     \n",
    "\n",
    "        tower_1 = parallel(inp1)\n",
    "        tower_2 = parallel(inp2)\n",
    "        tower_3 = parallel(inp3)\n",
    "        \n",
    "        if self.tower_min_max_only:\n",
    "            tower_max = layers.maximum([tower_1,tower_2,tower_3])\n",
    "            tower_min=layers.minimum([tower_max, tower_2,tower_3])\n",
    "            #tower_average=layers.average([tower_1, tower_2,tower_3])\n",
    "            #tower_mul= layers.Multiply([tower_1, tower_2,tower_3])#since multiplication is expensive\n",
    "            merged = layers.concatenate([tower_max, tower_min], axis=1)\n",
    "            model_name = ''.join(['MIN_MAX_model_inception_all__depths__inception_complic_vin_1_d1',str(d1),'_d3_',str(d3),'.h5'])\n",
    "\n",
    "        else:\n",
    "            merged = layers.concatenate([tower_1, tower_2, tower_3], axis=1)\n",
    "            model_name = ''.join(['model_inception_all__depths__inception_complic_vin_1_d1',str(d1),'_d3_',str(d3),'.h5'])\n",
    "\n",
    "        merged = layers.Flatten()(merged)\n",
    "        all_co = layers.Dense(100, activation='relu')(merged)\n",
    "        all_co = layers.Dropout(0.5)(all_co)\n",
    "        all_co = layers.Dense(50, activation='relu')(all_co)\n",
    "        all_co= layers.Dropout(0.5)(all_co)\n",
    "        outputs = layers.Dense(3, activation='softmax')(all_co)\n",
    "\n",
    "        model = keras.models.Model(inputs=[inp1, inp2,inp3], outputs=outputs, name=model_name)\n",
    "    \n",
    "        return model,model_name\n",
    "\n",
    "'''model parallel tower -6'''\n",
    "class model_par_parallel_inception_all__depths_min_max_inception_complic_vin_1:\n",
    "    \n",
    "    def __init__(self,channels=17,tower_min_max_only=False):\n",
    "        'Initialization'\n",
    "        self.channels = channels\n",
    "        self.tower_min_max_only = tower_min_max_only\n",
    "        \n",
    "\n",
    "    def model_maker(self,d1=8,d3=16):\n",
    "                \n",
    "        inputs = keras.Input(shape=(200, 200, self.channels))\n",
    "\n",
    "        inp1 = keras.Input(shape=(200, 200, self.channels))\n",
    "        inp2 = keras.Input(shape=(200, 200, self.channels))\n",
    "        inp3 = keras.Input(shape=(200, 200, self.channels))   \n",
    "        \n",
    "        incept_3 = layers.Conv2D(d1, (3,3),strides=(1,1),padding='same',activation='relu')(inputs)\n",
    "        incept_5 = layers.Conv2D(d1, (5,5),strides=(1,1),padding='same',activation='relu')(inputs)\n",
    "        incept_7 = layers.Conv2D(d1, (7,7),strides=(1,1),padding='same',activation='relu')(inputs)\n",
    "        incept_9 = layers.Conv2D(d1, (9,9),strides=(1,1),padding='same',activation='relu')(inputs)\n",
    "        \n",
    "        incept_3_pool = layers.MaxPooling2D(pool_size=(4, 4))(incept_3)\n",
    "        incept_5_pool = layers.MaxPooling2D(pool_size=(4, 4))(incept_5)\n",
    "        incept_7_pool = layers.MaxPooling2D(pool_size=(4, 4))(incept_7)\n",
    "        incept_9_pool = layers.MaxPooling2D(pool_size=(4, 4))(incept_9)\n",
    "        \n",
    "        all_output_max = layers.maximum([incept_3_pool, incept_5_pool, incept_7_pool,incept_9_pool])\n",
    "        all_output_min =layers.minimum([incept_3_pool, incept_5_pool, incept_7_pool,incept_9_pool])\n",
    "        all_output = layers.concatenate([all_output_max, all_output_min], axis=3)\n",
    "        \n",
    "        incept_3 = layers.Conv2D(d1, (3,3),strides=(1,1),padding='same',activation='relu')(all_output)\n",
    "        incept_5 = layers.Conv2D(d1, (5,5),strides=(1,1),padding='same',activation='relu')(all_output)\n",
    "        incept_7 = layers.Conv2D(d1, (7,7),strides=(1,1),padding='same',activation='relu')(all_output)\n",
    "        incept_9 = layers.Conv2D(d1, (9,9),strides=(1,1),padding='same',activation='relu')(all_output)\n",
    "        \n",
    "        all_output_2_max = layers.maximum([incept_3, incept_5, incept_7,incept_9]) \n",
    "        all_output_2_min =layers.minimum([incept_3, incept_5, incept_7,incept_9]) \n",
    "        all_output_2 = layers.concatenate([all_output_2_max, all_output_2_min], axis=3)\n",
    "        \n",
    "        all_output=  layers.add([all_output,all_output_2])\n",
    "        all_output = layers.MaxPooling2D(pool_size=(2,2))(all_output)\n",
    "         \n",
    "        incept_3 = layers.Conv2D(d3, (3,3),strides=(1,1),padding='same',activation='relu')(all_output)\n",
    "        incept_5 = layers.Conv2D(d3, (5,5),strides=(1,1),padding='same',activation='relu')(all_output)\n",
    "        incept_7 = layers.Conv2D(d3, (7,7),strides=(1,1),padding='same',activation='relu')(all_output)\n",
    "        incept_9 = layers.Conv2D(d3, (9,9),strides=(1,1),padding='same',activation='relu')(all_output)\n",
    "        \n",
    "        incept_3_pool = layers.MaxPooling2D(pool_size=(2, 2))(incept_3)\n",
    "        incept_5_pool = layers.MaxPooling2D(pool_size=(2, 2))(incept_5)\n",
    "        incept_7_pool = layers.MaxPooling2D(pool_size=(2, 2))(incept_7)\n",
    "        incept_9_pool = layers.MaxPooling2D(pool_size=(2, 2))(incept_9)\n",
    "        \n",
    "        all_output_max = layers.maximum([incept_3_pool, incept_5_pool, incept_7_pool,incept_9_pool])\n",
    "        all_output_min =layers.minimum([incept_3_pool, incept_5_pool, incept_7_pool,incept_9_pool])\n",
    "        all_output = layers.concatenate([all_output_max, all_output_min], axis=3)\n",
    "        \n",
    "        incept_3 = layers.Conv2D(d3, (3,3),strides=(1,1),padding='same',activation='relu')(all_output)\n",
    "        incept_5 = layers.Conv2D(d3, (5,5),strides=(1,1),padding='same',activation='relu')(all_output)\n",
    "        incept_7 = layers.Conv2D(d3, (7,7),strides=(1,1),padding='same',activation='relu')(all_output)\n",
    "        incept_9 = layers.Conv2D(d3, (9,9),strides=(1,1),padding='same',activation='relu')(all_output)\n",
    "        \n",
    "        all_output_2_max = layers.maximum([incept_3, incept_5, incept_7,incept_9]) \n",
    "        all_output_2_min =layers.minimum([incept_3, incept_5, incept_7,incept_9]) \n",
    "        all_output_2 = layers.concatenate([all_output_2_max, all_output_2_min], axis=3)\n",
    "        \n",
    "        all_output=  layers.add([all_output,all_output_2])\n",
    "        all_output = layers.MaxPooling2D(pool_size=(2,2))(all_output)\n",
    "\n",
    "        \n",
    "        parallel = keras.models.Model(inputs, all_output, name='parallel')     \n",
    "\n",
    "        tower_1 = parallel(inp1)\n",
    "        tower_2 = parallel(inp2)\n",
    "        tower_3 = parallel(inp3)\n",
    "        \n",
    "        if self.tower_min_max_only:\n",
    "            tower_max = layers.maximum([tower_1,tower_2,tower_3])\n",
    "            tower_min=layers.minimum([tower_max, tower_2,tower_3])\n",
    "            #tower_average=layers.average([tower_1, tower_2,tower_3])\n",
    "            #tower_mul= layers.Multiply([tower_1, tower_2,tower_3])#since multiplication is expensive\n",
    "            merged = layers.concatenate([tower_max, tower_min], axis=1)\n",
    "            model_name = ''.join(['MIN_MAX_model_parallel_inception_all__depths_min_max_inception_complic_vin_1_d1',str(d1),'_d3_',str(d3),'.h5'])\n",
    "        else:\n",
    "            merged = layers.concatenate([tower_1, tower_2, tower_3], axis=1)\n",
    "            model_name = ''.join(['model_parallel_inception_all__depths_min_max_inception_complic_vin_1_d1',str(d1),'_d3_',str(d3),'.h5'])\n",
    "\n",
    "        merged = layers.Flatten()(merged)\n",
    "        all_co = layers.Dense(100, activation='relu')(merged)\n",
    "        all_co = layers.Dropout(0.5)(all_co)\n",
    "        all_co = layers.Dense(50, activation='relu')(all_co)\n",
    "        all_co= layers.Dropout(0.5)(all_co)\n",
    "        outputs = layers.Dense(3, activation='softmax')(all_co)\n",
    "\n",
    "        model = keras.models.Model(inputs=[inp1, inp2,inp3], outputs=outputs, name=model_name)\n",
    "    \n",
    "        return model,model_name\n",
    "    \n",
    "class model_par_inception_w_o_addition_vin_exp:\n",
    "    \n",
    "    def __init__(self,channels=17,tower_min_max_only=False):\n",
    "        'Initialization'\n",
    "        self.channels = channels\n",
    "        self.tower_min_max_only = tower_min_max_only       \n",
    "\n",
    "    def model_maker(self,d1=96,d2=32,k2=3,k3=3,k4=3):#or d1=64 (32 x 3)\n",
    "        \n",
    "        inputs = keras.Input(shape=(200, 200, self.channels))\n",
    "\n",
    "        inp1 = keras.Input(shape=(200, 200, self.channels))\n",
    "        inp2 = keras.Input(shape=(200, 200, self.channels))\n",
    "        inp3 = keras.Input(shape=(200, 200, self.channels))\n",
    "        \n",
    "        incept_3 = layers.Conv2D(d1, (3,3),strides=(1,1),padding='same',activation='relu')(inputs)\n",
    "        incept_5 = layers.Conv2D(d2, (5,5),strides=(1,1),padding='same',activation='relu')(inputs)\n",
    "        data = layers.concatenate([incept_3, incept_5], axis=3)\n",
    "        data = layers.MaxPooling2D(pool_size=(4, 4))(data)\n",
    "        \n",
    "        data = layers.Conv2D(d1+d2, (k2,k2),strides=(1,1),padding='same',activation='relu')(data)   \n",
    "        data = layers.MaxPooling2D(pool_size=(2, 2))(data)\n",
    "    \n",
    "        data = layers.Conv2D(d1+d2, (k3,k3), activation='relu', padding='same')(data)\n",
    "        '''to be removed'''\n",
    "        data = layers.MaxPooling2D(pool_size=(2, 2))(data)\n",
    "        data = layers.Conv2D(64, (k4,k4), activation='relu', padding='same')(data)\n",
    "        data = layers.MaxPooling2D(pool_size=(2, 2))(data)\n",
    "        \n",
    "        parallel = keras.models.Model(inputs, data, name='parallel')     \n",
    "\n",
    "        tower_1 = parallel(inp1)\n",
    "        tower_2 = parallel(inp2)\n",
    "        tower_3 = parallel(inp3)\n",
    "        \n",
    "        if self.tower_min_max_only:\n",
    "            tower_max = layers.maximum([tower_1,tower_2,tower_3])\n",
    "            tower_min=layers.minimum([tower_max, tower_2,tower_3])\n",
    "            #tower_average=layers.average([tower_1, tower_2,tower_3])\n",
    "            #tower_mul= layers.Multiply([tower_1, tower_2,tower_3])#since multiplication is expensive\n",
    "            merged = layers.concatenate([tower_max, tower_min], axis=1)\n",
    "            model_name = ''.join(['MIN_MAX_model_par_inception_w_o_addition_vin_exp_d1_',str(d1),'_d2',str(d2),'_k2_',str(k2),'_k3_',str(k3),'_k4_',str(k4),'.h5'])\n",
    "\n",
    "        else:\n",
    "            merged = layers.concatenate([tower_1, tower_2, tower_3], axis=1)\n",
    "            model_name = ''.join(['model_par_inception_w_o_addition_vin_exp_d1_',str(d1),'_d2_',str(d2),'_k2_',str(k2),'_k3_',str(k3),'_k4_',str(k4),'.h5'])\n",
    "\n",
    "        merged = layers.Flatten()(merged)\n",
    "        all_co = layers.Dense(100, activation='relu')(merged)\n",
    "        all_co = layers.Dropout(0.5)(all_co)\n",
    "        all_co = layers.Dense(50, activation='relu')(all_co)\n",
    "        all_co= layers.Dropout(0.5)(all_co)\n",
    "        outputs = layers.Dense(3, activation='softmax')(all_co)\n",
    "\n",
    "        model = keras.models.Model(inputs=[inp1, inp2,inp3], outputs=outputs, name=model_name)\n",
    "        \n",
    "        return model,model_name\n",
    "\n",
    "class model_par_parallel_inception_all_k3x3_k_5_depths_min_max_inception_vin_1:\n",
    "    \n",
    "    def __init__(self,channels=17,tower_min_max_only=False):\n",
    "        'Initialization'\n",
    "        self.channels = channels\n",
    "        self.tower_min_max_only = tower_min_max_only\n",
    "        \n",
    "    def model_maker(self,d3=96,d5=32):\n",
    "        inputs = keras.Input(shape=(200, 200, self.channels))\n",
    "\n",
    "        inp1 = keras.Input(shape=(200, 200, self.channels))\n",
    "        inp2 = keras.Input(shape=(200, 200, self.channels))\n",
    "        inp3 = keras.Input(shape=(200, 200, self.channels))    \n",
    "       \n",
    "        incept_3 = layers.Conv2D(d3, (3,3),strides=(1,1),padding='same',activation='relu')(inputs)\n",
    "        incept_5 = layers.Conv2D(d5, (5,5),strides=(1,1),padding='same',activation='relu')(inputs)\n",
    "        \n",
    "        incept_3_pool = layers.MaxPooling2D(pool_size=(4, 4))(incept_3)\n",
    "        incept_5_pool = layers.MaxPooling2D(pool_size=(4, 4))(incept_5)\n",
    "\n",
    "        all_output = layers.concatenate([incept_3_pool, incept_5_pool], axis=3)\n",
    "        \n",
    "        incept_3 = layers.Conv2D(d3, (3,3),strides=(1,1),padding='same',activation='relu')(all_output)\n",
    "        incept_5 = layers.Conv2D(d5, (5,5),strides=(1,1),padding='same',activation='relu')(all_output)\n",
    "        \n",
    "        incept_3_pool = layers.MaxPooling2D(pool_size=(2, 2))(incept_3)\n",
    "        incept_5_pool = layers.MaxPooling2D(pool_size=(2, 2))(incept_5)\n",
    "\n",
    "        all_output = layers.concatenate([incept_3_pool, incept_5_pool], axis=3)\n",
    "        \n",
    "        incept_3 = layers.Conv2D(2*d3, (3,3),strides=(1,1),padding='same',activation='relu')(all_output)\n",
    "        incept_5 = layers.Conv2D(2*d5, (5,5),strides=(1,1),padding='same',activation='relu')(all_output)\n",
    "        \n",
    "        incept_3_pool = layers.MaxPooling2D(pool_size=(2, 2))(incept_3)\n",
    "        incept_5_pool = layers.MaxPooling2D(pool_size=(2, 2))(incept_5)\n",
    "\n",
    "        all_output = layers.concatenate([incept_3_pool, incept_5_pool], axis=3)\n",
    "        \n",
    "        incept_3 = layers.Conv2D(2*d3, (3,3),strides=(1,1),padding='same',activation='relu')(all_output)\n",
    "        incept_5 = layers.Conv2D(2*d5, (5,5),strides=(1,1),padding='same',activation='relu')(all_output)\n",
    "        \n",
    "        incept_3_pool = layers.MaxPooling2D(pool_size=(2, 2))(incept_3)\n",
    "        incept_5_pool = layers.MaxPooling2D(pool_size=(2, 2))(incept_5)\n",
    "\n",
    "        all_output = layers.concatenate([incept_3_pool, incept_5_pool], axis=3)\n",
    "    \n",
    "        parallel = keras.models.Model(inputs, all_output, name='parallel')     \n",
    "\n",
    "        tower_1 = parallel(inp1)\n",
    "        tower_2 = parallel(inp2)\n",
    "        tower_3 = parallel(inp3)\n",
    "        \n",
    "        if self.tower_min_max_only:\n",
    "            tower_max = layers.maximum([tower_1,tower_2,tower_3])\n",
    "            tower_min=layers.minimum([tower_max, tower_2,tower_3])\n",
    "            #tower_average=layers.average([tower_1, tower_2,tower_3])\n",
    "            #tower_mul= layers.Multiply([tower_1, tower_2,tower_3])#since multiplication is expensive\n",
    "            merged = layers.concatenate([tower_max, tower_min], axis=1)\n",
    "            model_name = ''.join(['MIN_MAX_model_par_parallel_inception_all_k3x3_k_5_depths_min_max_inception_vin_1_d3_',str(d3),'_d5_',str(d5),'.h5'])\n",
    "\n",
    "        else:\n",
    "            merged = layers.concatenate([tower_1, tower_2, tower_3], axis=1)\n",
    "            model_name = ''.join(['model_par_parallel_inception_all_k3x3_k_5_depths_min_max_inception_vin_1_d3_',str(d3),'_d5_',str(d5),'.h5'])\n",
    "        \n",
    "        merged = layers.Flatten()(merged)\n",
    "        all_co = layers.Dense(100, activation='relu')(merged)\n",
    "        all_co = layers.Dropout(0.5)(all_co)\n",
    "        all_co = layers.Dense(50, activation='relu')(all_co)\n",
    "        all_co= layers.Dropout(0.5)(all_co)\n",
    "        outputs = layers.Dense(3, activation='softmax')(all_co)\n",
    "\n",
    "        model = keras.models.Model(inputs=[inp1, inp2,inp3], outputs=outputs, name=model_name)\n",
    "    \n",
    "        return model,model_name    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "784a7649",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcopy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deepcopy\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import keras\n",
    "from copy import deepcopy\n",
    "import pickle\n",
    "import os\n",
    "import random\n",
    "\n",
    "class DataGenerator_splited:\n",
    "    'Generates 2D projected data for Keras'\n",
    "    def __init__(self, labels,main_directory, batch_size=32, dim = (200,200), n_channels=17 ,n_classes=3,iter_train=199, shuffle=True,Full_clean=True,balance_batch_must=True,validation_split=0.2):\n",
    "        '''Initialization\n",
    "        labels\n",
    "        main_directory:\n",
    "        batch_size=32\n",
    "        dim = (200,200)\n",
    "        n_channels=17 \n",
    "        n_classes=3\n",
    "        iter_train=199\n",
    "        shuffle=True\n",
    "        Full_clean=True\n",
    "        balance_batch_must: True make sure all batches has same class distribution\n",
    "                            False not forcing the all batches have same class distribution\n",
    "        validation_split: Fraction split the validation data from the whole training set\n",
    "                        Default value is 20%\n",
    "                     **   Especially the TSG split is same as OG split size\n",
    "                       **  and the Fusion split is 0.5 x Split persent to avoid models biased to OG\n",
    "        '''\n",
    "        self.batch_size = batch_size\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.dim = dim\n",
    "        self.Full_clean= Full_clean\n",
    "        self.iter_train=iter_train\n",
    "        self.labels=labels\n",
    "        self.main_directory= main_directory\n",
    "        self.balance_batch_must=balance_batch_must\n",
    "        self.validation_split=validation_split\n",
    "     \n",
    "        os.chdir('/')\n",
    "        os.chdir(main_directory)    \n",
    "        if validation_split>0:\n",
    "            #split the training and validation data\n",
    "            #first load the all training data\n",
    "            all_OG_train_PDBs = pickle.load(open(\"OG_train_PDBs.p\", \"rb\")) \n",
    "            all_TSG_train_PDBs = pickle.load(open(\"TSG_train_PDBs.p\", \"rb\"))      \n",
    "            all_Fusion_train_PDBs = pickle.load(open(\"Fusion_train_PDBs.p\", \"rb\"))  \n",
    "            \n",
    "            print('')\n",
    "            print('Over all training data in:')\n",
    "            print('OG: ',len(all_OG_train_PDBs))\n",
    "            print('TSG: ',len(all_TSG_train_PDBs))        \n",
    "            print('Fusion: ',len(all_Fusion_train_PDBs))    \n",
    "            #to avoid validation class has overlapping classes\n",
    "            self.overlapped_PDBs_ONGO = pickle.load(open(\"overlapped_PDBs_ONGO.p\", \"rb\"))    \n",
    "            self.overlapped_PDBs_TSG = pickle.load(open(\"overlapped_PDBs_TSG.p\", \"rb\"))  \n",
    "            # shuffle the data first\n",
    "            chk_OG=deepcopy(list(range(len(all_OG_train_PDBs))))\n",
    "            chk_TSG=deepcopy(list(range(len(all_TSG_train_PDBs))))\n",
    "            chk_Fusion=deepcopy(list(range(len(all_Fusion_train_PDBs))))    \n",
    "            if shuffle:\n",
    "                random.shuffle(chk_OG)\n",
    "                random.shuffle(chk_TSG)\n",
    "                random.shuffle(chk_Fusion)    \n",
    "\n",
    "            validation_OG = []\n",
    "            over_chk_OG=0\n",
    "            # print(' self.overlapped_PDBs_ONGO: ',len(self.overlapped_PDBs_ONGO))\n",
    "            # print('all_OG_train_PDBs: ',len(all_OG_train_PDBs))\n",
    "            # print('len chk_OG: ',len(chk_OG))\n",
    "            OG_valid_split_size = int(validation_split*len(chk_OG))\n",
    "            for i in range(0,OG_valid_split_size):\n",
    "                # print(' all_OG_train_PDBs[chk_OG[i+over_chk_OG]] i: ',i,'over_chk_OG ',over_chk_OG)\n",
    "                if all_OG_train_PDBs[chk_OG[i+over_chk_OG]] not in self.overlapped_PDBs_ONGO:\n",
    "                    validation_OG.append(all_OG_train_PDBs.pop(chk_OG[i+over_chk_OG]))\n",
    "                else:\n",
    "                    while (all_OG_train_PDBs[chk_OG[i+over_chk_OG]] in self.overlapped_PDBs_ONGO):\n",
    "                        over_chk_OG=over_chk_OG+1\n",
    "                    validation_OG.append(all_OG_train_PDBs.pop(chk_OG[i+over_chk_OG]))         \n",
    "                chk_OG = self.pop_range_fix(chk_OG,chk_OG[i+over_chk_OG])\n",
    "\n",
    "            self.validation_OG=validation_OG\n",
    "            self.OG_train_PDBs = all_OG_train_PDBs\n",
    "            print('validation creation ONGO done :)')\n",
    "            print('')\n",
    "            print('Training data size OG: ',len(self.OG_train_PDBs))\n",
    "            print('Validati data size OG: ',len(validation_OG))\n",
    "            print('')\n",
    "\n",
    "            validation_TSG=[]\n",
    "            over_chk_TSG=0\n",
    "            # print(int(validation_split*len(chk_TSG)))\n",
    "            # print('overlapped_PDBs_TSG: ',len(self.overlapped_PDBs_TSG))\n",
    "            for i in range(0,OG_valid_split_size):\n",
    "                '''here the validation split is equal as OG split'''\n",
    "                if all_TSG_train_PDBs[chk_TSG[i+over_chk_TSG]] not in self.overlapped_PDBs_TSG:\n",
    "                    validation_TSG.append(all_TSG_train_PDBs.pop(chk_TSG[i+over_chk_TSG]))\n",
    "                else:\n",
    "                    while (all_TSG_train_PDBs[chk_TSG[i+over_chk_TSG]] in self.overlapped_PDBs_TSG):\n",
    "                        over_chk_TSG=over_chk_TSG+1\n",
    "                    validation_TSG.append(all_TSG_train_PDBs.pop(chk_TSG[i+over_chk_TSG]))   \n",
    "                chk_TSG = self.pop_range_fix(chk_TSG,chk_TSG[i+over_chk_TSG])\n",
    "\n",
    "            print('validation creation TSG done :)')\n",
    "            print('')\n",
    "      \n",
    "            validation_Fusion = []\n",
    "            over_chk_Fusion=0\n",
    "            for i in range(0,int(validation_split*0.5*len(chk_Fusion))):\n",
    "                if (all_Fusion_train_PDBs[chk_Fusion[i+over_chk_Fusion]] not in self.overlapped_PDBs_TSG) and (all_Fusion_train_PDBs[chk_Fusion[i+over_chk_Fusion]] not in self.overlapped_PDBs_ONGO):\n",
    "                    validation_Fusion.append(all_Fusion_train_PDBs.pop(chk_Fusion[i+over_chk_Fusion]))    \n",
    "                else:\n",
    "                    while  (all_Fusion_train_PDBs[chk_Fusion[i+over_chk_Fusion]] in self.overlapped_PDBs_TSG) or (all_Fusion_train_PDBs[chk_Fusion[i+over_chk_Fusion]] in self.overlapped_PDBs_ONGO):\n",
    "                        over_chk_Fusion = over_chk_Fusion + 1\n",
    "                    validation_Fusion.append(all_Fusion_train_PDBs.pop(chk_Fusion[i+over_chk_Fusion]))    \n",
    "                chk_Fusion = self.pop_range_fix(chk_Fusion,chk_Fusion[i+over_chk_Fusion])\n",
    "\n",
    "            print('validation creation Fusion done :)')\n",
    "            print('')\n",
    "\n",
    "            self.validation_TSG=validation_TSG\n",
    "            self.validation_Fusion=validation_Fusion\n",
    "            \n",
    "            self.TSG_train_PDBs = all_TSG_train_PDBs  \n",
    "            self.Fusion_train_PDBs = all_Fusion_train_PDBs\n",
    "            print('')\n",
    "            print('-------------After vlidation split ------------')\n",
    "\n",
    "            print('Training data size TSG: ',len(self.TSG_train_PDBs))\n",
    "            print('Validati data size TSG: ',len(validation_TSG))\n",
    "            print('Training data size Fusion: ',len(self.Fusion_train_PDBs))\n",
    "            print('Validati data size Fusion: ',len(validation_Fusion))\n",
    "            print('')\n",
    "            print('')\n",
    "\n",
    "            valid_labels_dic={}\n",
    "            valid_ids=validation_OG+validation_TSG+validation_Fusion\n",
    "            \n",
    "            for i in range(0,len(validation_OG)):\n",
    "                valid_labels_dic.update({validation_OG[i]: 0})\n",
    "            for i in range(0,len(validation_TSG)):\n",
    "                valid_labels_dic.update({validation_TSG[i]: 1})\n",
    "            for i in range(0,len(validation_Fusion)):\n",
    "                valid_labels_dic.update({validation_Fusion[i]: 2})\n",
    "\n",
    "            pickle.dump(valid_labels_dic, open(\"valid_labels_dic.p\", \"wb\"))  \n",
    "            pickle.dump(valid_ids, open(\"valid_ids.p\", \"wb\"))  \n",
    "            lengths_summery_dic={}\n",
    "            lengths_summery_dic.update({'valid_len':len(self.validation_OG)+ len(self.validation_TSG)+ len(self.validation_Fusion)})\n",
    "            lengths_summery_dic.update({'train_len':len(self.OG_train_PDBs)+len(self.TSG_train_PDBs)+len(self.Fusion_train_PDBs)})\n",
    "            lengths_summery_dic.update({'valid_OG_len':len(self.validation_OG)})\n",
    "            lengths_summery_dic.update({'valid_TSG_len':len(self.validation_TSG)})\n",
    "            lengths_summery_dic.update({'valid_Fusion_len':len(self.validation_Fusion)})\n",
    "            pickle.dump(lengths_summery_dic, open(\"lengths_summery_dic.p\", \"wb\")) \n",
    "\n",
    "            self.lengths_summery_dic = lengths_summery_dic\n",
    "            self.iter_train=len(self.OG_train_PDBs)+len(self.TSG_train_PDBs)+len(self.Fusion_train_PDBs)#or whole train set size\n",
    "            #according to this only the data is fetched\n",
    "        else:\n",
    "            self.OG_train_PDBs = pickle.load(open(\"OG_train_PDBs.p\", \"rb\"))  \n",
    "            self.TSG_train_PDBs = pickle.load(open(\"TSG_train_PDBs.p\", \"rb\"))      \n",
    "            self.Fusion_train_PDBs = pickle.load(open(\"Fusion_train_PDBs.p\", \"rb\"))  \n",
    "            lengths_summery_dic={}\n",
    "            lengths_summery_dic.update({'train_len':len(self.OG_train_PDBs)+len(self.TSG_train_PDBs)+len(self.Fusion_train_PDBs)})\n",
    "            lengths_summery_dic.update({'train_OG_len':len(self.OG_train_PDBs)})\n",
    "            lengths_summery_dic.update({'train_TSG_len':len(self.TSG_train_PDBs)})\n",
    "            lengths_summery_dic.update({'train_Fusion_len':len(self.Fusion_train_PDBs)})\n",
    "\n",
    "            self.lengths_summery_dic = lengths_summery_dic\n",
    "            self.iter_train=len(self.OG_train_PDBs)+len(self.TSG_train_PDBs)+len(self.Fusion_train_PDBs)#or whole train set size\n",
    "            #according to this only the data is fetched\n",
    "        if self.Full_clean:\n",
    "           self.clean_list_of_ids= []\n",
    "           self.clean_list_of_id_classes = []\n",
    "        else:\n",
    "           self.overlapped_PDBs_ONGO = pickle.load(open(\"overlapped_PDBs_ONGO.p\", \"rb\"))    \n",
    "           self.overlapped_PDBs_TSG = pickle.load(open(\"overlapped_PDBs_TSG.p\", \"rb\"))  \n",
    "           self.list_of_ids= []\n",
    "           self.list_of_id_classes = []\n",
    "       \n",
    "        # thisindex_former create the randomly shuffled data\n",
    "        self.index_former()\n",
    "\n",
    "    def fetch_size(self):\n",
    "        return self.iter_train\n",
    "\n",
    "    def pop_range_fix(self,list_1,pop_element_num):\n",
    "        '''\n",
    "        To fixing the overlap remoaval for validation data creation\n",
    "        '''\n",
    "        list_2=deepcopy(list_1)\n",
    "        for k in range(0,len(list_1)):\n",
    "            if list_1[k]>pop_element_num:\n",
    "                list_2[k]=list_2[k]-1\n",
    "        return list_2\n",
    "    \n",
    "    \n",
    "    def index_former(self):\n",
    "        \n",
    "        self.indexes = np.arange(self.iter_train)\n",
    "        #to creat the indexes easy\n",
    "        index_ongo=[]\n",
    "        index_tsg=[]\n",
    "        index_fusion=[]\n",
    "        # since the Fusion indexes runing out early \n",
    "        ''' reuse the Fusion class again or combined class'''\n",
    "        # since the TSG has higher number of labels where iter means the number of irteration each training\n",
    "        # and since the batch size is 32 and data taken by model is 16 thus the iter is factored by 2\n",
    "        if self.Full_clean:\n",
    "            if self.balance_batch_must:\n",
    "                for i in range(0,(self.iter_train)//16): \n",
    "                    #choose 5 from ONGO 7 from TSG and 4 From Fusion to make a batch that distribute the all most all PDBs\n",
    "                    #Fusion_chk=True\n",
    "                    '''Since the Models performance always biased to ONGO'''\n",
    "                    index_ongo,index_tsg,index_fusion=self.adding_fully_clean(True,index_ongo,index_tsg,index_fusion)\n",
    "                    index_ongo,index_tsg,index_fusion=self.adding_fully_clean(False,index_ongo,index_tsg,index_fusion,OG_IN=False)\n",
    "                    index_ongo,index_tsg,index_fusion=self.adding_fully_clean(True,index_ongo,index_tsg,index_fusion)\n",
    "                    index_ongo,index_tsg,index_fusion=self.adding_fully_clean(False,index_ongo,index_tsg,index_fusion)\n",
    "                    index_ongo,index_tsg,index_fusion=self.adding_fully_clean(True,index_ongo,index_tsg,index_fusion)\n",
    "                    index_ongo,index_tsg,index_fusion=self.adding_fully_clean(True,index_ongo,index_tsg,index_fusion)\n",
    "                    index_ongo,index_tsg,index_fusion=self.adding_fully_clean(False,index_ongo,index_tsg,index_fusion,OG_IN=False)\n",
    "\n",
    "                if (self.iter_train)%16>0: \n",
    "                    index_ongo,index_tsg,index_fusion=self.adding_fully_clean(True,index_ongo,index_tsg,index_fusion,OG_IN=False)\n",
    "                    index_ongo,index_tsg,index_fusion=self.adding_fully_clean(True,index_ongo,index_tsg,index_fusion,OG_IN=False)\n",
    "                    index_ongo,index_tsg,index_fusion=self.adding_fully_clean(False,index_ongo,index_tsg,index_fusion)\n",
    "                    index_ongo,index_tsg,index_fusion=self.adding_fully_clean(True,index_ongo,index_tsg,index_fusion)\n",
    "                    index_ongo,index_tsg,index_fusion=self.adding_fully_clean(False,index_ongo,index_tsg,index_fusion)\n",
    "                    index_ongo,index_tsg,index_fusion=self.adding_fully_clean(False,index_ongo,index_tsg,index_fusion)\n",
    "                    index_ongo,index_tsg,index_fusion=self.adding_fully_clean(True,index_ongo,index_tsg,index_fusion)\n",
    "                    \n",
    "                os.chdir('/')\n",
    "                os.chdir(self.main_directory)               \n",
    "                if self.validation_split>0:\n",
    "                    pickle.dump(self.clean_list_of_ids, open(\"V_clean_balnce_must_list_of_ids.p\", \"wb\"))  \n",
    "                    pickle.dump(self.clean_list_of_id_classes, open(\"V_clean_balnce_must_list_of_id_classes.p\", \"wb\")) \n",
    "                else:\n",
    "                    pickle.dump(self.clean_list_of_ids, open(\"clean_balnce_must_list_of_ids.p\", \"wb\"))  \n",
    "                    pickle.dump(self.clean_list_of_id_classes, open(\"clean_balnce_must_list_of_id_classes.p\", \"wb\")) \n",
    "            else:\n",
    "                self.not_balance_must_and_Fully_clean()\n",
    "            \n",
    "        else:\n",
    "            if self.balance_batch_must:\n",
    "                '''First create the indexes for overlapped PDBs'''\n",
    "                index_ongo_fusion = self.index_stack_overlapped_chk(True,False)\n",
    "                index_tsg_fusion = self.index_stack_overlapped_chk(False,True)\n",
    "                for i in range(0,(self.iter_train)//16): \n",
    "                    #choose 6 from ONGO 6 from TSG and 4 From Fusion to make a batch that distribute the all most all PDBs\n",
    "                    #Fusion_chk=True\n",
    "                    index_ongo,index_tsg,index_fusion,index_ongo_fusion,index_tsg_fusion=self.adding(True,index_ongo,index_tsg,index_fusion,index_ongo_fusion,index_tsg_fusion)\n",
    "                    index_ongo,index_tsg,index_fusion,index_ongo_fusion,index_tsg_fusion=self.adding(False,index_ongo,index_tsg,index_fusion,index_ongo_fusion,index_tsg_fusion,OG_IN=False)\n",
    "                    index_ongo,index_tsg,index_fusion,index_ongo_fusion,index_tsg_fusion=self.adding(True,index_ongo,index_tsg,index_fusion,index_ongo_fusion,index_tsg_fusion)\n",
    "                    index_ongo,index_tsg,index_fusion,index_ongo_fusion,index_tsg_fusion=self.adding(False,index_ongo,index_tsg,index_fusion,index_ongo_fusion,index_tsg_fusion)\n",
    "                    index_ongo,index_tsg,index_fusion,index_ongo_fusion,index_tsg_fusion=self.adding(True,index_ongo,index_tsg,index_fusion,index_ongo_fusion,index_tsg_fusion)\n",
    "                    index_ongo,index_tsg,index_fusion,index_ongo_fusion,index_tsg_fusion=self.adding(True,index_ongo,index_tsg,index_fusion,index_ongo_fusion,index_tsg_fusion)\n",
    "                    index_ongo,index_tsg,index_fusion,index_ongo_fusion,index_tsg_fusion=self.adding(False,index_ongo,index_tsg,index_fusion,index_ongo_fusion,index_tsg_fusion,OG_IN=False)\n",
    "\n",
    "                if (self.iter_train)%16>0: \n",
    "                    index_ongo,index_tsg,index_fusion,index_ongo_fusion,index_tsg_fusion=self.adding(True,index_ongo,index_tsg,index_fusion,index_ongo_fusion,index_tsg_fusion,OG_IN=False)\n",
    "                    index_ongo,index_tsg,index_fusion,index_ongo_fusion,index_tsg_fusion=self.adding(False,index_ongo,index_tsg,index_fusion,index_ongo_fusion,index_tsg_fusion)\n",
    "                    index_ongo,index_tsg,index_fusion,index_ongo_fusion,index_tsg_fusion=self.adding(True,index_ongo,index_tsg,index_fusion,index_ongo_fusion,index_tsg_fusion,OG_IN=False)\n",
    "                    index_ongo,index_tsg,index_fusion,index_ongo_fusion,index_tsg_fusion=self.adding(False,index_ongo,index_tsg,index_fusion,index_ongo_fusion,index_tsg_fusion)\n",
    "                    index_ongo,index_tsg,index_fusion,index_ongo_fusion,index_tsg_fusion=self.adding(True,index_ongo,index_tsg,index_fusion,index_ongo_fusion,index_tsg_fusion)\n",
    "                    index_ongo,index_tsg,index_fusion,index_ongo_fusion,index_tsg_fusion=self.adding(True,index_ongo,index_tsg,index_fusion,index_ongo_fusion,index_tsg_fusion)\n",
    "                    index_ongo,index_tsg,index_fusion,index_ongo_fusion,index_tsg_fusion=self.adding(False,index_ongo,index_tsg,index_fusion,index_ongo_fusion,index_tsg_fusion)\n",
    "\n",
    "                os.chdir('/')\n",
    "                os.chdir(self.main_directory)               \n",
    "                if self.validation_split>0:\n",
    "                    pickle.dump(self.list_of_ids, open(\"V_list_of_ids.p\", \"wb\"))  \n",
    "                    pickle.dump(self.list_of_id_classes, open(\"V_list_of_id_classes.p\", \"wb\")) \n",
    "                else:\n",
    "                    pickle.dump(self.list_of_ids, open(\"list_of_ids.p\", \"wb\"))  \n",
    "                    pickle.dump(self.list_of_id_classes, open(\"list_of_id_classes.p\", \"wb\")) \n",
    "            else:\n",
    "                self.overalp_included_but_not_balance_must()\n",
    "    \n",
    "    def not_balance_must_and_Fully_clean(self):\n",
    "        '''\n",
    "        This function craete the IDS and corresponding lable by randomly shuffling the data\n",
    "        \n",
    "        Not balance data only shuffled once; thus the data order doesn't changed\n",
    "       \n",
    "        Since the TSG class higer number of PDBs remove some of the PDBs from TSG to keep the dataset unique in all sessions\n",
    "        (using the same data as train and validation)\n",
    "        ''' \n",
    "       \n",
    "        raise(\"This function has to be reconstructed to avoid  remove PDBs randomly from the TSG group\")\n",
    "        \n",
    "        \n",
    "        clean_list_of_ids_temp=self.OG_train_PDBs+deepcopy(self.TSG_train_PDBs)+self.Fusion_train_PDBs#since removing some PDBs from TSG PDBs\n",
    "        clean_list_of_ids=[]\n",
    "        removed_list_tsg=[]\n",
    "        if (len(clean_list_of_ids_temp)%self.batch_size)>0:\n",
    "            chk=list(range(len(self.OG_train_PDBs),len(self.OG_train_PDBs)+len(self.TSG_train_PDBs)))\n",
    "            random.shuffle(chk)#shuffle to remove PDBs randomly from the TSG group\n",
    "            for i in range(0,len(clean_list_of_ids_temp)%self.batch_size):\n",
    "                removed_list_tsg.append(clean_list_of_ids_temp.pop(chk[i]))\n",
    "  \n",
    "        if self.shuffle:\n",
    "            random.shuffle(clean_list_of_ids_temp)\n",
    "            \n",
    "        for i in range(0,(self.batch_size*self.iter_train)//len(clean_list_of_ids_temp)):\n",
    "            clean_list_of_ids=clean_list_of_ids + deepcopy(clean_list_of_ids_temp)\n",
    "\n",
    "        clean_list_of_ids = clean_list_of_ids + deepcopy(clean_list_of_ids_temp[0:(self.batch_size*self.iter_train)%len(clean_list_of_ids_temp)])           \n",
    "        self.clean_list_of_ids = deepcopy(clean_list_of_ids)\n",
    "        \n",
    "        print('')\n",
    "        print(\"Not balanced Clean list_of_ids lenth: \",len(self.clean_list_of_ids))\n",
    "        print('')\n",
    " \n",
    "        os.chdir('/')\n",
    "        os.chdir(self.main_directory)\n",
    "        if self.validation_split>0:\n",
    "            pickle.dump(removed_list_tsg, open(\"V_removed_list_tsg_unbalanced_clean.p\", \"wb\"))  \n",
    "            pickle.dump(self.clean_list_of_ids, open(\"V_clean_list_of_ids.p\", \"wb\"))  \n",
    "            pickle.dump(self.clean_list_of_id_classes, open(\"V_clean_list_of_id_classes.p\", \"wb\")) \n",
    "        else:\n",
    "            pickle.dump(removed_list_tsg, open(\"removed_list_tsg_unbalanced_clean.p\", \"wb\"))  \n",
    "            pickle.dump(self.clean_list_of_ids, open(\"clean_list_of_ids.p\", \"wb\"))  \n",
    "            pickle.dump(self.clean_list_of_id_classes, open(\"clean_list_of_id_classes.p\", \"wb\"))   \n",
    "            \n",
    "    def overalp_included_but_not_balance_must(self):\n",
    "        '''\n",
    "        This function craete the IDS and corresponding lable by randomly shuffling the data\n",
    "        '''\n",
    "        raise(\"This function has to be reconstructed to avoid  remove PDBs randomly from the TSG group\")\n",
    "\n",
    "        \n",
    "        import random \n",
    "        list_of_ids_temp=self.OG_train_PDBs+self.TSG_train_PDBs+self.Fusion_train_PDBs+self.overlapped_PDBs_ONGO+self.overlapped_PDBs_TSG\n",
    "        \n",
    "        removed_list_tsg=[]\n",
    "        if (list_of_ids_temp%self.batch_size)>0:\n",
    "            chk=list(range(len(self.OG_train_PDBs),len(self.OG_train_PDBs)+len(self.TSG_train_PDBs)))\n",
    "            random.shuffle(chk)\n",
    "            for i in range(0,list_of_ids_temp%self.batch_size):\n",
    "                removed_list_tsg.append(list_of_ids_temp.pop(chk[i]))#the removal of TSG PDBs\n",
    "        \n",
    "        list_of_ids_classes_temp=[]\n",
    "        for i in range(0,len(self.OG_train_PDBs)):\n",
    "            list_of_ids_classes_temp.append(np.array([1,0,0],dtype=float))\n",
    "        #the removal of TSG PDBs\n",
    "        for i in range(0,(len(self.TSG_train_PDBs)-len(removed_list_tsg))):\n",
    "            list_of_ids_classes_temp.append(np.array([0,1,0],dtype=float))       \n",
    "        for i in range(0,len(self.Fusion_train_PDBs)):\n",
    "            list_of_ids_classes_temp.append(np.array([0,0,1],dtype=float))   \n",
    "        for i in range(0,len(self.overlapped_PDBs_ONGO)):\n",
    "            list_of_ids_classes_temp.append(np.array([0.5,0,0.5]))   \n",
    "        for i in range(0,len(self.overlapped_PDBs_TSG)):\n",
    "            list_of_ids_classes_temp.append(np.array([0,0.5,0.5]))   \n",
    "            \n",
    "        list_of_ids=[]\n",
    "        list_of_id_classes=[]\n",
    "        \n",
    "        if self.shuffle:\n",
    "            mapIndexPosition = list(zip(list_of_ids_temp, list_of_ids_classes_temp))\n",
    "            random.shuffle(mapIndexPosition)\n",
    "            list_of_ids_temp, list_of_ids_classes_temp = zip(*mapIndexPosition)   \n",
    "\n",
    "        for i in range(0,self.iter_train//len(list_of_ids_temp)):\n",
    "            list_of_ids = list_of_ids + deepcopy(list_of_ids_temp)\n",
    "            list_of_id_classes = list_of_id_classes + deepcopy(list_of_ids_classes_temp)\n",
    "        \n",
    "        list_of_ids = list_of_ids + deepcopy(list_of_ids_temp[0:self.iter_train%len(list_of_ids_classes_temp)])           \n",
    "        list_of_id_classes = list_of_id_classes + deepcopy(list_of_ids_classes_temp[0:self.iter_train%len(list_of_ids_classes_temp)])\n",
    "        \n",
    "        self.list_of_ids = deepcopy(list_of_ids)\n",
    "        self.list_of_id_classes=deepcopy(list_of_id_classes)\n",
    "    \n",
    "        os.chdir('/')\n",
    "        os.chdir(self.main_directory)\n",
    "        if self.validation_split>0:\n",
    "            pickle.dump(self.list_of_ids, open(\"V_list_of_ids.p\", \"wb\"))  \n",
    "            pickle.dump(self.list_of_id_classes, open(\"V_list_of_id_classes.p\", \"wb\"))\n",
    "            pickle.dump(removed_list_tsg, open(\"V_removed_list_tsg_unbalanced.p\", \"wb\"))  \n",
    "        else:\n",
    "            pickle.dump(self.list_of_ids, open(\"list_of_ids.p\", \"wb\"))  \n",
    "            pickle.dump(self.list_of_id_classes, open(\"list_of_id_classes.p\", \"wb\"))\n",
    "            pickle.dump(removed_list_tsg, open(\"removed_list_tsg_unbalanced.p\", \"wb\"))  \n",
    "\n",
    "    def adding(self,Fusion_chk,index_ongo,index_tsg,index_fusion,index_ongo_fusion,index_tsg_fusion,OG_IN=True):\n",
    "        '''\n",
    "        This function add the elements of PDBs from the stack\n",
    "        Fusion_chk: True;means add the Fusion PDB in the list\n",
    "        \n",
    "        This check the Fusion class if it's empty only move on to the overlapped PDBs\n",
    "        Where the weightage is splited equally\n",
    "        '''\n",
    "        list_of_ids=self.list_of_ids\n",
    "        list_of_id_classes=self.list_of_id_classes\n",
    "        # adding the ONGO elements\n",
    "        if OG_IN:\n",
    "            if len(index_ongo)>0: \n",
    "                list_of_ids.append(self.OG_train_PDBs[index_ongo.pop()])\n",
    "                list_of_id_classes.append(np.array([1,0,0],dtype=float))\n",
    "            else:\n",
    "                print(\"ONGO idexes reset\")\n",
    "                index_ongo = self.index_stack_chk(index_ongo_chk=True,index_tsg_chk=False,index_fusion_chk=False)\n",
    "                list_of_ids.append(self.OG_train_PDBs[index_ongo.pop()])\n",
    "                list_of_id_classes.append(np.array([1,0,0],dtype=float))\n",
    "            \n",
    "        if Fusion_chk:\n",
    "            # adding the Fusion elements\n",
    "            # print(\"Fusion element added\")\n",
    "            if len(index_fusion)>0:\n",
    "                list_of_ids.append(self.Fusion_train_PDBs[index_fusion.pop()])\n",
    "                list_of_id_classes.append(np.array([0,0,1],dtype=float))\n",
    "                overlap_chk=False\n",
    "            else:\n",
    "                overlap_chk=True\n",
    "        # adding the TSG elements\n",
    "        if len(index_tsg)>0:\n",
    "            list_of_ids.append(self.TSG_train_PDBs[index_tsg.pop()])\n",
    "            list_of_id_classes.append(np.array([0,1,0],dtype=float))\n",
    "        else:\n",
    "            print(\"TSG idexes reset\")\n",
    "            index_tsg = self.index_stack_chk(index_ongo_chk=False,index_tsg_chk=True,index_fusion_chk=False)\n",
    "            list_of_ids.append(self.TSG_train_PDBs[index_tsg.pop()])\n",
    "            list_of_id_classes.append(np.array([0,1,0],dtype=float))\n",
    "        \n",
    "\n",
    "        if Fusion_chk and overlap_chk:\n",
    "            if not len(index_ongo_fusion)>0:\n",
    "                index_ongo_fusion = self.index_stack_overlapped_chk(True,False)\n",
    "                if not len(index_tsg_fusion)>0:\n",
    "                    print(\"Since both overlapped done thus Fusion idexes are reseten\")\n",
    "                    index_fusion = self.index_stack_chk(index_ongo_chk=False,index_tsg_chk=False,index_fusion_chk=True)\n",
    "                    index_tsg_fusion = self.index_stack_overlapped_chk(False,True)\n",
    "                else:\n",
    "                    list_of_ids.append(self.overlapped_PDBs_TSG[index_tsg_fusion.pop()])\n",
    "                    list_of_id_classes.append(np.array([0,0.5,0.5],dtype=float))\n",
    "            else:\n",
    "                list_of_ids.append(self.overlapped_PDBs_ONGO[index_ongo_fusion.pop()])\n",
    "                list_of_id_classes.append(np.array([0.5,0,0.5],dtype=float))\n",
    "                            \n",
    "        \n",
    "        self.list_of_ids = list_of_ids\n",
    "        self.list_of_id_classes = list_of_id_classes\n",
    "       \n",
    "        return index_ongo,index_tsg,index_fusion,index_ongo_fusion,index_tsg_fusion\n",
    "\n",
    "\n",
    "\n",
    "    def adding_fully_clean(self,Fusion_chk,index_ongo,index_tsg,index_fusion,OG_IN=True):\n",
    "        '''\n",
    "        This function add the elements of PDBs from the stack\n",
    "        Fusion_chk: True;means add the Fusion PDB in the list\n",
    "        '''\n",
    "        clean_list_of_ids=self.clean_list_of_ids\n",
    "        clean_list_of_id_classes=self.clean_list_of_id_classes\n",
    "        # adding the ONGO elements\n",
    "        if OG_IN:\n",
    "            if len(index_ongo)>0: \n",
    "                clean_list_of_ids.append(self.OG_train_PDBs[index_ongo.pop()])\n",
    "                clean_list_of_id_classes.append(0)\n",
    "            else:\n",
    "                print(\"ONGO idexes reset\")\n",
    "                index_ongo = self.index_stack_chk(index_ongo_chk=True,index_tsg_chk=False,index_fusion_chk=False)\n",
    "                clean_list_of_ids.append(self.OG_train_PDBs[index_ongo.pop()])\n",
    "                clean_list_of_id_classes.append(0)\n",
    "        \n",
    "        if Fusion_chk:\n",
    "            # adding the Fusion elements\n",
    "            # print(\"Fusion element added\")\n",
    "            if len(index_fusion)>0:\n",
    "                clean_list_of_ids.append(self.Fusion_train_PDBs[index_fusion.pop()])\n",
    "                clean_list_of_id_classes.append(2)\n",
    "            else:\n",
    "                print(\"Fusion idexes reset\")\n",
    "                index_fusion = self.index_stack_chk(index_ongo_chk=False,index_tsg_chk=False,index_fusion_chk=True)\n",
    "                clean_list_of_ids.append(self.Fusion_train_PDBs[index_fusion.pop()])\n",
    "                clean_list_of_id_classes.append(2)\n",
    "        # adding the TSG elements\n",
    "        if len(index_tsg)>0:\n",
    "            clean_list_of_ids.append(self.TSG_train_PDBs[index_tsg.pop()])\n",
    "            clean_list_of_id_classes.append(1)\n",
    "        else:\n",
    "            print(\"TSG idexes reset\")\n",
    "            index_tsg = self.index_stack_chk(index_ongo_chk=False,index_tsg_chk=True,index_fusion_chk=False)\n",
    "            clean_list_of_ids.append(self.TSG_train_PDBs[index_tsg.pop()])\n",
    "            clean_list_of_id_classes.append(1)\n",
    "        \n",
    "        self.clean_list_of_ids = clean_list_of_ids\n",
    "        self.clean_list_of_id_classes = clean_list_of_id_classes\n",
    "        return index_ongo,index_tsg,index_fusion\n",
    "\n",
    "    def index_stack_overlapped_chk(self,index_ongo_fusion_chk,index_tsg_fusion_chk):\n",
    "        '''\n",
    "        This function is used for mainintaining balanced data set \n",
    "        thus it shuffled each time, while creating new data\n",
    "        \n",
    "        check the stack if any of them is empty then creat a new stack\n",
    "        This way always change the train data when it cycles\n",
    "        \n",
    "        index_ongo_fusion_chk,index_tsg_fusion_chk if one of them is true then take the stack shuffle and return it\n",
    "        return the newly stacks index_ongo_fusion index_tsg or index_fusion\n",
    "        '''\n",
    "        shuffle = self.shuffle\n",
    "        if index_ongo_fusion_chk:\n",
    "            print(\"ONGO Fusion overlapped idexes reset\")\n",
    "            index_ongo_fusion= np.arange(len(self.overlapped_PDBs_ONGO))\n",
    "            if shuffle:\n",
    "                np.random.shuffle(index_ongo_fusion)\n",
    "            return list(index_ongo_fusion)\n",
    "        if index_tsg_fusion_chk:\n",
    "            print(\"TSG Fusion overlapped idexes reset\")\n",
    "            index_tsg_fusion = np.arange(len(self.overlapped_PDBs_TSG))\n",
    "            if  shuffle:\n",
    "                np.random.shuffle(index_tsg_fusion)\n",
    "            return list(index_tsg_fusion)\n",
    "\n",
    "\n",
    "    def index_stack_chk(self,index_ongo_chk,index_tsg_chk,index_fusion_chk):\n",
    "        '''\n",
    "        This function is used for mainintaining balanced data set \n",
    "        thus it shuffled each time, while creating new data\n",
    "        \n",
    "        This function check the stack if any of them is empty then creat a new stack\n",
    "        This way always change the train data when it cycles\n",
    "        \n",
    "        index_ongo_chk,index_tsg_chk,index_fusion_chk=If one of them is true then take the stack shuffle and return it\n",
    "        return the newly stacks index_ongo or index_tsg or index_fusion\n",
    "        '''\n",
    "        shuffle =self.shuffle\n",
    "        if index_ongo_chk:\n",
    "            print(\"ONGO idexes reset\")\n",
    "            index_ongo= np.arange(len(self.OG_train_PDBs))\n",
    "            if shuffle:\n",
    "                np.random.shuffle(index_ongo)\n",
    "            return list(index_ongo)\n",
    "        if index_tsg_chk:\n",
    "            print(\"TSG idexes reset\")\n",
    "            index_tsg = np.arange(len(self.TSG_train_PDBs))\n",
    "            if  shuffle:\n",
    "                np.random.shuffle(index_tsg)\n",
    "            return list(index_tsg)\n",
    "        if index_fusion_chk:\n",
    "            print(\"Fusion idexes reset\")\n",
    "            index_fusion = np.arange(len(self.Fusion_train_PDBs))\n",
    "            if  shuffle:\n",
    "                np.random.shuffle(index_fusion)\n",
    "            return list(index_fusion)\n",
    "\n",
    "\n",
    "    def __data_generation_full_clean(self, list_IDs_temp,list_IDs_class_temp):\n",
    "        'Generates data containing batch_size samples only this part iws editted' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        x1= np.empty((self.batch_size,*self.dim, self.n_channels))\n",
    "        x2= np.empty((self.batch_size,*self.dim, self.n_channels))\n",
    "        x3= np.empty((self.batch_size,*self.dim, self.n_channels))\n",
    "        if self.Full_clean:\n",
    "            y = np.empty((self.batch_size), dtype=int)\n",
    "        else:\n",
    "            raise (\"Miss use of Function __data_generation_full_clean\")\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            # Store sample\n",
    "            data_temp = np.load(ID)\n",
    "            x1[i,] = deepcopy(data_temp[0,:,:,:])\n",
    "            x2[i,] = deepcopy(data_temp[1,:,:,:])\n",
    "            x3[i,] = deepcopy(data_temp[2,:,:,:])\n",
    "            y[i] = self.labels[ID]\n",
    "            if  self.labels[ID]>2:\n",
    "                print('Wrongly placed id: ',ID)\n",
    "        if len(list_IDs_temp)>self.batch_size:\n",
    "            print('-----exceed batch size')\n",
    "        X=[x1,x2,x3]\n",
    "        return X,keras.utils.to_categorical(y, num_classes=self.n_classes)\n",
    "  \n",
    "    def __data_generation(self, list_IDs_temp,list_IDs_class_temp):\n",
    "      'Generates data containing batch_size samples only this part iws editted' # X : (n_samples, *dim, n_channels)\n",
    "      # Initialization\n",
    "      x1= np.empty((self.batch_size,*self.dim, self.n_channels))\n",
    "      x2= np.empty((self.batch_size,*self.dim, self.n_channels))\n",
    "      x3= np.empty((self.batch_size,*self.dim, self.n_channels))\n",
    "      if not self.Full_clean:\n",
    "          y = np.empty((self.batch_size,self.n_classes), dtype=float)\n",
    "      else:\n",
    "          raise (\"Miss use of Function __data_generation\")\n",
    "      # Generate data\n",
    "      for i, ID in enumerate(list_IDs_temp):\n",
    "          # Store sample\n",
    "          data_temp = np.load(ID)\n",
    "          x1[i,] = deepcopy(data_temp[0,:,:,:])\n",
    "          x2[i,] = deepcopy(data_temp[1,:,:,:])\n",
    "          x3[i,] = deepcopy(data_temp[2,:,:,:])\n",
    "          y[i,:] = deepcopy(list_IDs_class_temp[i])\n",
    "    \n",
    "      X=[x1,x2,x3]\n",
    "      return X,y\n",
    "          \n",
    "  \n",
    "    def __getitem__(self, index):\n",
    "      'Generate one batch of data'\n",
    "      if self.Full_clean:\n",
    "          # Generate indexes of the batch\n",
    "          if len(self.clean_list_of_ids)>(index+1)*self.batch_size:\n",
    "              indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "    #          print('here01')\n",
    "          else:\n",
    "              print(\"wqdddddddddddddddddddddddddddddddddddddddddddddddddddddddddddd\")\n",
    "              indexes = self.indexes[index*self.batch_size-(self.batch_size-(len(self.clean_list_of_ids)-index*self.batch_size)):len(self.clean_list_of_ids)]\n",
    "    #          print(len(indexes))\n",
    "              # Find list of IDs\n",
    "          list_IDs_temp = [self.clean_list_of_ids[k] for k in indexes]\n",
    "          list_IDs_class_temp= [self.labels[ID] for ID in list_IDs_temp]\n",
    "          \n",
    "          # Generate data\n",
    "\n",
    "          if index==0:\n",
    "              print('len(self.clean_list_of_ids: ', len(self.clean_list_of_ids))\n",
    "              print('(index+1)*self.batch_size: ',(index+1)*self.batch_size)\n",
    "              print('len(self.indexes): ',len(self.indexes))\n",
    "              print('len(indexes): ',len(indexes))\n",
    "              # print('__getitem__: ',index)\n",
    "              # print(list_IDs_temp)\n",
    "              print(\"\")\n",
    "              print(\"\")\n",
    "\n",
    "          X, y = self.__data_generation_full_clean(list_IDs_temp,list_IDs_class_temp)\n",
    "      else:\n",
    "          if len(self.list_of_ids)>(index+1)*self.batch_size:\n",
    "              indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "    #          print('here01')\n",
    "          else:\n",
    "              indexes = self.indexes[index*self.batch_size-(32-(len(self.list_of_ids)-index*self.batch_size)):len(self.list_of_ids)]\n",
    "    #          print(len(indexes))\n",
    "              # Find list of IDs\n",
    "          list_IDs_temp = [self.list_of_ids[k] for k in indexes]\n",
    "          list_IDs_class_temp= [self.list_of_id_classes[k] for k in indexes]\n",
    "\n",
    "          X, y = self.__data_generation(list_IDs_temp,list_IDs_class_temp)\n",
    "      return X, y\n",
    "\n",
    "\n",
    "class Data_test:\n",
    "    'Generates 2D projected data for Keras'\n",
    "    def __init__(self, list_IDs, labels, batch_size=32, dim = (200,200), n_channels=17 ,n_classes=3):\n",
    "        'Initialization'\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.dim = dim\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "\n",
    "    def getitem_test(self, index):\n",
    "      'Generate one batch of data'\n",
    "      # Generate indexes of the batch\n",
    "      indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "    \n",
    "      # Find list of IDs\n",
    "      list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "    \n",
    "      # Generate data\n",
    "      X, y = self.data_generation_test(list_IDs_temp)\n",
    "      return X, y      \n",
    "    \n",
    "    def data_generation_test(self, list_IDs_temp):\n",
    "      'Generates data containing batch_size samples only this part iws editted' # X : (n_samples, *dim, n_channels)\n",
    "       # Initialization\n",
    "      if len(list_IDs_temp)<self.batch_size:\n",
    "#              print(\"Intialised\")\n",
    "#              print(\"\")\n",
    "#              print(\"Intialised\")\n",
    "              x1= np.empty((len(list_IDs_temp),*self.dim, self.n_channels))\n",
    "              x2= np.empty((len(list_IDs_temp),*self.dim, self.n_channels))\n",
    "              x3= np.empty((len(list_IDs_temp),*self.dim, self.n_channels))\n",
    "              y = np.empty((len(list_IDs_temp)), dtype=int)\n",
    "      else:       \n",
    "          x1= np.empty((self.batch_size,*self.dim, self.n_channels))\n",
    "          x2= np.empty((self.batch_size,*self.dim, self.n_channels))\n",
    "          x3= np.empty((self.batch_size,*self.dim, self.n_channels))\n",
    "          y = np.empty((self.batch_size), dtype=int)\n",
    "      X=[]\n",
    "      # Generate data\n",
    "      for i, ID in enumerate(list_IDs_temp):\n",
    "          # Store sample\n",
    "          data_temp = np.load(ID)\n",
    "          x1[i,] = deepcopy(data_temp[0,:,:,:])\n",
    "          x2[i,] = deepcopy(data_temp[1,:,:,:])\n",
    "          x3[i,] = deepcopy(data_temp[2,:,:,:])\n",
    "\n",
    "          # Store class\n",
    "          y[i] = self.labels[ID]\n",
    "      X=[x1,x2,x3]\n",
    "      return X, keras.utils.to_categorical(y, num_classes=self.n_classes)\n",
    "  \n",
    "    def __len__(self):\n",
    "      'Denotes the number of batches per epoch'\n",
    "      return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "    \n",
    "    def __getitem_list_id__(self, index):\n",
    "      'Generate one batch of data'\n",
    "      # Generate indexes of the batch\n",
    "      if len(self.list_IDs)>(index+1)*self.batch_size:\n",
    "          indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "#          print('here01')\n",
    "      else:\n",
    "          indexes = self.indexes[index*self.batch_size-(32-(len(self.list_IDs)-index*self.batch_size)):len(self.list_IDs)]\n",
    "#          print(len(indexes))\n",
    "      # Find list of IDs\n",
    "      list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "      return list_IDs_temp\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300285c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_train_class_dat_valid_overall_vin_4 import ovear_all_training\n",
    "'''Models'''\n",
    "\n",
    "from Deep_model_architechtures import model_par_inception_residual_mentor_vin_1\n",
    "from Deep_model_architechtures import model_inception_residual_mentor_vin_1\n",
    "'''Not all the inception is done here'''\n",
    "from copy import deepcopy\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "\n",
    "'''To save the results'''\n",
    "overall_history_log_list=[]\n",
    "highest_test_accuracy_list=[]\n",
    "model_name_list=[]\n",
    "\n",
    "def chk_to_save(overall_history_log_list,highest_test_accuracy_list,model_name_list,highest_test_accuracy,overall_history_log,not_satisfied,model_name):\n",
    "    '''To save the results'''\n",
    "\n",
    "    highest_test_accuracy_list.append(deepcopy(highest_test_accuracy))\n",
    "    overall_history_log_list.append(deepcopy(overall_history_log))\n",
    "    model_name_list.append(deepcopy(model_name))\n",
    "    if not_satisfied:\n",
    "        return overall_history_log_list,highest_test_accuracy_list,model_name_list\n",
    "    else:\n",
    "        '''To save the results for future reference'''\n",
    "        os.chdir('/')\n",
    "        os.chdir('scratch/optimaly_tilted/Train')\n",
    "        pickle.dump(overall_history_log_list, open(\"overall_history_log.p\", \"wb\"))  \n",
    "        pickle.dump(highest_test_accuracy_list, open(\"highest_test_accuracy_list.p\", \"wb\"))  \n",
    "        pickle.dump(model_name_list, open(\"model_name_list.p\", \"wb\"))  \n",
    "        return overall_history_log_list,highest_test_accuracy_list,model_name_list\n",
    "    \n",
    "    \n",
    "def call_channel(channel_number,activation,number_of_overall_iertations_to_the_model,model_loss,optimizer,Full_clean=True,balance_batch_must=False,validation_split=0.15,tower_min_max_only=False,shuffle=False,SITE_MUST=True):\n",
    "    '''\n",
    "    Default values\n",
    "    \n",
    "    Full_clean=True\n",
    "    \n",
    "    validation_split=0.2\n",
    "        \n",
    "    balance_batch_must=False\n",
    "    tower_min_max_only=False\n",
    "    '''\n",
    "    print('-------------  training channel no: ',channel_number)\n",
    "    print('')\n",
    "    print('-------------  Activation function: ',activation)\n",
    "    print('')\n",
    "\n",
    "    '''To save the results'''\n",
    "    overall_history_log_list=[]\n",
    "    highest_test_accuracy_list=[]\n",
    "    model_name_list=[]\n",
    "    satisfied=False\n",
    "    not_satisfied=True\n",
    "#    model_inc_layer_1_chk=0\n",
    "\n",
    "#    if not_satisfied:\n",
    "#        '''model_chk `1'''\n",
    "#        model_chk=1\n",
    "#        model_train = ovear_all_training(channels=channel_number,number_of_overall_iertations_to_the_model=number_of_overall_iertations_to_the_model,model_loss=model_loss,optimize=optimizer,Full_clean=Full_clean,balance_batch_must=balance_batch_must,validation_split=validation_split,shuffle=shuffle,SITE_MUST=SITE_MUST)\n",
    "#        model_chk_model = model_inception_only_mentor_vin_1(channels=channel_number,tower_min_max_only=tower_min_max_only,activation=activation)\n",
    "#        model,model_name= model_chk_model.model_maker()#d1=80,d2=16\n",
    "#        highest_test_accuracy,overall_history_log,not_satisfied =model_train.model_parameter_train(model,model_name)\n",
    "#        del model\n",
    "#        del model_train\n",
    "#        overall_history_log_list,highest_test_accuracy_list,model_name_list =  chk_to_save(overall_history_log_list,highest_test_accuracy_list,model_name_list,highest_test_accuracy,overall_history_log,not_satisfied,model_name)\n",
    "\n",
    "    if not_satisfied:\n",
    "        '''model_chk `1'''\n",
    "        model_train = ovear_all_training(channels=channel_number,number_of_overall_iertations_to_the_model=number_of_overall_iertations_to_the_model,model_loss=model_loss,optimize=optimizer,Full_clean=Full_clean,balance_batch_must=balance_batch_must,validation_split=validation_split,shuffle=shuffle,SITE_MUST=SITE_MUST)\n",
    "        model_chk_model = model_par_inception_residual_mentor_vin_1(channels=channel_number,tower_min_max_only=tower_min_max_only,activation=activation)\n",
    "        model,model_name= model_chk_model.model_maker()#d1=80,d2=16\n",
    "        highest_test_accuracy,overall_history_log,not_satisfied =model_train.model_parameter_train(model,model_name)\n",
    "        del model\n",
    "        del model_train\n",
    "        overall_history_log_list,highest_test_accuracy_list,model_name_list =  chk_to_save(overall_history_log_list,highest_test_accuracy_list,model_name_list,highest_test_accuracy,overall_history_log,not_satisfied,model_name)\n",
    "\n",
    "    if not_satisfied:\n",
    "        '''model_chk `1'''\n",
    "        model_train = ovear_all_training(channels=channel_number,number_of_overall_iertations_to_the_model=number_of_overall_iertations_to_the_model,model_loss=model_loss,optimize=optimizer,Full_clean=Full_clean,balance_batch_must=balance_batch_must,validation_split=validation_split,shuffle=shuffle,SITE_MUST=SITE_MUST)\n",
    "        model_chk_model = model_inception_residual_mentor_vin_1(channels=channel_number,tower_min_max_only=tower_min_max_only,activation=activation)\n",
    "        model,model_name= model_chk_model.model_maker()#d1=80,d2=16\n",
    "        highest_test_accuracy,overall_history_log,not_satisfied =model_train.model_parameter_train(model,model_name)\n",
    "        del model\n",
    "        del model_train\n",
    "        overall_history_log_list,highest_test_accuracy_list,model_name_list =  chk_to_save(overall_history_log_list,highest_test_accuracy_list,model_name_list,highest_test_accuracy,overall_history_log,not_satisfied,model_name) \n",
    "    '''memory issue run on higher GPU machine'''\n",
    "#    if not_satisfied:\n",
    "#        '''model_chk `1'''\n",
    "#        model_train = ovear_all_training(channels=channel_number,number_of_overall_iertations_to_the_model=number_of_overall_iertations_to_the_model,model_loss=model_loss,optimize=optimizer,Full_clean=Full_clean,balance_batch_must=balance_batch_must,validation_split=validation_split,shuffle=shuffle,SITE_MUST=SITE_MUST)\n",
    "#        model_chk_model = model_par_inception_residual_mentor_vin_1(channels=channel_number,tower_min_max_only=tower_min_max_only,activation=activation)\n",
    "#        model,model_name= model_chk_model.model_maker(f_inc=2,f_d=1)\n",
    "#        highest_test_accuracy,overall_history_log,not_satisfied =model_train.model_parameter_train(model,model_name)\n",
    "#        del model\n",
    "#        del model_train\n",
    "#        overall_history_log_list,highest_test_accuracy_list,model_name_list =  chk_to_save(overall_history_log_list,highest_test_accuracy_list,model_name_list,highest_test_accuracy,overall_history_log,not_satisfied,model_name)\n",
    "#\n",
    "#    if not_satisfied:\n",
    "#        '''model_chk `1'''\n",
    "#        model_train = ovear_all_training(channels=channel_number,number_of_overall_iertations_to_the_model=number_of_overall_iertations_to_the_model,model_loss=model_loss,optimize=optimizer,Full_clean=Full_clean,balance_batch_must=balance_batch_must,validation_split=validation_split,shuffle=shuffle,SITE_MUST=SITE_MUST)\n",
    "#        model_chk_model = model_inception_residual_mentor_vin_1(channels=channel_number,tower_min_max_only=tower_min_max_only,activation=activation)\n",
    "#        model,model_name= model_chk_model.model_maker(f_inc=2,f_d=1)\n",
    "#        highest_test_accuracy,overall_history_log,not_satisfied =model_train.model_parameter_train(model,model_name)\n",
    "#        del model\n",
    "#        del model_train\n",
    "#        overall_history_log_list,highest_test_accuracy_list,model_name_list =  chk_to_save(overall_history_log_list,highest_test_accuracy_list,model_name_list,highest_test_accuracy,overall_history_log,not_satisfied,model_name) \n",
    "#     \n",
    "#    \n",
    "\n",
    "#    if not_satisfied:\n",
    "#        '''model_chk `1'''\n",
    "#        model_chk=1\n",
    "#        model_train = ovear_all_training(channels=channel_number,number_of_overall_iertations_to_the_model=number_of_overall_iertations_to_the_model,model_loss=model_loss,optimize=optimizer,Full_clean=Full_clean,balance_batch_must=balance_batch_must,validation_split=validation_split,shuffle=shuffle,SITE_MUST=SITE_MUST)\n",
    "#        model_chk_model = model_par_inception_only_mentor_vin_1(channels=channel_number,tower_min_max_only=tower_min_max_only,activation=activation)\n",
    "#        model,model_name= model_chk_model.model_maker()#d1=80,d2=16\n",
    "#        highest_test_accuracy,overall_history_log,not_satisfied =model_train.model_parameter_train(model,model_name)\n",
    "#        del model\n",
    "#        del model_train\n",
    "#        overall_history_log_list,highest_test_accuracy_list,model_name_list =  chk_to_save(overall_history_log_list,highest_test_accuracy_list,model_name_list,highest_test_accuracy,overall_history_log,not_satisfied,model_name)\n",
    "  \n",
    "  \n",
    "    if  not_satisfied:\n",
    "        print(\"Try with different dataset or model\")\n",
    "        return False\n",
    "\n",
    "    else:\n",
    "        print(channel_number, ' satisfied some where chk')\n",
    "        return True\n",
    "\n",
    "#first find which dataset works best with the model\n",
    "Full_clean=False\n",
    "balance_batch_must=True\n",
    "validation_split=0.2\n",
    "tower_min_max_only=True\n",
    "number_of_overall_iertations_to_the_model=2\n",
    "\n",
    "activation =  'swish'\n",
    "\n",
    "#chk_bal_tow_2_swish =call_channel(21,activation,number_of_overall_iertations_to_the_model,model_loss='msle',optimizer='RMSprop',Full_clean=Full_clean,balance_batch_must=True,tower_min_max_only=True)\n",
    "#if not chk_bal_tow_2_swish:\n",
    "chk_bal_2_swish = call_channel(20,activation,number_of_overall_iertations_to_the_model,model_loss='msle',optimizer='RMSprop',Full_clean=Full_clean,balance_batch_must=True,tower_min_max_only=False,SITE_MUST=False)\n",
    "if not chk_bal_2_swish:\n",
    "    print(\" -------------------- Running on SITE must new prop with threshold changed ----------------- \")\n",
    "    chk_bal_2_swish_SITE_MUST =call_channel(21,activation,number_of_overall_iertations_to_the_model,model_loss='msle',optimizer='RMSprop',Full_clean=Full_clean,balance_batch_must=True,tower_min_max_only=False,SITE_MUST=True)\n",
    "    if not chk_bal_2_swish_SITE_MUST:\n",
    "        print(\" --------------------- Running on SITE must with old threshold  ---------------\")\n",
    "        chk_bal_2_swish_SITE_MUST_old =call_channel(17,activation,number_of_overall_iertations_to_the_model,model_loss='msle',optimizer='RMSprop',Full_clean=Full_clean,balance_batch_must=True,tower_min_max_only=False,SITE_MUST=True,fusion_test_chk=0.2)\n",
    "\n",
    "if chk_bal_2_swish or chk_bal_2_swish_SITE_MUST:\n",
    "    chk_bal_2_swish_with_fusion = call_channel(20,activation,number_of_overall_iertations_to_the_model,model_loss='msle',optimizer='RMSprop',Full_clean=Full_clean,balance_batch_must=True,tower_min_max_only=False,SITE_MUST=False,fusion_test_chk=0.2)\n",
    "    if not chk_bal_2_swish_with_fusion:\n",
    "        chk_bal_2_swish_SITE_MUST_with_fusion =call_channel(21,activation,number_of_overall_iertations_to_the_model,model_loss='msle',optimizer='RMSprop',Full_clean=Full_clean,balance_batch_must=True,tower_min_max_only=False,SITE_MUST=True,fusion_test_chk=0.2)\n",
    "\n",
    "\n",
    "#    if not chk_bal_2_swish:\n",
    "#        activation =  'relu'\n",
    "#        chk_bal_tow_2_relu =call_channel(21,activation,number_of_overall_iertations_to_the_model,model_loss='msle',optimizer='RMSprop',Full_clean=Full_clean,balance_batch_must=True,tower_min_max_only=True)\n",
    "#        if not chk_bal_tow_2_relu:\n",
    "#            chk_bal_2_relu =call_channel(21,activation,number_of_overall_iertations_to_the_model,model_loss='msle',optimizer='RMSprop',Full_clean=Full_clean,balance_batch_must=True,tower_min_max_only=False)\n",
    "#    \n",
    "#            activation =  'swish'                   \n",
    "#            print(\"\")\n",
    "#            if not chk_bal_2_relu:\n",
    "#                print('')\n",
    "#                print('')\n",
    "#                print('<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<----  Shuffle activated  here-on   number_of_overall_iertations_to_the_model: 2 ---->>>>>>>>>>>>>>>>>>>>>>>>>>>>>')\n",
    "#                print('')\n",
    "#                number_of_overall_iertations_to_the_model=1\n",
    "#                print('')\n",
    "#                chk_bal_tow_2_shuf_1_swish =call_channel(21,activation,number_of_overall_iertations_to_the_model,model_loss='msle',optimizer='RMSprop',Full_clean=Full_clean,balance_batch_must=True,tower_min_max_only=True,shuffle=True)\n",
    "#                if not chk_bal_tow_2_shuf_1_swish:   \n",
    "#                    chk_bal_2_shuf_1_swish =call_channel(21,activation,number_of_overall_iertations_to_the_model,model_loss='msle',optimizer='RMSprop',Full_clean=Full_clean,balance_batch_must=True,tower_min_max_only=False,shuffle=True)\n",
    "#                    if not chk_bal_2_shuf_1_swish:\n",
    "#                        print('')\n",
    "#                        print('')\n",
    "#                        print('<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<----  Shuffled & Not balanced number_of_overall_iertations_to_the_model: 1 ---->>>>>>>>>>>>>>>>>>>>>>>>>>>>>')\n",
    "#                        print('')\n",
    "#                        number_of_overall_iertations_to_the_model=1\n",
    "#                        print('')\n",
    "#                        chk_bal_tow_2_shuf_Not_bal1_swish =call_channel(21,activation,number_of_overall_iertations_to_the_model,model_loss='msle',optimizer='RMSprop',Full_clean=Full_clean,balance_batch_must=True,tower_min_max_only=True,shuffle=True)\n",
    "#                        if not chk_bal_tow_2_shuf_Not_bal1_swish:\n",
    "#                            chk_bal_2_shuf_Not_bal1_swish =call_channel(21,activation,number_of_overall_iertations_to_the_model,model_loss='msle',optimizer='RMSprop',Full_clean=Full_clean,balance_batch_must=True,tower_min_max_only=False,shuffle=True)\n",
    "#                            if not chk_bal_2_shuf_Not_bal1_swish:\n",
    "#                                print('')\n",
    "#                                print('')\n",
    "#                                print('<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<----  Shuffle activated  here-on   number_of_overall_iertations_to_the_model: 2 ---->>>>>>>>>>>>>>>>>>>>>>>>>>>>>')\n",
    "#                                print('')\n",
    "#                                number_of_overall_iertations_to_the_model=1\n",
    "#                                print('')\n",
    "#                                chk_bal_tow_2_shuf_1_relu =call_channel(21,activation,number_of_overall_iertations_to_the_model,model_loss='msle',optimizer='RMSprop',Full_clean=Full_clean,balance_batch_must=True,tower_min_max_only=True,shuffle=True)\n",
    "#                                if not chk_bal_tow_2_shuf_1_relu:   \n",
    "#                                    chk_bal_2_shuf_1_relu =call_channel(21,activation,number_of_overall_iertations_to_the_model,model_loss='msle',optimizer='RMSprop',Full_clean=Full_clean,balance_batch_must=True,tower_min_max_only=False,shuffle=True)\n",
    "#                                    if not chk_bal_2_shuf_1_relu:\n",
    "#                                        print('')\n",
    "#                                        print('')\n",
    "#                                        print('<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<----  Shuffled & Not balanced number_of_overall_iertations_to_the_model: 1 ---->>>>>>>>>>>>>>>>>>>>>>>>>>>>>')\n",
    "#                                        print('')\n",
    "#                                        number_of_overall_iertations_to_the_model=1\n",
    "#                                        print('')\n",
    "#                                        chk_bal_tow_2_shuf_Not_bal1_relu =call_channel(21,activation,number_of_overall_iertations_to_the_model,model_loss='msle',optimizer='RMSprop',Full_clean=Full_clean,balance_batch_must=True,tower_min_max_only=True,shuffle=True)\n",
    "#                                        if not chk_bal_tow_2_shuf_Not_bal1_relu:\n",
    "#                                            chk_bal_2_shuf_Not_bal1_relu =call_channel(21,activation,number_of_overall_iertations_to_the_model,model_loss='msle',optimizer='RMSprop',Full_clean=Full_clean,balance_batch_must=True,tower_min_max_only=False,shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "# chk_bal_2 =call_channel(21,number_of_overall_iertations_to_the_model,model_loss='msle',optimizer='RMSprop',balance_batch_must=True)\n",
    "# chk_bal_0 =call_channel(21,number_of_overall_iertations_to_the_model,model_loss='categorical_crossentropy',optimizer='Adam',balance_batch_must=True)\n",
    "\n",
    "# chk_bal_0 =call_channel(21,number_of_overall_iertations_to_the_model,model_loss='categorical_crossentropy',optimizer='Adam',balance_batch_must=True)\n",
    "# chk_bal_3 =call_channel(21,number_of_overall_iertations_to_the_model,model_loss='categorical_hinge',optimizer='Adagrad',balance_batch_must=True)\n",
    "# chk_bal_4= call_channel(21,number_of_overall_iertations_to_the_model,model_loss='categorical_hinge',optimizer='Adadelta',balance_batch_must=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6712bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "#import tensorflow as tf\n",
    "from data_generation_valid_vin3 import DataGenerator_splited \n",
    "from data_generation_valid_vin3 import Data_test\n",
    "\n",
    "\n",
    "from tensorflow import keras\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from time import sleep# to make the GPU cool down\n",
    "import gc\n",
    "\n",
    "class ovear_all_training:\n",
    "    def __init__(self,channels,test_accuracy=82,total_iteration=100,number_of_overall_iertations_to_the_model=10,validation_steps_check=5,minimum_iteration_validation_chk=1,sleep_time=30, model_loss='MSE', optimize='Adam',Full_clean=True,balance_batch_must=True,validation_split=0.15,shuffle=False,threshold=True,batch_size=32,SITE_MUST=True,fusion_test_chk=0):\n",
    "        '''Initialization\n",
    "        channels        : the depth of the input\n",
    "        test_accuracy   : the wanted minimum test accuracy(since I obtained 86% in earlier model accidently/ but training accuracy is 85.--)\n",
    "        total_iteration : This decide the number of time the data set is used; since the dataset is feed forward and backward once mean epoch count is one)\n",
    "                            preferes if valid split 0.15 then 82    \n",
    "                                if valid split 0.2 then 73   \n",
    "        number_of_overall_iertations_to_the_model: To validate the model architechture gonna satisfy the condition or not, \n",
    "                            thus the training is done on  different ways like changing the hyper aprameters(like depth in each layer or kernal size)\n",
    "                            of the model at each time\n",
    "                            and check the performance changed or not   \n",
    "        sleep_time       : define the waiting time(in milli seconds) to cool down between training sessions\n",
    "        \n",
    "        **\n",
    "        checking the model whether it stuck in the suboptimal or optimal\n",
    "        \n",
    "        minimum_iteration_validation_chk: How many iterations(minimum should be 1) should run before going on check it reached optimal or not\n",
    "\n",
    "        Inorder to do this first check the training accuracy cross the limitation such as \n",
    "                self.train_acc_limit=0.82 thus training accuracy must cross the given accuracy\n",
    "\n",
    "            if the training accuracy cross the limitation then check Validation accuracy\n",
    "            First check the validation acuracy is higher than 0.7(70%);\n",
    "                \n",
    "                validation_steps_check(limit assigned as 5): How many times the validation accuracy stucked in the same position; \n",
    "                                            to avoid leaving the optimal model\n",
    "                                            if the accuracy is <70% then it is reset\n",
    "                                            \n",
    "            if validation accuracy <70%:\n",
    "                validation_low_steps_check(limit assigned as 10): If the validation accuracy less than 70 for long time while the training accuracy is high\n",
    "\n",
    "      validation_split: gives the validation data seperately\n",
    "      '''\n",
    "        self.model_loss=model_loss#'MSE'#\"mean squared error\"\n",
    "        self.optimizer=optimize#'Adam'\n",
    "\n",
    "        self.channels = channels\n",
    "        print('channel initialised: ',self.channels)\n",
    "\n",
    "        self.sleep_time = sleep_time\n",
    "\t\t\n",
    "        #the parameters for data split\n",
    "        self.validation_split = validation_split#0.2 \n",
    "        self.shuffle = shuffle\n",
    "        self.balance_batch_must=balance_batch_must\n",
    "        self.Full_clean = Full_clean\n",
    "        self.batch_size=batch_size\n",
    "#        self.batch_size=10\n",
    "        print(\"\")\n",
    "        print(\"Over ridden self.batch_size as \",self.batch_size)\n",
    "        print(\"\")\n",
    "        self.load_train_size=64\n",
    "\n",
    "\n",
    "        self.test_accuracy=test_accuracy #82 is default the wanted minimum test accuracy\n",
    "        self.total_iteration=total_iteration #earlier v102\n",
    "        self.number_of_overall_iertations_to_the_model=number_of_overall_iertations_to_the_model\n",
    "        self.validation_steps_check = validation_steps_check#earliet its 2\n",
    "        self.minimum_iteration_validation_chk=minimum_iteration_validation_chk\n",
    "        \n",
    "        self.validation_acc_chk_limit = 0.595\n",
    "        self.validation_acc_OG_TSG_limit = 0.712#0.81\n",
    "        self.train_acc_limit=0.82#to check the validation if it cross the training limitations \n",
    "        self.validation_low_steps_check=10\n",
    "        self.fusion_test_chk=fusion_test_chk#0.2 is earlier\n",
    "        print(\"Here doesn't consider fusion at all\")\n",
    "        self.epochs =55#howmany timne the same data used for training/ or feed through the network forward and backward\n",
    "       \n",
    "        optimal_tilted=False\n",
    "        if SITE_MUST:\n",
    "            self.SITE_MUST=True\n",
    "            if channels==21:\n",
    "                self.main_dir=''.join(['scratch/optimaly_tilted_21'])\n",
    "                if threshold and optimal_tilted:\n",
    "                    '''Different threshold condition for surface define'''\n",
    "                    self.main_dir =''.join(['scratch/21_aminoacids__thrsh_new_SITE_MUST'])\n",
    "                elif threshold:\n",
    "                    self.main_dir =''.join(['scratch/21_aminoacids_NO_TILT_thrsh_new_SITE_MUST'])\n",
    "            elif channels==20:\n",
    "                self.main_dir=''.join(['scratch/neg_size_20_21_aminoacids'])\n",
    "            elif channels==17:\n",
    "                self.main_dir=''.join(['scratch/optimaly_tilted_17'])\n",
    "            elif  channels==15:\n",
    "                if threshold:\n",
    "                    self.main_dir=''.join(['scratch/15_aminoacids_thrsh_new_SITE_MUST'])\n",
    "        else:\n",
    "            self.SITE_MUST=False\n",
    "            if channels==20:\n",
    "#                self.main_dir=''.join(['scratch/optimaly_tilted_21'])\n",
    "                if threshold:\n",
    "                    '''Different threshold condition for surface define'''\n",
    "                    self.main_dir =''.join(['scratch/20_aminoacids__thrsh'])\n",
    "            if channels==14:\n",
    "                if threshold:\n",
    "                    self.main_dir=''.join(['scratch/neg_14_21_aminoacids'])       \n",
    "\n",
    "    def validation_accuracy_reached_optimal(self,validation_accuracy_new,validation_accuracy_last,validation_count):\n",
    "        '''\n",
    "        This fucntion check\n",
    "        if the training stucked in sub optimal point then retrive the model, to check the performance\n",
    "       \n",
    "        validation_count        : check howmany times the validation accuracy hasn't changed\n",
    "        validation_accuracy_last: The validation accuracy got for last time\n",
    "        \n",
    "        it returns\n",
    "            to stop the training(True) or (False)\n",
    "        '''\n",
    "        if validation_accuracy_new==validation_accuracy_last:\n",
    "            validation_count = validation_count+1\n",
    "            if validation_count>self.validation_steps_check:\n",
    "                return True,self.validation_steps_check-1\n",
    "            else:\n",
    "                return False,validation_count\n",
    "        else:\n",
    "            return False,0#reset the validation count to 0\n",
    "     \n",
    "    def validation_accuracy_reached_low_optimal(self,validation_accuracy_new,validation_accuracy_last,validation_low_count):\n",
    "        '''\n",
    "        This fucntion check\n",
    "        if the training stucked in sub optimal point then retrive the model, to check the performance\n",
    "       \n",
    "        validation_count        : check howmany times the validation accuracy hasn't changed\n",
    "        validation_accuracy_last: The validation accuracy got for last time\n",
    "        \n",
    "        it returns\n",
    "            to stop the training(True) or (False)\n",
    "        '''\n",
    "        if validation_accuracy_new==validation_accuracy_last:\n",
    "            validation_low_count = validation_low_count + 1 \n",
    "            if validation_low_count > self.validation_low_steps_check:\n",
    "                return True,validation_low_count\n",
    "            else:\n",
    "                return False,validation_low_count\n",
    "        else:\n",
    "            return False,0#reset the validation count to 0    \n",
    "        \n",
    "    def validation_accuracy_reached_optimal_loop(self,validation_accuracy_new,validation_accuracy_last_1,validation_accuracy_last,validation_count_loop):\n",
    "        '''\n",
    "        This fucntion check\n",
    "        if the training stucked in sub optimal point then retrive the model, to check the performance\n",
    "        this check the optimal solution fell in loop(varying between optimal solution back and forward)\n",
    "       \n",
    "        validation_count        : check howmany times the validation accuracy hasn't changed\n",
    "        validation_accuracy_last: The validation accuracy got for last time\n",
    "        \n",
    "        it returns\n",
    "            to stop the training(True) or (False)\n",
    "        '''\n",
    "        # print('validation_accuracy_new: ',validation_accuracy_new)\n",
    "        # print('validation_accuracy_last_1: ',validation_accuracy_last_1)\n",
    "        # print('validation_accuracy_last: ',validation_accuracy_last)\n",
    "        # print('validation_count_loop: ',validation_count_loop)\n",
    "        if (validation_accuracy_new==validation_accuracy_last) and (validation_accuracy_last==validation_accuracy_last_1):\n",
    "\n",
    "            validation_count_loop = validation_count_loop+1\n",
    "            if validation_count_loop > (self.validation_steps_check):\n",
    "                return True,((self.validation_steps_check)-1) # to check whether it stuctk in suboptimal again\n",
    "            else:\n",
    "                return False,validation_count_loop+1\n",
    "        elif validation_accuracy_last<validation_accuracy_new:\n",
    "            return False,0\n",
    "        else:\n",
    "            return False,(validation_count_loop-1)\n",
    "\n",
    "        \n",
    "    def model_parameter_train(self,model,model_name):\n",
    "       \n",
    "        number_of_overall_iertations_to_the_model=  self.number_of_overall_iertations_to_the_model\n",
    "        \n",
    "        total_iteration = self.total_iteration   \n",
    "        train_data_dir= ''.join([self.main_dir,'/Train'])\n",
    "\n",
    "        os.chdir('/')\n",
    "        os.chdir(self.main_dir)\n",
    "        \n",
    "        '''To load the train dataset'''\n",
    "        train_labels = pickle.load(open(\"train_labels_dic.p\", \"rb\"))\n",
    "        '''To load the test dataset'''\n",
    "        test_labels = pickle.load(open(\"test_labels_dic.p\", \"rb\"))\n",
    "\n",
    "\n",
    "        '''train the model\n",
    "        \n",
    "        If Adam optimizer or RMSprop: it take care of the leraning rate after it intialised\n",
    "        \n",
    "        '''\n",
    "        if self.model_loss==\"MSE\":\n",
    "            if self.optimizer=='RMSprop':\n",
    "                model.compile(loss='mean_squared_error',\n",
    "                      optimizer=keras.optimizers.RMSprop(),\n",
    "                      metrics=['accuracy'])\n",
    "            elif self.optimizer=='Adam':\n",
    "                 model.compile(loss='mean_squared_error',\n",
    "                      optimizer=keras.optimizers.Adagrad(),\n",
    "                      metrics=['accuracy'])\n",
    "        elif  self.model_loss == 'msle':\n",
    "            if self.optimizer=='RMSprop':\n",
    "                model.compile(loss='mean_squared_logarithmic_error',\n",
    "                  optimizer=keras.optimizers.RMSprop(),\n",
    "                  metrics=['accuracy'])\n",
    "            elif self.optimizer=='Adam':\n",
    "                 model.compile(loss='mean_squared_logarithmic_error',\n",
    "                      optimizer=keras.optimizers.Adagrad(),\n",
    "                      metrics=['accuracy'])\n",
    "        elif self.model_loss == 'categorical_crossentropy':\n",
    "            if self.optimizer=='RMSprop':\n",
    "                model.compile(loss='categorical_crossentropy',\n",
    "                              optimizer=keras.optimizers.RMSprop(),\n",
    "                              metrics=['accuracy'])\n",
    "            elif self.optimizer=='Adam':\n",
    "                 model.compile(loss='categorical_crossentropy',\n",
    "                      optimizer=keras.optimizers.Adagrad(),\n",
    "                      metrics=['accuracy'])    \n",
    "        elif self.model_loss == 'sparse_categorical_crossentropy' and self.optimizer=='RMSprop':\n",
    "            model.compile(loss='sparse_categorical_crossentropy',\n",
    "                          optimizer=keras.optimizers.RMSprop(),\n",
    "                          metrics=['accuracy'])\n",
    "        elif self.model_loss =='categorical_hinge':\n",
    "           if self.optimizer=='Adagrad':\n",
    "              model.compile(loss='categorical_hinge',\n",
    "                          optimizer=keras.optimizers.Adagrad(),\n",
    "                          metrics=['accuracy'])\n",
    "           elif self.optimizer=='Adadelta':\n",
    "              model.compile(loss='categorical_hinge',\n",
    "                          optimizer=keras.optimizers.Adadelta(),\n",
    "                          metrics=['accuracy'])\n",
    "        '''To break this while if it auceed the limiattions provided'''\n",
    "\n",
    "        #temperory variables\n",
    "        total_sub_optimal_loop=0\n",
    "        self.highest_test_accuracy=0\n",
    "        while number_of_overall_iertations_to_the_model>0:\n",
    "            \n",
    "            print('')\n",
    "            if total_sub_optimal_loop>5:\n",
    "                print('The mdel exceed the suboptimal limitations')\n",
    "                break# while loop Session break\n",
    "\n",
    "            '''To check the validation accuracy stuck in suboptimal'''\n",
    "            self.validation_low_count=0#if the validation accuracy less than 50 for more than 15 iterations continiously \n",
    "                                    #check the test and flush it as new iteration\n",
    "            correct=0 \n",
    "            self.validation_count=0\n",
    "            self.validation_accuracy_last=0\n",
    "            self.validation_accuracy_last_1=-1#to make both different\n",
    "            self.validation_count_loop=0\n",
    "    \n",
    "            print('')\n",
    "            print('')\n",
    "            print('')\n",
    "            print('')\n",
    "            print('*****************************************---------------------->>>>>>>>>>>>>>>>>>>>>>>>>>> ******************')\n",
    "            print('')\n",
    "            print('Remaining iterations ', number_of_overall_iertations_to_the_model, ' in model ', model_name)\n",
    "            print('')\n",
    "            print('loss function: ',self.model_loss)\n",
    "            print('')\n",
    "            print('Optimiser function: ',self.optimizer)\n",
    "            print('')\n",
    "            print('Highest accuracy upto now sessions in test set: ',self.highest_test_accuracy)\n",
    "            print('')\n",
    "#            training_generator = DataGenerator_splited(train_labels,self.main_dir,batch_size=self.batch_size,n_channels=self.channels,Full_clean=self.Full_clean,balance_batch_must=self.balance_batch_must,validation_split=self.validation_split,shuffle=self.shuffle)\n",
    "            training_generator = DataGenerator_splited(train_labels,self.main_dir,batch_size=self.load_train_size,n_channels=self.channels,Full_clean=self.Full_clean,balance_batch_must=self.balance_batch_must,validation_split=self.validation_split,shuffle=self.shuffle)\n",
    "            \n",
    "\n",
    "            if number_of_overall_iertations_to_the_model != self.number_of_overall_iertations_to_the_model:\n",
    "                # to avoid unnecessary reinitialisation in the beginning\n",
    "                model = self.shuffle_weights(model)\n",
    "                print('Reintialised :)')\n",
    "                \n",
    "            print(\"check the test set\")      \n",
    "            session_break,itera_chk,correct,test_probabilities,pdbs_test=self.test_per_chk(model,validation_cond_low=False,session_break=False)\n",
    "            print(\"checking the test set done : )\")      \n",
    "\n",
    "            overall_history_log=[]\n",
    "            validation_history_log=[]\n",
    "            \n",
    "            for itera_train in range(0,total_iteration):\n",
    "                gc.collect()\n",
    "            #    print(history.history.keys())\n",
    "            #    print('validation_accuracy: ',history.history['val_acc'][-1])\n",
    "                train_correct=0\n",
    "                session_break=False\n",
    "#                for fetch in range(0,-(-training_generator.fetch_size()//self.batch_size)):\n",
    "                for fetch in range(0,-(-training_generator.fetch_size()//self.load_train_size)):\n",
    "                    os.chdir('/')\n",
    "                    os.chdir(train_data_dir)\n",
    "                    # Train model on dataset\n",
    "                    x_train, y_train = training_generator.__getitem__(itera_train)\n",
    "    #                train_ids_temp = training_generator.__getitem_list_id(itera)\n",
    "                    if self.batch_size<32:\n",
    "                        history = model.fit(x_train, y_train,batch_size=self.batch_size,epochs=1)#,validation_split=0.2)\n",
    "                        train_correct =train_correct+ history.history['acc'][-1]*self.load_train_size\n",
    "#                        print(history.history.keys())\n",
    "#                        if itera_train==0:\n",
    "#                            if fetch==1:\n",
    "#                                print('History accuarcys look like')\n",
    "#                                print(history.history['acc'])\n",
    "                    else:\n",
    "                        history = model.fit(x_train, y_train,batch_size=self.batch_size,epochs=1)#,validation_split=0.2)\n",
    "#                        history = model.fit(x_train, y_train,batch_size=16,epochs=1)#,validation_split=0.2)\n",
    "                        train_correct =train_correct+ history.history['acc'][-1]*self.load_train_size\n",
    "                    train_acc=(train_correct/((fetch+1)*self.load_train_size))\n",
    "                    if train_acc>0.91 and fetch%4==0 and itera_train>9:\n",
    "                        session_break,while_break_suboptimal,while_break_correct = self.overall_model_per_chk(train_acc,itera_train,validation_history_log,model,model_name)\n",
    "                        if session_break:\n",
    "                            break\n",
    "#                    print('Accuracy in fetch: ',fetch,' ', history.history['acc'][-1])\n",
    "                if session_break:\n",
    "                    break\n",
    "                else:\n",
    "                    train_acc = train_correct/training_generator.fetch_size()\n",
    "                    print(\"\")\n",
    "                    print(\"Overall train iteration of \",itera_train,\"'s accuracy is \",train_acc*100,' %')\n",
    "                    print(\"\")\n",
    "\n",
    "                    session_break,while_break_suboptimal,while_break_correct = self.overall_model_per_chk(train_acc,itera_train,validation_history_log,model,model_name)\n",
    "                if session_break:\n",
    "                    break              \n",
    "                \n",
    "            '''To break the overall while'''\n",
    "            if while_break_correct:\n",
    "                print('Model suceed the limitations :)')\n",
    "                if self.SITE_MUST:\n",
    "                    model_name_latest_part=''.join([model_name[0:-3],'_SITE_',self.model_loss,'_',self.optimizer,'_',str(int(100*(correct/len(test_labels))))])\n",
    "                else:\n",
    "                    model_name_latest_part=''.join([model_name[0:-3],'_',self.model_loss,'_',self.optimizer,'_',str(int(100*(correct/len(test_labels))))])\n",
    "                if self.Full_clean and self.balance_batch_must:\n",
    "                    model_name_latest=''.join([model_name_latest_part,'_Full_clean_bal_bat.h5'])\n",
    "                elif self.Full_clean:\n",
    "                    model_name_latest=''.join([model_name_latest_part,'_Full_clean.h5'])\n",
    "                elif self.balance_batch_must:\n",
    "                    model_name_latest=''.join([model_name_latest_part,'_bal_bat.h5'])\n",
    "                else:\n",
    "                    model_name_latest=''.join([model_name_latest_part,'.h5'])\n",
    "             \n",
    "                print(\"\")                \n",
    "                print(\"suceed model\")\n",
    "                print(model_name_latest)\n",
    "                print(\"\")\n",
    "                model.save(model_name_latest)\n",
    "                gc.collect()\n",
    "                del model\n",
    "                break\n",
    "            \n",
    "            elif while_break_suboptimal:\n",
    "                print(\"since stuck in suboptimal doing the iteration agian\")\n",
    "                overall_history_log=[]\n",
    "                validation_history_log=[]\n",
    "                total_sub_optimal_loop=total_sub_optimal_loop+1\n",
    "            else:\n",
    "                print(\"Session redo\")\n",
    "                number_of_overall_iertations_to_the_model=number_of_overall_iertations_to_the_model-1\n",
    "                overall_history_log=[]\n",
    "                validation_history_log=[]\n",
    "        print('Highest_test_accuracy: ',self.highest_test_accuracy)\n",
    "        if not (while_break_correct):\n",
    "            gc.collect()\n",
    "            del model\n",
    "     \n",
    "        history_all=[validation_history_log,overall_history_log]    \n",
    "        if 100*(correct/len(test_labels))>=self.test_accuracy:\n",
    "            print('')\n",
    "            print(str(self.sleep_time),' Sec break :) satisfied ')\n",
    "            sleep(self.sleep_time)\n",
    "            print('')\n",
    "            return self.highest_test_accuracy,history_all,False#return not_satisfied as false\n",
    "        else:\n",
    "            print('')\n",
    "            print(str(self.sleep_time),' Sec break ')\n",
    "            sleep(self.sleep_time)\n",
    "            print('')\n",
    "            return self.highest_test_accuracy,history_all,True##return not_satisfied as True\n",
    "        \n",
    "\n",
    "\n",
    "    def shuffle_weights(self,model, weights=None):\n",
    "        \"\"\"Randomly permute the weights in `model`, or the given `weights`.\n",
    "        This is a fast approximation of re-initializing the weights of a model.\n",
    "        Assumes weights are distributed independently of the dimensions of the weight tensors\n",
    "          (i.e., the weights have the same distribution along each dimension).\n",
    "        :param Model model: Modify the weights of the given model.\n",
    "        :param list(ndarray) weights: The model's weights will be replaced by a random permutation of these weights.\n",
    "          If `None`, permute the model's current weights.\n",
    "        \"\"\"\n",
    "        if weights is None:\n",
    "            weights = model.get_weights()\n",
    "        \n",
    "        weights = [np.random.permutation(w.flat).reshape(w.shape) for w in weights]\n",
    "        # Faster, but less random: only permutes along the first dimension\n",
    "        # weights = [np.random.permutation(w) for w in weights]\n",
    "        model.set_weights(weights)\n",
    "        return model\n",
    "   \n",
    "    def create_train_accuracy_for_whole_set(self,model,train_data_dir,test_probabilities,pdbs_test):\n",
    "        '''\n",
    "        This function go through the whole training data set and produce the corresponding results\n",
    "        '''\n",
    "        os.chdir('/')\n",
    "        os.chdir(self.main_dir)\n",
    "        pickle.dump(test_probabilities, open(\"test_probabilities.p\", \"wb\"))  \n",
    "        pickle.dump(pdbs_test, open(\"pdbs_test.p\", \"wb\"))  \n",
    "        \n",
    "        train_test_list_IDs = pickle.load(open(\"train_list_ids.p\", \"rb\"))\n",
    "        train_test_labels= pickle.load(open(\"train_labels_dic.p\", \"rb\"))\n",
    "        \n",
    "        os.chdir('/')\n",
    "        os.chdir(train_data_dir)\n",
    "        train_test_generator = Data_test(train_test_list_IDs, train_test_labels,batch_size=self.batch_size,n_channels=self.channels)\n",
    "        train_test_probabilities=[]\n",
    "        pdbs_train_test=[]\n",
    "        c_train_test=0\n",
    "        for itera in range(0,len(train_test_list_IDs)//self.batch_size):\n",
    "            x_train_test, y_train_test = train_test_generator.getitem_test(itera)\n",
    "            test_ids_temp = train_test_generator.__getitem_list_id__(itera)\n",
    "        \n",
    "            train_test_scores = model.evaluate(x_train_test, y_train_test, verbose=2)\n",
    "            probabilities = model.predict(x_train_test)\n",
    "           \n",
    "            train_test_probabilities.append(deepcopy(probabilities))\n",
    "            pdbs_train_test.append(deepcopy(test_ids_temp))\n",
    "            c_train_test=c_train_test+int(train_test_scores[1]*self.batch_size)\n",
    "            \n",
    "        list_IDs_temp=[]\n",
    "        for itera in range((len(train_test_list_IDs)-len(train_test_list_IDs)%self.batch_size),len(train_test_list_IDs)):\n",
    "            list_IDs_temp.append(train_test_list_IDs[itera])\n",
    "        \n",
    "        x_test, y_test = train_test_generator.data_generation_test(list_IDs_temp)\n",
    "        test_scores = model.evaluate(x_test, y_test, verbose=2)\n",
    "        \n",
    "        c_train_test=c_train_test+int(test_scores[1]*(len(train_test_list_IDs)%self.batch_size))\n",
    "        probabilities = model.predict(x_test)\n",
    "        train_test_probabilities.append(deepcopy(probabilities))\n",
    "        pdbs_train_test.append(deepcopy(list_IDs_temp))\n",
    "        \n",
    "        os.chdir('/')\n",
    "        os.chdir(self.main_dir)\n",
    "        pickle.dump(train_test_probabilities, open(\"train_test_probabilities.p\", \"wb\"))  \n",
    "        pickle.dump(pdbs_train_test, open(\"pdbs_train_test.p\", \"wb\"))\n",
    "        print(\"Corresponding_training accuracy: \",100*c_train_test/len(train_test_list_IDs),\" %\")\n",
    "        os.chdir('/')\n",
    "        os.chdir(train_data_dir)\n",
    "        \n",
    "        \n",
    "    def validating_per_chk(self,valid_list_IDs,valid_generator,model,validation_history_log,itera_train):\n",
    "        '''validating performance check'''\n",
    "        train_data_dir= ''.join([self.main_dir,'/Train'])\n",
    "        os.chdir('/')\n",
    "        os.chdir(train_data_dir)\n",
    "        valid_may_sat=False\n",
    "\n",
    "        valid_correct=0\n",
    "        valid_correct_combo=[]\n",
    "        \n",
    "        break_valid=False\n",
    "        valid_chk_rest_TSG_OK=True\n",
    "        valid_chk_rest_ONGO_OK=True\n",
    "        valid_chk_rest_Fusion_OK=True\n",
    "        for itera in range(0,len(valid_list_IDs)//self.batch_size):\n",
    "            x_valid, y_valid = valid_generator.getitem_test(itera)\n",
    "        \n",
    "            valid_scores = model.evaluate(x_valid, y_valid, verbose=2)\n",
    "            print(\"Valid_iter: \",itera,\" Validation acc: \",valid_scores[1])\n",
    "            valid_correct=valid_correct+int(valid_scores[1]*self.batch_size)\n",
    "            valid_acc=valid_correct/((itera+1)*self.batch_size)\n",
    "            break_valid,valid_chk_rest_ONGO_OK,valid_chk_rest_TSG_OK,valid_chk_rest_Fusion_OK,valid_acc= self.validation_break_chk(valid_scores,itera,valid_correct,valid_acc,break_valid,valid_chk_rest_TSG_OK,valid_chk_rest_ONGO_OK,valid_chk_rest_Fusion_OK)\n",
    "            valid_correct_combo.append(valid_scores[1])     \n",
    "            if break_valid:\n",
    "                break\n",
    "            \n",
    "        if valid_chk_rest_ONGO_OK and valid_chk_rest_TSG_OK and valid_chk_rest_Fusion_OK:\n",
    "            valid_ids_temp=[]\n",
    "            for itera in range((len(valid_list_IDs)-len(valid_list_IDs)%self.batch_size),len(valid_list_IDs)):\n",
    "                valid_ids_temp.append(valid_list_IDs[itera])\n",
    "            \n",
    "            x_valid, y_valid = valid_generator.data_generation_test(valid_ids_temp)\n",
    "            valid_scores = model.evaluate(x_valid, y_valid, verbose=2)\n",
    "            print(\"Valid_iter final Validation acc: \",valid_scores[1])\n",
    "\n",
    "            valid_correct=valid_correct+int(valid_scores[1]*(len(valid_list_IDs)%self.batch_size))\n",
    "            valid_acc=valid_correct/len(valid_list_IDs)\n",
    "            print('Validation accuracy in iteration ',itera_train,': ', 100*valid_acc, '%')\n",
    "            validation_history_log.append([itera_train,valid_acc])\n",
    "            if valid_acc>self.train_acc_limit:\n",
    "                print('Validation accuracy:', 100*valid_acc, '%')\n",
    "            test_OG_chk=False\n",
    "            test_TSG_chk=False\n",
    "            for val_acc_i in range(0,len(valid_correct_combo)):\n",
    "                if val_acc_i<3:\n",
    "                    if valid_correct_combo[val_acc_i]>= self.validation_acc_OG_TSG_limit:\n",
    "                        test_OG_chk=True\n",
    "                if 3<val_acc_i<7:\n",
    "                    if valid_correct_combo[val_acc_i]>= self.validation_acc_OG_TSG_limit:\n",
    "                        test_TSG_chk=True\n",
    "            if test_OG_chk and test_TSG_chk and valid_acc >= self.validation_acc_chk_limit:\n",
    "                valid_may_sat=True\n",
    "\n",
    "        return valid_acc,valid_may_sat,validation_history_log\n",
    "    \n",
    "    def overall_model_per_chk(self,train_acc,itera_train,validation_history_log,model,model_name):\n",
    "        '''\n",
    "        check the overall peroformance of the model\n",
    "        '''\n",
    "        '''Initialising validation steps'''\n",
    "        validation_low_count=self.validation_low_count\n",
    "        validation_count=self.validation_count\n",
    "        validation_accuracy_last=self.validation_accuracy_last\n",
    "        validation_count_loop=self.validation_count_loop\n",
    "        validation_accuracy_last_1=self.validation_accuracy_last_1\n",
    "        \n",
    "        train_data_dir= ''.join([self.main_dir,'/Train'])\n",
    "        \n",
    "        while_break_suboptimal=False\n",
    "        while_break_correct=False\n",
    "        session_break=False\n",
    "        os.chdir('/')\n",
    "        os.chdir(self.main_dir)\n",
    "\n",
    "        if self.validation_split>0:\n",
    "            valid_labels = pickle.load(open(\"valid_labels_dic.p\",\"rb\"))\n",
    "            valid_list_IDs = pickle.load(open(\"valid_ids.p\", \"rb\"))\n",
    "            valid_generator = Data_test(valid_list_IDs, valid_labels,batch_size=self.batch_size,n_channels=self.channels)           \n",
    "            valid_acc,valid_may_sat,validation_history_log = self.validating_per_chk(valid_list_IDs,valid_generator,model,validation_history_log,itera_train)\n",
    "        \n",
    "        if (train_acc>self.train_acc_limit) or (valid_acc>(self.test_accuracy/100) or valid_acc>= self.validation_acc_chk_limit):\n",
    "            '''To check it reaches over all optimal  Given limit validation 71%'''\n",
    "            if (train_acc>self.train_acc_limit and itera_train>46):\n",
    "                validation_cond_low,validation_low_count= self.validation_accuracy_reached_low_optimal(valid_acc,validation_accuracy_last,validation_low_count)\n",
    "                validation_cond,validation_count =  self.validation_accuracy_reached_optimal(valid_acc,validation_accuracy_last,validation_count)\n",
    "            else:\n",
    "                validation_cond_low=False\n",
    "                validation_cond=False\n",
    "\n",
    "            validation_cond_loop,validation_count_loop =  self.validation_accuracy_reached_optimal_loop(valid_acc,validation_accuracy_last_1,validation_accuracy_last,validation_count_loop)\n",
    "            validation_accuracy_last_1=deepcopy(validation_accuracy_last)\n",
    "            validation_accuracy_last=deepcopy(valid_acc)\n",
    "\n",
    "            if validation_cond:\n",
    "                '''\n",
    "                if accuracy above 70% only considered\n",
    "                    else reset the conditioin\n",
    "                '''\n",
    "                if valid_acc<self.train_acc_limit:\n",
    "                    validation_count=0\n",
    "                    validation_cond=False\n",
    "                else:\n",
    "                    print(\"Validation accuracy optimal\")\n",
    "                \n",
    "            if (((valid_acc>(self.test_accuracy/100)) or (valid_may_sat) or (validation_cond)) or validation_cond_loop or validation_cond_low) and (itera_train+1)>self.minimum_iteration_validation_chk:\n",
    "               \n",
    "                os.chdir('/')\n",
    "                os.chdir(self.main_dir)\n",
    "                \n",
    "                '''To load the test dataset'''\n",
    "                test_labels = pickle.load(open(\"test_labels_dic.p\", \"rb\"))\n",
    "                session_break,itera_chk,correct,test_probabilities,pdbs_test=self.test_per_chk(model,validation_cond_low,session_break)\n",
    "                \n",
    "                if itera_chk:#to avoid unnecessary evaluation in test set \n",
    "\n",
    "                    os.chdir('/')\n",
    "                    os.chdir(train_data_dir)\n",
    "                    if self.highest_test_accuracy<100*(correct/len(test_labels)):\n",
    "                        self.highest_test_accuracy=100*(correct/len(test_labels))           \n",
    "                        self.create_train_accuracy_for_whole_set(model,train_data_dir,test_probabilities,pdbs_test)\n",
    "                        \n",
    "                    if (100*(correct/len(test_labels)))>=85 or ((100*(correct/len(test_labels)))>=self.test_accuracy and self.channels !=17):\n",
    "                        while_break_correct=True\n",
    "                        session_break=True#to break the iteration For loop\n",
    "                    elif  ((100*(correct/len(test_labels)))> 81 and self.channels != 17):\n",
    "                        if self.SITE_MUST:                           \n",
    "                            #'''For saving the suboptimal soultion like if the results obtained like 80%'''\n",
    "                            model_name_81_part=''.join([model_name[0:-3],'_SITE_',self.model_loss,'_',self.optimizer,'_',str(int(100*(correct/len(test_labels))))])\n",
    "                        else:\n",
    "                            model_name_81_part=''.join([model_name[0:-3],'_',self.model_loss,'_',self.optimizer,'_',str(int(100*(correct/len(test_labels))))])\n",
    "\n",
    "                        if self.Full_clean and self.balance_batch_must:\n",
    "                            model_name_81=''.join([model_name_81_part,'_Full_clean_bal_bat.h5'])\n",
    "                        elif self.Full_clean:\n",
    "                            model_name_81=''.join([model_name_81_part,'_Full_clean.h5'])\n",
    "                        elif self.balance_batch_must:\n",
    "                            model_name_81=''.join([model_name_81_part,'_bal_bat.h5'])\n",
    "                        else:\n",
    "                            model_name_81=''.join([model_name_81_part,'.h5'])\n",
    "\n",
    "                        model.save(model_name_81)\n",
    "                        sleep(self.sleep_time)\n",
    "                    \n",
    "                    elif  ((100*(correct/len(test_labels)))> 71 and self.channels != 17):\n",
    "                        if self.SITE_MUST:       \n",
    "                            #'''For saving the suboptimal soultion like if the results obtained like 80%'''\n",
    "                            model_name_71_part=''.join([model_name[0:-3],'_SITE_',self.model_loss,'_',self.optimizer,'_',str(int(100*(correct/len(test_labels))))])\n",
    "                        else:\n",
    "                            model_name_71_part=''.join([model_name[0:-3],'_',self.model_loss,'_',self.optimizer,'_',str(int(100*(correct/len(test_labels))))])\n",
    "                           \n",
    "                        if self.Full_clean and self.balance_batch_must:\n",
    "                            model_name_71=''.join([model_name_71_part,'_Full_clean_bal_bat.h5'])\n",
    "                        elif self.Full_clean:\n",
    "                            model_name_71=''.join([model_name_71_part,'_Full_clean.h5'])\n",
    "                        elif self.balance_batch_must:\n",
    "                            model_name_71=''.join([model_name_71_part,'_bal_bat.h5'])\n",
    "                        else:\n",
    "                            model_name_71=''.join([model_name_71_part,'.h5'])\n",
    "\n",
    "                        model.save(model_name_71)\n",
    "                        sleep(self.sleep_time)\n",
    "                        \n",
    "                if validation_cond_loop and itera_chk and (100*(correct/len(test_labels)))<self.test_accuracy:\n",
    "                    while_break_suboptimal=True\n",
    "                    session_break=True       \n",
    "                else:\n",
    "                    validation_cond_loop=False\n",
    "                    validation_count_loop=0\n",
    "        \n",
    "        self.validation_low_count=validation_low_count\n",
    "        self.validation_count=validation_count\n",
    "        self.validation_accuracy_last=validation_accuracy_last\n",
    "        self.validation_count_loop=validation_count_loop\n",
    "        self.validation_accuracy_last_1=validation_accuracy_last_1\n",
    "        \n",
    "        gc.collect()\n",
    "        return session_break,while_break_suboptimal,while_break_correct\n",
    "    \n",
    "    def test_per_chk(self,model,validation_cond_low,session_break):\n",
    "        \n",
    "        test_data_dir= ''.join([self.main_dir,'/Test'])     \n",
    "        train_data_dir= ''.join([self.main_dir,'/Train'])\n",
    "\n",
    "        os.chdir('/')\n",
    "        os.chdir(self.main_dir)\n",
    "        \n",
    "        '''To load the test dataset'''\n",
    "        test_list_IDs = pickle.load(open(\"test_list_ids.p\", \"rb\"))\n",
    "        test_labels = pickle.load(open(\"test_labels_dic.p\", \"rb\"))\n",
    "        test_generator = Data_test(test_list_IDs, test_labels,batch_size=self.batch_size,n_channels=self.channels)\n",
    "\n",
    "        print(' ')\n",
    "        print(\"Since epach size is 1 one PDB used once per training \")\n",
    "        print(' ')\n",
    "        os.chdir('/')\n",
    "        os.chdir(test_data_dir)\n",
    "        #% testring\n",
    "        correct=0\n",
    "        \n",
    "        if self.SITE_MUST:\n",
    "            Fusion_test_size=18\n",
    "            TSG_test_size=82\n",
    "        else:\n",
    "            Fusion_test_size=27\n",
    "            TSG_test_size=131\n",
    "        \n",
    "        list_IDs_temp_all=[]\n",
    "        list_IDs_temp=[]\n",
    "        '''check the fusion class'''           \n",
    "        for itera in range(len(test_labels)-Fusion_test_size,len(test_labels)):\n",
    "            list_IDs_temp.append(test_list_IDs[itera])\n",
    "            if len(list_IDs_temp)==self.batch_size:\n",
    "                list_IDs_temp_all.append(deepcopy(list_IDs_temp))\n",
    "                list_IDs_temp=[]                \n",
    "        if len(list_IDs_temp)>0:\n",
    "           list_IDs_temp_all.append(deepcopy(list_IDs_temp))\n",
    "#        print('')\n",
    "#        print('len(list_IDs_temp_all): ',len(list_IDs_temp_all))\n",
    "#        print('')\n",
    "        Fusion_test_correct=0\n",
    "        for list_IDs_temp in list_IDs_temp_all:\n",
    "            if len(list_IDs_temp)>0:\n",
    "                x_test, y_test = test_generator.data_generation_test(list_IDs_temp)\n",
    "                test_scores = model.evaluate(x_test, y_test, verbose=2)\n",
    "                Fusion_test_correct=Fusion_test_correct+test_scores[1]*(len(list_IDs_temp))\n",
    "        Fusion_test_acc = Fusion_test_correct/Fusion_test_size\n",
    "\n",
    "#            print('Test loss    :', test_scores[0])\n",
    "        print('Fusion test accuracy:', 100*Fusion_test_acc, '%')\n",
    "        itera_chk=False #if TSG and Fusion not satisfied \n",
    "        if validation_cond_low and test_scores[1]<self.fusion_test_chk:\n",
    "            print(\"\")\n",
    "            print(\"Stuck in Local Minimum\")\n",
    "            print(\"Starting new iteration in Same **Session\")\n",
    "            print(\"\")\n",
    "            os.chdir('/')\n",
    "            os.chdir(train_data_dir)\n",
    "            session_break=True\n",
    "        else:\n",
    "            test_acc_tsg=0\n",
    "            list_IDs_temp_all=[]\n",
    "            list_IDs_temp=[]\n",
    "            '''check the TSG class'''\n",
    "            for itera in range(len(test_labels)-TSG_test_size-Fusion_test_size,len(test_labels)-Fusion_test_size):\n",
    "                list_IDs_temp.append(test_list_IDs[itera])\n",
    "                if len(list_IDs_temp)==self.batch_size:\n",
    "                    list_IDs_temp_all.append(deepcopy(list_IDs_temp))\n",
    "                    list_IDs_temp=[]                \n",
    "            if len(list_IDs_temp)>0:\n",
    "               list_IDs_temp_all.append(deepcopy(list_IDs_temp))\n",
    "\n",
    "            test_acc_tsg_correct=0\n",
    "            for list_IDs_temp in list_IDs_temp_all:\n",
    "                if len(list_IDs_temp)>0:\n",
    "                    x_test, y_test = test_generator.data_generation_test(list_IDs_temp)\n",
    "                    test_scores = model.evaluate(x_test, y_test, verbose=2)\n",
    "                    test_acc_tsg_correct = test_acc_tsg_correct + test_scores[1]*len(list_IDs_temp)\n",
    "           \n",
    "            test_acc_tsg = test_acc_tsg_correct/TSG_test_size\n",
    "            print('TSG test accuracy:', 100*test_acc_tsg, '%')\n",
    "            if validation_cond_low and test_acc_tsg<self.validation_acc_OG_TSG_limit:\n",
    "                print(\"\")\n",
    "                print(\"Stuck in Local Minimum Due to TSG not satisfied\")\n",
    "                print(\"Starting new iteration in Same **Session\")\n",
    "                print(\"\")\n",
    "                os.chdir('/')\n",
    "                os.chdir(train_data_dir)\n",
    "                session_break=True\n",
    "            \n",
    "        test_probabilities=[]\n",
    "        pdbs_test=[]\n",
    "        correct=0   \n",
    "        if Fusion_test_acc>=self.fusion_test_chk and test_acc_tsg>=self.validation_acc_OG_TSG_limit:\n",
    "            print('Check the Test set overall performance')\n",
    "            itera_chk=True#to avoid unnecessary evaluation in test set \n",
    "            for itera in range(0,len(test_labels)//self.batch_size):\n",
    "                x_test, y_test = test_generator.getitem_test(itera)\n",
    "                test_ids_temp = test_generator.__getitem_list_id__(itera)\n",
    "\n",
    "                test_scores = model.evaluate(x_test, y_test, verbose=2)\n",
    "                probabilities = model.predict(x_test)\n",
    "                test_probabilities.append(deepcopy(probabilities))\n",
    "                pdbs_test.append(deepcopy(test_ids_temp))\n",
    "                correct=correct+int(test_scores[1]*self.batch_size)\n",
    "                print('In iter: ',itera ,'Test acc: ',test_scores[1])\n",
    "\n",
    "        else:\n",
    "            if test_acc_tsg<self.validation_acc_OG_TSG_limit:\n",
    "                print(\"TSG test set haven't got higher than \" ,self.validation_acc_OG_TSG_limit*100 ,\"% accuracy so no test check\")\n",
    "                #to avoid unnecessary evaluation in test set \n",
    "                print(\"Back to normal iteration\")\n",
    "            else:\n",
    "                print(\"Fusion test set haven't got higher than \" ,self.fusion_test_chk*100 ,\"% accuracy so no test check\")\n",
    "                #to avoid unnecessary evaluation in test set \n",
    "                print(\"Back to normal iteration\")\n",
    "        \n",
    "        if itera_chk:\n",
    "            '''To check the last test batch'''\n",
    "            list_IDs_temp=[]\n",
    "            for itera in range((len(test_labels)-len(test_labels)%self.batch_size),len(test_labels)):\n",
    "                list_IDs_temp.append(test_list_IDs[itera])\n",
    "            if len(list_IDs_temp)>0:\n",
    "                x_test, y_test = test_generator.data_generation_test(list_IDs_temp)\n",
    "                test_scores = model.evaluate(x_test, y_test, verbose=2)\n",
    "                print('In last iter test acc: ',test_scores[1])\n",
    "\n",
    "                correct=correct+int(test_scores[1]*(len(list_IDs_temp)))\n",
    "                probabilities = model.predict(x_test)\n",
    "                test_probabilities.append(deepcopy(probabilities))\n",
    "                pdbs_test.append(deepcopy(list_IDs_temp))\n",
    "\n",
    "            print(\"\")\n",
    "            print(\"Overall test accuracy: \",100*correct/len(test_labels))\n",
    "        return session_break,itera_chk,correct,test_probabilities,pdbs_test\n",
    "    \n",
    "    def validation_break_chk(self,valid_scores,itera,valid_correct,valid_acc,break_valid,valid_chk_rest_TSG_OK,valid_chk_rest_ONGO_OK,valid_chk_rest_Fusion_OK):\n",
    "        '''\n",
    "        This function used for early stopping in validation evaluation\n",
    "        '''\n",
    "        os.chdir('/')\n",
    "        os.chdir(self.main_dir)\n",
    "        \n",
    "        lengths_summery_dic = pickle.load(open(\"lengths_summery_dic.p\", \"rb\")) \n",
    "        valid_OG_len = lengths_summery_dic['valid_OG_len']\n",
    "        valid_TSG_len = lengths_summery_dic['valid_TSG_len']\n",
    "        valid_len = lengths_summery_dic['valid_len']\n",
    "        \n",
    "        if itera==0 and valid_scores[1]<0.1:\n",
    "            valid_chk_rest_ONGO_OK=False\n",
    "            valid_acc=valid_TSG_len/valid_len#since only fit with TSG\n",
    "            print(\"Validation since Not fit with ONGO\")\n",
    "            break_valid=True\n",
    "            \n",
    "        elif itera==(-(-valid_OG_len//self.batch_size))+1 and valid_scores[1]<0.1:\n",
    "            valid_chk_rest_TSG_OK=False\n",
    "            valid_acc=valid_OG_len/valid_len#since only fit with ONGO\n",
    "            print(\"Validation since NOt fit with TSG\")\n",
    "            break_valid=True\n",
    "            \n",
    "        elif itera== itera==(-(-(valid_OG_len+valid_TSG_len)//self.batch_size))+1 and valid_scores[1]<0.1:\n",
    "#            valid_chk_rest_Fusion_OK=False#Not fit with Fusion at all\n",
    "#            valid_acc = valid_correct/valid_len\n",
    "#            print(\"Validation since Not fit with Fusion\")\n",
    "#            break_valid=True\n",
    "            print(\"Warning Fusion is not checked in Validation\")\n",
    "        train_data_dir= ''.join([self.main_dir,'/Train'])\n",
    "        os.chdir('/')\n",
    "        os.chdir(train_data_dir)\n",
    "        \n",
    "        return break_valid,valid_chk_rest_ONGO_OK,valid_chk_rest_TSG_OK,valid_chk_rest_Fusion_OK,valid_acc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
